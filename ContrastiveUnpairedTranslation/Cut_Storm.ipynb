{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN b18z CUT 40x 63x dataset\n",
    "aligend training 60-63x\n",
    "unaligend testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                 CUT_mode: CUT                           \n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "                    beta2: 0.999                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/train/\t[default: placeholder]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: None                          \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8087                          \t[default: 8097]\n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "               easy_label: experiment_name               \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "          evaluation_freq: 5000                          \n",
      "        flip_equivariance: False                         \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: xavier                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "               lambda_GAN: 1.0                           \n",
      "               lambda_NCE: 1.0                           \n",
      "                load_size: 256                           \t[default: 286]\n",
      "                       lr: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 40                            \t[default: 50]\n",
      "                lr_policy: step                          \t[default: linear]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: cut                           \n",
      "                 n_epochs: 150                           \t[default: 200]\n",
      "           n_epochs_decay: 200                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: cut_b18z40_63x_aligned_resize \t[default: experiment_name]\n",
      "                    nce_T: 0.07                          \n",
      "                  nce_idt: True                          \n",
      "nce_includes_all_negatives_from_minibatch: False                         \n",
      "               nce_layers: 0,4,8,12,16                   \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netF: mlp_sample                    \n",
      "                  netF_nc: 256                           \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "             no_antialias: False                         \n",
      "          no_antialias_up: False                         \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                    normD: instance                      \n",
      "                    normG: instance                      \n",
      "              num_patches: 256                           \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize                        \t[default: resize_and_crop]\n",
      "          pretrained_name: None                          \n",
      "               print_freq: 100                           \n",
      "         random_scale_max: 3.0                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 60                            \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "stylegan2_G_num_downsampling: 1                             \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "model [CUTModel] was created\n",
      "The number of training images = 333\n",
      "Setting up a new session...\n",
      "Exception in user code:\n",
      "------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 170, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1285, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1331, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1280, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1046, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 984, in send\n",
      "    self.connect()\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 182, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f94b72e4a90>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8087): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f94b72e4a90>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/visdom/__init__.py\", line 711, in _send\n",
      "    data=json.dumps(msg),\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/visdom/__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8087): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f94b72e4a90>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "[Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Could not connect to Visdom server. \n",
      " Trying to start a server....\n",
      "Command: /storage01/grexai/dev/envs/pix2pix/bin/python -m visdom.server -p 8087 &>/dev/null &\n",
      "create web directory ./checkpoints/cut_b18z40_63x_aligned_resize/web...\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "[Network F] Total number of parameters : 0.560 M\n",
      "[Network D] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "(epoch: 1, iters: 100, time: 0.164, data: 0.532) G_GAN: 0.277 D_real: 0.230 D_fake: 0.279 G: 3.514 NCE: 3.279 NCE_Y: 3.195 \n",
      "(epoch: 1, iters: 200, time: 0.194, data: 0.001) G_GAN: 0.271 D_real: 0.285 D_fake: 0.250 G: 2.202 NCE: 2.075 NCE_Y: 1.786 \n",
      "(epoch: 1, iters: 300, time: 0.211, data: 0.001) G_GAN: 0.337 D_real: 0.383 D_fake: 0.245 G: 3.026 NCE: 1.947 NCE_Y: 3.432 \n",
      "End of epoch 1 / 350 \t Time Taken: 83 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 2, iters: 67, time: 0.223, data: 0.001) G_GAN: 0.271 D_real: 0.260 D_fake: 0.241 G: 1.935 NCE: 1.821 NCE_Y: 1.507 \n",
      "(epoch: 2, iters: 167, time: 0.229, data: 0.001) G_GAN: 0.275 D_real: 0.253 D_fake: 0.267 G: 1.805 NCE: 1.748 NCE_Y: 1.311 \n",
      "(epoch: 2, iters: 267, time: 0.233, data: 0.001) G_GAN: 0.283 D_real: 0.199 D_fake: 0.253 G: 3.991 NCE: 3.833 NCE_Y: 3.584 \n",
      "End of epoch 2 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 3, iters: 34, time: 0.235, data: 0.001) G_GAN: 0.308 D_real: 0.232 D_fake: 0.240 G: 1.762 NCE: 1.530 NCE_Y: 1.379 \n",
      "(epoch: 3, iters: 134, time: 0.236, data: 0.002) G_GAN: 0.226 D_real: 0.371 D_fake: 0.123 G: 1.999 NCE: 2.003 NCE_Y: 1.542 \n",
      "(epoch: 3, iters: 234, time: 0.236, data: 0.001) G_GAN: 0.697 D_real: 0.159 D_fake: 0.053 G: 2.221 NCE: 1.500 NCE_Y: 1.549 \n",
      "End of epoch 3 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 4, iters: 1, time: 0.237, data: 0.001) G_GAN: 0.227 D_real: 0.728 D_fake: 0.018 G: 1.768 NCE: 1.684 NCE_Y: 1.398 \n",
      "(epoch: 4, iters: 101, time: 0.238, data: 0.002) G_GAN: 0.307 D_real: 0.430 D_fake: 0.051 G: 1.945 NCE: 1.666 NCE_Y: 1.611 \n",
      "(epoch: 4, iters: 201, time: 0.238, data: 0.001) G_GAN: 0.966 D_real: 0.020 D_fake: 0.031 G: 2.307 NCE: 1.494 NCE_Y: 1.189 \n",
      "(epoch: 4, iters: 301, time: 0.238, data: 0.001) G_GAN: 0.300 D_real: 0.264 D_fake: 0.256 G: 1.636 NCE: 1.441 NCE_Y: 1.232 \n",
      "End of epoch 4 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 5, iters: 68, time: 0.238, data: 0.001) G_GAN: 0.449 D_real: 0.166 D_fake: 0.176 G: 1.789 NCE: 1.445 NCE_Y: 1.235 \n",
      "(epoch: 5, iters: 168, time: 0.238, data: 0.001) G_GAN: 0.259 D_real: 0.210 D_fake: 0.476 G: 1.522 NCE: 1.435 NCE_Y: 1.089 \n",
      "(epoch: 5, iters: 268, time: 0.238, data: 0.001) G_GAN: 0.349 D_real: 0.238 D_fake: 0.196 G: 1.564 NCE: 1.345 NCE_Y: 1.085 \n",
      "End of epoch 5 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 6, iters: 35, time: 0.238, data: 0.001) G_GAN: 0.298 D_real: 0.243 D_fake: 0.234 G: 1.442 NCE: 1.274 NCE_Y: 1.014 \n",
      "(epoch: 6, iters: 135, time: 0.239, data: 0.002) G_GAN: 0.321 D_real: 0.329 D_fake: 0.210 G: 1.472 NCE: 1.313 NCE_Y: 0.989 \n",
      "(epoch: 6, iters: 235, time: 0.239, data: 0.001) G_GAN: 0.501 D_real: 0.088 D_fake: 0.247 G: 1.782 NCE: 1.455 NCE_Y: 1.106 \n",
      "End of epoch 6 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 7, iters: 2, time: 0.239, data: 0.002) G_GAN: 0.333 D_real: 0.259 D_fake: 0.203 G: 1.523 NCE: 1.309 NCE_Y: 1.071 \n",
      "(epoch: 7, iters: 102, time: 0.239, data: 0.001) G_GAN: 0.277 D_real: 0.231 D_fake: 0.233 G: 1.452 NCE: 1.334 NCE_Y: 1.015 \n",
      "(epoch: 7, iters: 202, time: 0.239, data: 0.001) G_GAN: 0.285 D_real: 0.188 D_fake: 0.309 G: 1.426 NCE: 1.261 NCE_Y: 1.022 \n",
      "(epoch: 7, iters: 302, time: 0.238, data: 0.001) G_GAN: 0.259 D_real: 0.348 D_fake: 0.270 G: 1.505 NCE: 1.347 NCE_Y: 1.145 \n",
      "End of epoch 7 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 8, iters: 69, time: 0.235, data: 0.001) G_GAN: 0.249 D_real: 0.365 D_fake: 0.283 G: 1.627 NCE: 1.606 NCE_Y: 1.152 \n",
      "(epoch: 8, iters: 169, time: 0.236, data: 0.002) G_GAN: 0.455 D_real: 0.051 D_fake: 0.317 G: 1.676 NCE: 1.358 NCE_Y: 1.084 \n",
      "(epoch: 8, iters: 269, time: 0.236, data: 0.001) G_GAN: 0.329 D_real: 0.227 D_fake: 0.257 G: 1.425 NCE: 1.218 NCE_Y: 0.975 \n",
      "End of epoch 8 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 9, iters: 36, time: 0.236, data: 0.001) G_GAN: 1.018 D_real: 0.006 D_fake: 0.014 G: 2.173 NCE: 1.254 NCE_Y: 1.055 \n",
      "(epoch: 9, iters: 136, time: 0.237, data: 0.001) G_GAN: 0.398 D_real: 0.211 D_fake: 0.216 G: 1.573 NCE: 1.292 NCE_Y: 1.058 \n",
      "(epoch: 9, iters: 236, time: 0.235, data: 0.002) G_GAN: 0.351 D_real: 0.398 D_fake: 0.140 G: 1.655 NCE: 1.361 NCE_Y: 1.248 \n",
      "End of epoch 9 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 10, iters: 3, time: 0.236, data: 0.001) G_GAN: 0.385 D_real: 0.266 D_fake: 0.181 G: 1.533 NCE: 1.257 NCE_Y: 1.038 \n",
      "(epoch: 10, iters: 103, time: 0.236, data: 0.002) G_GAN: 0.408 D_real: 0.178 D_fake: 0.242 G: 1.591 NCE: 1.258 NCE_Y: 1.107 \n",
      "(epoch: 10, iters: 203, time: 0.236, data: 0.001) G_GAN: 0.607 D_real: 0.101 D_fake: 0.144 G: 1.879 NCE: 1.480 NCE_Y: 1.064 \n",
      "(epoch: 10, iters: 303, time: 0.236, data: 0.002) G_GAN: 1.038 D_real: 0.011 D_fake: 0.024 G: 2.238 NCE: 1.312 NCE_Y: 1.088 \n",
      "End of epoch 10 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 11, iters: 70, time: 0.237, data: 0.001) G_GAN: 0.251 D_real: 0.222 D_fake: 0.351 G: 1.420 NCE: 1.292 NCE_Y: 1.046 \n",
      "(epoch: 11, iters: 170, time: 0.238, data: 0.001) G_GAN: 0.669 D_real: 0.150 D_fake: 0.081 G: 1.797 NCE: 1.233 NCE_Y: 1.024 \n",
      "(epoch: 11, iters: 270, time: 0.238, data: 0.001) G_GAN: 0.243 D_real: 0.398 D_fake: 0.248 G: 1.410 NCE: 1.208 NCE_Y: 1.124 \n",
      "End of epoch 11 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 12, iters: 37, time: 0.237, data: 0.001) G_GAN: 0.300 D_real: 0.231 D_fake: 0.234 G: 1.405 NCE: 1.223 NCE_Y: 0.987 \n",
      "(epoch: 12, iters: 137, time: 0.237, data: 0.001) G_GAN: 0.357 D_real: 0.269 D_fake: 0.250 G: 1.483 NCE: 1.192 NCE_Y: 1.059 \n",
      "(epoch: 12, iters: 237, time: 0.237, data: 0.001) G_GAN: 0.542 D_real: 0.069 D_fake: 0.198 G: 1.705 NCE: 1.256 NCE_Y: 1.071 \n",
      "End of epoch 12 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 13, iters: 4, time: 0.237, data: 0.001) G_GAN: 0.525 D_real: 0.050 D_fake: 0.275 G: 1.865 NCE: 1.466 NCE_Y: 1.215 \n",
      "(epoch: 13, iters: 104, time: 0.238, data: 0.003) G_GAN: 0.398 D_real: 0.246 D_fake: 0.113 G: 1.482 NCE: 1.136 NCE_Y: 1.031 \n",
      "(epoch: 13, iters: 204, time: 0.238, data: 0.001) G_GAN: 0.318 D_real: 0.339 D_fake: 0.219 G: 1.506 NCE: 1.373 NCE_Y: 1.002 \n",
      "(epoch: 13, iters: 304, time: 0.238, data: 0.001) G_GAN: 0.325 D_real: 0.252 D_fake: 0.232 G: 1.413 NCE: 1.238 NCE_Y: 0.938 \n",
      "End of epoch 13 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 14, iters: 71, time: 0.238, data: 0.001) G_GAN: 0.298 D_real: 0.397 D_fake: 0.175 G: 1.349 NCE: 1.165 NCE_Y: 0.936 \n",
      "(epoch: 14, iters: 171, time: 0.238, data: 0.001) G_GAN: 0.701 D_real: 0.031 D_fake: 0.180 G: 1.839 NCE: 1.223 NCE_Y: 1.053 \n",
      "(epoch: 14, iters: 271, time: 0.238, data: 0.001) G_GAN: 0.354 D_real: 0.112 D_fake: 0.209 G: 1.508 NCE: 1.149 NCE_Y: 1.159 \n",
      "End of epoch 14 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 15, iters: 38, time: 0.238, data: 0.001) G_GAN: 0.341 D_real: 0.348 D_fake: 0.150 G: 1.536 NCE: 1.409 NCE_Y: 0.981 \n",
      "(epoch: 15, iters: 138, time: 0.238, data: 0.001) G_GAN: 0.245 D_real: 0.249 D_fake: 0.283 G: 1.311 NCE: 1.172 NCE_Y: 0.959 \n",
      "(epoch: 15, iters: 238, time: 0.237, data: 0.001) G_GAN: 0.431 D_real: 0.418 D_fake: 0.125 G: 1.551 NCE: 1.197 NCE_Y: 1.044 \n",
      "End of epoch 15 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 16, iters: 5, time: 0.238, data: 0.001) G_GAN: 0.215 D_real: 0.222 D_fake: 0.269 G: 1.286 NCE: 1.207 NCE_Y: 0.933 \n",
      "saving the latest model (epoch 16, total_iters 5000)\n",
      "cut_b18z40_63x_aligned_resize\n",
      "(epoch: 16, iters: 105, time: 0.238, data: 0.002) G_GAN: 0.452 D_real: 0.050 D_fake: 0.322 G: 1.825 NCE: 1.086 NCE_Y: 1.660 \n",
      "(epoch: 16, iters: 205, time: 0.238, data: 0.001) G_GAN: 0.419 D_real: 0.029 D_fake: 0.256 G: 1.599 NCE: 1.158 NCE_Y: 1.202 \n",
      "(epoch: 16, iters: 305, time: 0.238, data: 0.001) G_GAN: 0.308 D_real: 0.183 D_fake: 0.285 G: 1.389 NCE: 1.246 NCE_Y: 0.917 \n",
      "End of epoch 16 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 17, iters: 72, time: 0.237, data: 0.001) G_GAN: 0.251 D_real: 0.376 D_fake: 0.245 G: 1.337 NCE: 1.209 NCE_Y: 0.962 \n",
      "(epoch: 17, iters: 172, time: 0.237, data: 0.001) G_GAN: 0.321 D_real: 0.386 D_fake: 0.164 G: 1.475 NCE: 1.374 NCE_Y: 0.935 \n",
      "(epoch: 17, iters: 272, time: 0.238, data: 0.001) G_GAN: 0.480 D_real: 0.008 D_fake: 0.282 G: 1.803 NCE: 1.192 NCE_Y: 1.456 \n",
      "End of epoch 17 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 18, iters: 39, time: 0.237, data: 0.001) G_GAN: 0.328 D_real: 0.299 D_fake: 0.195 G: 1.408 NCE: 1.167 NCE_Y: 0.993 \n",
      "(epoch: 18, iters: 139, time: 0.238, data: 0.001) G_GAN: 0.420 D_real: 0.171 D_fake: 0.203 G: 1.561 NCE: 1.229 NCE_Y: 1.054 \n",
      "(epoch: 18, iters: 239, time: 0.237, data: 0.001) G_GAN: 0.296 D_real: 0.340 D_fake: 0.178 G: 1.315 NCE: 1.152 NCE_Y: 0.887 \n",
      "End of epoch 18 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 19, iters: 6, time: 0.237, data: 0.001) G_GAN: 0.516 D_real: 0.565 D_fake: 0.357 G: 1.668 NCE: 1.180 NCE_Y: 1.123 \n",
      "(epoch: 19, iters: 106, time: 0.237, data: 0.001) G_GAN: 0.547 D_real: 0.119 D_fake: 0.112 G: 1.650 NCE: 1.226 NCE_Y: 0.981 \n",
      "(epoch: 19, iters: 206, time: 0.236, data: 0.001) G_GAN: 0.272 D_real: 0.270 D_fake: 0.219 G: 1.286 NCE: 1.149 NCE_Y: 0.880 \n",
      "(epoch: 19, iters: 306, time: 0.236, data: 0.001) G_GAN: 0.465 D_real: 0.463 D_fake: 0.113 G: 1.551 NCE: 1.187 NCE_Y: 0.986 \n",
      "End of epoch 19 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 20, iters: 73, time: 0.237, data: 0.001) G_GAN: 0.272 D_real: 0.156 D_fake: 0.307 G: 1.375 NCE: 1.234 NCE_Y: 0.971 \n",
      "(epoch: 20, iters: 173, time: 0.236, data: 0.001) G_GAN: 0.454 D_real: 0.038 D_fake: 0.153 G: 1.674 NCE: 1.371 NCE_Y: 1.068 \n",
      "(epoch: 20, iters: 273, time: 0.237, data: 0.002) G_GAN: 0.335 D_real: 0.037 D_fake: 0.327 G: 1.511 NCE: 1.377 NCE_Y: 0.975 \n",
      "End of epoch 20 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 21, iters: 40, time: 0.237, data: 0.001) G_GAN: 0.541 D_real: 0.141 D_fake: 0.216 G: 1.898 NCE: 1.411 NCE_Y: 1.304 \n",
      "(epoch: 21, iters: 140, time: 0.237, data: 0.001) G_GAN: 0.304 D_real: 0.216 D_fake: 0.348 G: 1.340 NCE: 1.191 NCE_Y: 0.881 \n",
      "(epoch: 21, iters: 240, time: 0.237, data: 0.002) G_GAN: 0.283 D_real: 0.256 D_fake: 0.271 G: 1.380 NCE: 1.196 NCE_Y: 0.998 \n",
      "End of epoch 21 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 22, iters: 7, time: 0.236, data: 0.002) G_GAN: 0.231 D_real: 0.197 D_fake: 0.314 G: 1.272 NCE: 1.148 NCE_Y: 0.933 \n",
      "(epoch: 22, iters: 107, time: 0.237, data: 0.000) G_GAN: 0.286 D_real: 0.300 D_fake: 0.218 G: 1.312 NCE: 1.173 NCE_Y: 0.880 \n",
      "(epoch: 22, iters: 207, time: 0.238, data: 0.001) G_GAN: 0.397 D_real: 0.057 D_fake: 0.233 G: 1.593 NCE: 1.195 NCE_Y: 1.199 \n",
      "(epoch: 22, iters: 307, time: 0.238, data: 0.001) G_GAN: 0.405 D_real: 0.111 D_fake: 0.229 G: 1.518 NCE: 1.205 NCE_Y: 1.021 \n",
      "End of epoch 22 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 23, iters: 74, time: 0.237, data: 0.001) G_GAN: 0.265 D_real: 0.294 D_fake: 0.271 G: 1.321 NCE: 1.155 NCE_Y: 0.958 \n",
      "(epoch: 23, iters: 174, time: 0.236, data: 0.002) G_GAN: 0.253 D_real: 0.233 D_fake: 0.250 G: 1.264 NCE: 1.148 NCE_Y: 0.874 \n",
      "(epoch: 23, iters: 274, time: 0.237, data: 0.002) G_GAN: 0.649 D_real: 0.010 D_fake: 0.145 G: 2.049 NCE: 1.173 NCE_Y: 1.626 \n",
      "End of epoch 23 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 24, iters: 41, time: 0.237, data: 0.001) G_GAN: 0.236 D_real: 0.264 D_fake: 0.292 G: 1.241 NCE: 1.132 NCE_Y: 0.878 \n",
      "(epoch: 24, iters: 141, time: 0.238, data: 0.001) G_GAN: 0.541 D_real: 0.126 D_fake: 0.098 G: 1.626 NCE: 1.164 NCE_Y: 1.006 \n",
      "(epoch: 24, iters: 241, time: 0.238, data: 0.001) G_GAN: 0.340 D_real: 0.331 D_fake: 0.328 G: 1.466 NCE: 1.275 NCE_Y: 0.978 \n",
      "End of epoch 24 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 25, iters: 8, time: 0.239, data: 0.001) G_GAN: 0.469 D_real: 0.009 D_fake: 0.218 G: 1.673 NCE: 1.176 NCE_Y: 1.231 \n",
      "(epoch: 25, iters: 108, time: 0.238, data: 0.001) G_GAN: 0.268 D_real: 0.255 D_fake: 0.248 G: 1.291 NCE: 1.152 NCE_Y: 0.895 \n",
      "(epoch: 25, iters: 208, time: 0.238, data: 0.001) G_GAN: 0.376 D_real: 0.214 D_fake: 0.196 G: 1.541 NCE: 1.375 NCE_Y: 0.955 \n",
      "(epoch: 25, iters: 308, time: 0.239, data: 0.001) G_GAN: 0.546 D_real: 0.439 D_fake: 0.222 G: 1.591 NCE: 1.161 NCE_Y: 0.929 \n",
      "End of epoch 25 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 26, iters: 75, time: 0.238, data: 0.001) G_GAN: 0.454 D_real: 0.161 D_fake: 0.202 G: 1.663 NCE: 1.293 NCE_Y: 1.124 \n",
      "(epoch: 26, iters: 175, time: 0.238, data: 0.001) G_GAN: 0.286 D_real: 0.243 D_fake: 0.247 G: 1.304 NCE: 1.145 NCE_Y: 0.892 \n",
      "(epoch: 26, iters: 275, time: 0.238, data: 0.001) G_GAN: 0.343 D_real: 0.072 D_fake: 0.302 G: 1.325 NCE: 1.137 NCE_Y: 0.826 \n",
      "End of epoch 26 / 350 \t Time Taken: 79 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 27, iters: 42, time: 0.236, data: 0.001) G_GAN: 0.421 D_real: 0.006 D_fake: 0.565 G: 1.466 NCE: 1.120 NCE_Y: 0.971 \n",
      "(epoch: 27, iters: 142, time: 0.238, data: 0.001) G_GAN: 0.307 D_real: 0.129 D_fake: 0.241 G: 1.348 NCE: 1.182 NCE_Y: 0.900 \n",
      "(epoch: 27, iters: 242, time: 0.237, data: 0.001) G_GAN: 0.334 D_real: 0.313 D_fake: 0.204 G: 1.403 NCE: 1.234 NCE_Y: 0.904 \n",
      "End of epoch 27 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 28, iters: 9, time: 0.237, data: 0.002) G_GAN: 0.278 D_real: 0.247 D_fake: 0.247 G: 1.354 NCE: 1.161 NCE_Y: 0.991 \n",
      "(epoch: 28, iters: 109, time: 0.237, data: 0.002) G_GAN: 0.290 D_real: 0.371 D_fake: 0.183 G: 1.292 NCE: 1.140 NCE_Y: 0.862 \n",
      "(epoch: 28, iters: 209, time: 0.238, data: 0.001) G_GAN: 0.382 D_real: 0.212 D_fake: 0.224 G: 1.490 NCE: 1.178 NCE_Y: 1.038 \n",
      "(epoch: 28, iters: 309, time: 0.237, data: 0.002) G_GAN: 0.256 D_real: 0.230 D_fake: 0.286 G: 1.287 NCE: 1.166 NCE_Y: 0.896 \n",
      "End of epoch 28 / 350 \t Time Taken: 80 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 29, iters: 76, time: 0.243, data: 0.001) G_GAN: 0.650 D_real: 0.089 D_fake: 0.125 G: 2.083 NCE: 1.123 NCE_Y: 1.744 \n",
      "(epoch: 29, iters: 176, time: 0.243, data: 0.002) G_GAN: 0.311 D_real: 0.322 D_fake: 0.193 G: 1.357 NCE: 1.179 NCE_Y: 0.912 \n",
      "(epoch: 29, iters: 276, time: 0.241, data: 0.001) G_GAN: 0.331 D_real: 0.284 D_fake: 0.175 G: 1.372 NCE: 1.170 NCE_Y: 0.914 \n",
      "End of epoch 29 / 350 \t Time Taken: 82 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 30, iters: 43, time: 0.240, data: 0.001) G_GAN: 0.387 D_real: 0.164 D_fake: 0.495 G: 1.403 NCE: 1.197 NCE_Y: 0.837 \n",
      "(epoch: 30, iters: 143, time: 0.240, data: 0.001) G_GAN: 0.264 D_real: 0.279 D_fake: 0.241 G: 1.288 NCE: 1.176 NCE_Y: 0.872 \n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/train/ \\\n",
    "--name cut_b18z40_63x_aligned_resize --lr=0.0001 --lr_decay_iters=40 --lr_policy step --batch_size=1 \\\n",
    "--preprocess resize --load_size=256 --save_epoch_freq=60 --n_epochs=150  --display_port=8087\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                 CUT_mode: CUT                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/\t[default: placeholder]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "               easy_label: experiment_name               \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "        flip_equivariance: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: xavier                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "               lambda_GAN: 1.0                           \n",
      "               lambda_NCE: 1.0                           \n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cut                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: cut_b18z40_63x_aligned_resize \t[default: experiment_name]\n",
      "                    nce_T: 0.07                          \n",
      "                  nce_idt: True                          \n",
      "nce_includes_all_negatives_from_minibatch: False                         \n",
      "               nce_layers: 0,4,8,12,16                   \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netF: mlp_sample                    \n",
      "                  netF_nc: 256                           \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "             no_antialias: False                         \n",
      "          no_antialias_up: False                         \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                    normD: instance                      \n",
      "                    normG: instance                      \n",
      "              num_patches: 256                           \n",
      "                 num_test: 99999                         \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \t[default: test]\n",
      "                pool_size: 0                             \n",
      "               preprocess: resize                        \t[default: resize_and_crop]\n",
      "         random_scale_max: 3.0                           \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "stylegan2_G_num_downsampling: 1                             \n",
      "                   suffix:                               \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "dataset [UnalignedDataset] was created\n",
      "model [CUTModel] was created\n",
      "creating web directory ./results/cut_b18z40_63x_aligned_resize/train_latest\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "loading the model from ./checkpoints/cut_b18z40_63x_aligned_resize/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "processing (0000)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m1700_c1_z1_l1_o0_1.png']\n",
      "processing (0005)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m1800_c1_z1_l1_o0_3.png']\n",
      "processing (0010)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m2030_c1_z1_l1_o0_2.png']\n",
      "processing (0015)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m2720_c1_z1_l1_o0_1.png']\n",
      "processing (0020)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m2800_c1_z1_l1_o0_3.png']\n",
      "processing (0025)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m3500_c1_z1_l1_o0_2.png']\n",
      "processing (0030)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m4010_c1_z1_l1_o0_1.png']\n",
      "processing (0035)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m4320_c1_z1_l1_o0_3.png']\n",
      "processing (0040)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m4810_c1_z1_l1_o0_2.png']\n",
      "processing (0045)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m5030_c1_z1_l1_o0_1.png']\n",
      "processing (0050)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m5110_c1_z1_l1_o0_3.png']\n",
      "processing (0055)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m5220_c1_z1_l1_o0_2.png']\n",
      "processing (0060)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m5710_c1_z1_l1_o0_1.png']\n",
      "processing (0065)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m6010_c1_z1_l1_o0_3.png']\n",
      "processing (0070)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m6030_c1_z1_l1_o0_2.png']\n",
      "processing (0075)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m6220_c1_z1_l1_o0_1.png']\n",
      "processing (0080)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m6300_c1_z1_l1_o0_3.png']\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/ \\\n",
    "--preprocess resize --load_size=256 --name cut_b18z40_63x_aligned_resize --CUT_mode CUT --phase train --num_test=99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                augment_a: False                         \n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "               brightness: False                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                 contrast: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/Aligned/train/\t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8087                          \t[default: 8097]\n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                    gauss: False                         \n",
      "                  gpu_ids: 0,1                           \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.45                          \t[default: 0.5]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 1024                          \t[default: 286]\n",
      "                       lr: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 40                            \t[default: 50]\n",
      "                lr_policy: step                          \t[default: linear]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: cycle_b18_40x63xscale_crop_1024_v2\t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: scale_width_and_crop          \t[default: resize_and_crop]\n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 60                            \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                sharpness: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 896\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "Setting up a new session...\n",
      "create web directory ./checkpoints/cycle_b18_40x63xscale_crop_1024_v2/web...\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 1, iters: 100, time: 0.273, data: 2.561) D_A: 0.366 G_A: 0.388 cycle_A: 3.209 idt_A: 0.562 D_B: 0.541 G_B: 0.339 cycle_B: 1.236 idt_B: 1.439 \n",
      "(epoch: 1, iters: 200, time: 0.271, data: 0.001) D_A: 0.233 G_A: 0.341 cycle_A: 1.732 idt_A: 0.563 D_B: 0.221 G_B: 0.458 cycle_B: 1.285 idt_B: 0.814 \n",
      "(epoch: 1, iters: 300, time: 0.312, data: 0.002) D_A: 0.278 G_A: 0.306 cycle_A: 1.343 idt_A: 0.637 D_B: 0.285 G_B: 0.666 cycle_B: 1.421 idt_B: 0.763 \n",
      "(epoch: 1, iters: 400, time: 1.102, data: 0.001) D_A: 0.274 G_A: 0.423 cycle_A: 1.184 idt_A: 0.482 D_B: 0.272 G_B: 0.309 cycle_B: 1.119 idt_B: 0.461 \n",
      "(epoch: 1, iters: 500, time: 0.282, data: 0.001) D_A: 0.131 G_A: 0.328 cycle_A: 1.157 idt_A: 0.580 D_B: 0.318 G_B: 0.340 cycle_B: 1.412 idt_B: 0.322 \n",
      "(epoch: 1, iters: 600, time: 0.277, data: 0.002) D_A: 0.356 G_A: 1.099 cycle_A: 1.778 idt_A: 0.671 D_B: 0.270 G_B: 0.517 cycle_B: 1.211 idt_B: 0.994 \n",
      "(epoch: 1, iters: 700, time: 0.315, data: 0.002) D_A: 0.218 G_A: 0.477 cycle_A: 1.627 idt_A: 0.628 D_B: 0.053 G_B: 0.277 cycle_B: 1.129 idt_B: 0.624 \n",
      "(epoch: 1, iters: 800, time: 0.580, data: 0.001) D_A: 0.104 G_A: 0.703 cycle_A: 1.141 idt_A: 0.664 D_B: 0.167 G_B: 0.249 cycle_B: 1.883 idt_B: 0.450 \n",
      "End of epoch 1 / 200 \t Time Taken: 253 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 2, iters: 4, time: 0.305, data: 0.002) D_A: 0.276 G_A: 0.445 cycle_A: 0.712 idt_A: 0.840 D_B: 0.308 G_B: 1.157 cycle_B: 1.388 idt_B: 0.258 \n",
      "(epoch: 2, iters: 104, time: 0.317, data: 0.002) D_A: 0.071 G_A: 0.363 cycle_A: 1.697 idt_A: 0.810 D_B: 0.238 G_B: 0.449 cycle_B: 1.513 idt_B: 0.476 \n",
      "(epoch: 2, iters: 204, time: 0.310, data: 0.001) D_A: 0.174 G_A: 0.384 cycle_A: 0.856 idt_A: 0.464 D_B: 0.083 G_B: 0.540 cycle_B: 1.241 idt_B: 0.305 \n",
      "(epoch: 2, iters: 304, time: 0.789, data: 0.001) D_A: 0.217 G_A: 0.974 cycle_A: 0.907 idt_A: 0.524 D_B: 0.172 G_B: 0.667 cycle_B: 1.078 idt_B: 0.409 \n",
      "(epoch: 2, iters: 404, time: 0.293, data: 0.002) D_A: 0.236 G_A: 0.285 cycle_A: 0.709 idt_A: 0.627 D_B: 0.144 G_B: 0.407 cycle_B: 1.094 idt_B: 0.296 \n",
      "(epoch: 2, iters: 504, time: 0.284, data: 0.001) D_A: 0.165 G_A: 0.474 cycle_A: 0.597 idt_A: 0.372 D_B: 0.064 G_B: 0.411 cycle_B: 1.031 idt_B: 0.269 \n",
      "(epoch: 2, iters: 604, time: 0.293, data: 0.002) D_A: 0.190 G_A: 0.340 cycle_A: 1.425 idt_A: 0.356 D_B: 0.153 G_B: 0.710 cycle_B: 0.730 idt_B: 0.893 \n",
      "(epoch: 2, iters: 704, time: 0.579, data: 0.001) D_A: 0.140 G_A: 0.740 cycle_A: 1.374 idt_A: 0.316 D_B: 0.085 G_B: 0.480 cycle_B: 0.903 idt_B: 0.353 \n",
      "(epoch: 2, iters: 804, time: 0.316, data: 0.003) D_A: 0.076 G_A: 0.559 cycle_A: 1.045 idt_A: 0.414 D_B: 0.067 G_B: 0.610 cycle_B: 0.708 idt_B: 0.241 \n",
      "End of epoch 2 / 200 \t Time Taken: 256 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 3, iters: 8, time: 0.315, data: 0.002) D_A: 0.144 G_A: 0.513 cycle_A: 0.654 idt_A: 0.425 D_B: 0.190 G_B: 0.647 cycle_B: 0.930 idt_B: 0.274 \n",
      "(epoch: 3, iters: 108, time: 0.306, data: 0.000) D_A: 0.167 G_A: 0.277 cycle_A: 0.874 idt_A: 0.392 D_B: 0.261 G_B: 0.629 cycle_B: 0.767 idt_B: 0.378 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 3, iters: 208, time: 0.998, data: 0.001) D_A: 0.169 G_A: 0.802 cycle_A: 1.016 idt_A: 0.634 D_B: 0.204 G_B: 0.350 cycle_B: 1.223 idt_B: 0.511 \n",
      "(epoch: 3, iters: 308, time: 0.300, data: 0.001) D_A: 0.118 G_A: 0.810 cycle_A: 1.612 idt_A: 0.348 D_B: 0.140 G_B: 0.644 cycle_B: 1.087 idt_B: 0.800 \n",
      "(epoch: 3, iters: 408, time: 0.313, data: 0.001) D_A: 0.217 G_A: 0.222 cycle_A: 1.197 idt_A: 0.477 D_B: 0.118 G_B: 0.215 cycle_B: 1.365 idt_B: 0.488 \n",
      "(epoch: 3, iters: 508, time: 0.315, data: 0.000) D_A: 0.380 G_A: 0.129 cycle_A: 0.739 idt_A: 0.478 D_B: 0.118 G_B: 0.789 cycle_B: 1.144 idt_B: 0.452 \n",
      "(epoch: 3, iters: 608, time: 0.609, data: 0.002) D_A: 0.124 G_A: 0.377 cycle_A: 1.117 idt_A: 0.403 D_B: 0.205 G_B: 0.730 cycle_B: 0.889 idt_B: 0.401 \n",
      "(epoch: 3, iters: 708, time: 0.299, data: 0.002) D_A: 0.322 G_A: 0.123 cycle_A: 0.832 idt_A: 0.540 D_B: 0.082 G_B: 0.965 cycle_B: 1.339 idt_B: 0.265 \n",
      "(epoch: 3, iters: 808, time: 0.319, data: 0.002) D_A: 0.175 G_A: 0.600 cycle_A: 2.317 idt_A: 0.392 D_B: 0.147 G_B: 0.241 cycle_B: 0.931 idt_B: 1.193 \n",
      "End of epoch 3 / 200 \t Time Taken: 257 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 4, iters: 12, time: 0.311, data: 0.002) D_A: 0.111 G_A: 0.498 cycle_A: 0.819 idt_A: 1.991 D_B: 0.048 G_B: 0.734 cycle_B: 3.985 idt_B: 0.265 \n",
      "(epoch: 4, iters: 112, time: 0.996, data: 0.002) D_A: 0.441 G_A: 0.046 cycle_A: 1.015 idt_A: 0.333 D_B: 0.170 G_B: 0.316 cycle_B: 0.770 idt_B: 0.328 \n",
      "(epoch: 4, iters: 212, time: 0.296, data: 0.001) D_A: 0.270 G_A: 0.772 cycle_A: 0.701 idt_A: 0.416 D_B: 0.077 G_B: 0.995 cycle_B: 0.862 idt_B: 0.295 \n",
      "(epoch: 4, iters: 312, time: 0.282, data: 0.001) D_A: 0.226 G_A: 0.209 cycle_A: 0.643 idt_A: 0.434 D_B: 0.111 G_B: 0.398 cycle_B: 0.782 idt_B: 0.270 \n",
      "(epoch: 4, iters: 412, time: 0.316, data: 0.002) D_A: 0.213 G_A: 0.248 cycle_A: 0.528 idt_A: 0.310 D_B: 0.081 G_B: 0.459 cycle_B: 1.001 idt_B: 0.225 \n",
      "(epoch: 4, iters: 512, time: 0.656, data: 0.002) D_A: 0.036 G_A: 0.451 cycle_A: 1.116 idt_A: 0.389 D_B: 0.112 G_B: 0.445 cycle_B: 0.700 idt_B: 0.426 \n",
      "(epoch: 4, iters: 612, time: 0.295, data: 0.002) D_A: 0.077 G_A: 1.026 cycle_A: 0.482 idt_A: 0.324 D_B: 0.083 G_B: 0.421 cycle_B: 0.819 idt_B: 0.164 \n",
      "(epoch: 4, iters: 712, time: 0.318, data: 0.002) D_A: 0.034 G_A: 0.490 cycle_A: 1.113 idt_A: 0.361 D_B: 0.104 G_B: 0.368 cycle_B: 0.597 idt_B: 0.348 \n",
      "(epoch: 4, iters: 812, time: 0.310, data: 0.002) D_A: 0.266 G_A: 0.840 cycle_A: 0.601 idt_A: 0.356 D_B: 0.186 G_B: 0.854 cycle_B: 0.727 idt_B: 0.200 \n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/Aligned/train/ \\\n",
    "--name cycle_b18_40x63xscale_crop_1024_v2 \\\n",
    "--model cycle_gan --lr=0.0001 --lr_decay_iters=40 --lr_policy step --batch_size=1 \\\n",
    "--preprocess scale_width_and_crop --load_size=1024\\\n",
    "--lambda_identity=0.45 --save_epoch_freq=60 --n_epochs=100  --display_port=8087 --gpu_ids=0,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "                augment_a: False                         \n",
      "               batch_size: 1                             \n",
      "               brightness: False                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                 contrast: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/\t[default: None]\n",
      "             dataset_mode: single                        \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                    gauss: False                         \n",
      "                  gpu_ids: 2                             \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 1024                          \t[default: 256]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: test                          \n",
      "             model_suffix:                               \n",
      "               n_layers_D: 3                             \n",
      "                     name: cycle_hela63_scalecrop_1024_v2\t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \t[default: False]\n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 10000                         \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: scale_width                   \t[default: resize_and_crop]\n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                sharpness: False                         \n",
      "                   suffix:                               \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [SingleDataset] was created\n",
      "initialize network with normal\n",
      "model [TestModel] was created\n",
      "loading the model from ./checkpoints/cycle_hela63_scalecrop_1024_v2/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "creating web directory ./results/cycle_hela63_scalecrop_1024_v2/test_latest\n",
      "<data.CustomDatasetDataLoader object at 0x7f83f519fc18>\n",
      "processing (0000)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m112_c1_z0_l1_o0_1.png']\n",
      "processing (0001)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m112_c1_z0_l1_o0_2.png']\n",
      "processing (0002)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m112_c1_z0_l1_o0_3.png']\n",
      "processing (0003)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m141_c1_z0_l1_o0_1.png']\n",
      "processing (0004)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m141_c1_z0_l1_o0_2.png']\n",
      "processing (0005)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m141_c1_z0_l1_o0_3.png']\n",
      "processing (0006)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m151_c1_z0_l1_o0_1.png']\n",
      "processing (0007)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m151_c1_z0_l1_o0_2.png']\n",
      "processing (0008)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m151_c1_z0_l1_o0_3.png']\n",
      "processing (0009)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m192_c1_z0_l1_o0_1.png']\n",
      "processing (0010)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m192_c1_z0_l1_o0_2.png']\n",
      "processing (0011)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m192_c1_z0_l1_o0_3.png']\n",
      "processing (0012)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m212_c1_z0_l1_o0_1.png']\n",
      "processing (0013)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m212_c1_z0_l1_o0_2.png']\n",
      "processing (0014)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m212_c1_z0_l1_o0_3.png']\n",
      "processing (0015)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m222_c1_z0_l1_o0_1.png']\n",
      "processing (0016)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m222_c1_z0_l1_o0_2.png']\n",
      "processing (0017)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m222_c1_z0_l1_o0_3.png']\n",
      "processing (0018)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m22_c1_z0_l1_o0_1.png']\n",
      "processing (0019)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m22_c1_z0_l1_o0_2.png']\n",
      "processing (0020)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m22_c1_z0_l1_o0_3.png']\n",
      "processing (0021)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m260_c1_z0_l1_o0_1.png']\n",
      "processing (0022)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m260_c1_z0_l1_o0_2.png']\n",
      "processing (0023)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m260_c1_z0_l1_o0_3.png']\n",
      "processing (0024)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m27_c1_z0_l1_o0_1.png']\n",
      "processing (0025)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m27_c1_z0_l1_o0_2.png']\n",
      "processing (0026)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m27_c1_z0_l1_o0_3.png']\n",
      "processing (0027)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m281_c1_z0_l1_o0_1.png']\n",
      "processing (0028)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m281_c1_z0_l1_o0_2.png']\n",
      "processing (0029)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m281_c1_z0_l1_o0_3.png']\n",
      "processing (0030)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m290_c1_z0_l1_o0_1.png']\n",
      "processing (0031)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m290_c1_z0_l1_o0_2.png']\n",
      "processing (0032)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m290_c1_z0_l1_o0_3.png']\n",
      "processing (0033)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m322_c1_z0_l1_o0_1.png']\n",
      "processing (0034)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m322_c1_z0_l1_o0_2.png']\n",
      "processing (0035)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m322_c1_z0_l1_o0_3.png']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing (0036)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m323_c1_z0_l1_o0_1.png']\n",
      "processing (0037)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m323_c1_z0_l1_o0_2.png']\n",
      "processing (0038)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m323_c1_z0_l1_o0_3.png']\n",
      "processing (0039)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m332_c1_z0_l1_o0_1.png']\n",
      "processing (0040)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m332_c1_z0_l1_o0_2.png']\n",
      "processing (0041)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m332_c1_z0_l1_o0_3.png']\n",
      "processing (0042)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m333_c1_z0_l1_o0_1.png']\n",
      "processing (0043)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m333_c1_z0_l1_o0_2.png']\n",
      "processing (0044)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m333_c1_z0_l1_o0_3.png']\n",
      "processing (0045)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m37_c1_z0_l1_o0_1.png']\n",
      "processing (0046)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m37_c1_z0_l1_o0_2.png']\n",
      "processing (0047)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m37_c1_z0_l1_o0_3.png']\n",
      "processing (0048)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m60_c1_z0_l1_o0_1.png']\n",
      "processing (0049)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m60_c1_z0_l1_o0_2.png']\n",
      "processing (0050)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m60_c1_z0_l1_o0_3.png']\n",
      "processing (0051)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m63_c1_z0_l1_o0_1.png']\n",
      "processing (0052)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m63_c1_z0_l1_o0_2.png']\n",
      "processing (0053)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m63_c1_z0_l1_o0_3.png']\n",
      "processing (0054)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m72_c1_z0_l1_o0_1.png']\n",
      "processing (0055)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m72_c1_z0_l1_o0_2.png']\n",
      "processing (0056)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m72_c1_z0_l1_o0_3.png']\n",
      "processing (0057)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m84_c1_z0_l1_o0_1.png']\n",
      "processing (0058)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m84_c1_z0_l1_o0_2.png']\n",
      "processing (0059)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m84_c1_z0_l1_o0_3.png']\n",
      "processing (0060)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m87_c1_z0_l1_o0_1.png']\n",
      "processing (0061)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m87_c1_z0_l1_o0_2.png']\n",
      "processing (0062)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m87_c1_z0_l1_o0_3.png']\n",
      "processing (0063)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m94_c1_z0_l1_o0_1.png']\n",
      "processing (0064)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m94_c1_z0_l1_o0_2.png']\n",
      "processing (0065)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m94_c1_z0_l1_o0_3.png']\n",
      "processing (0066)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m9_c1_z0_l1_o0_1.png']\n",
      "processing (0067)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m9_c1_z0_l1_o0_2.png']\n",
      "processing (0068)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m9_c1_z0_l1_o0_3.png']\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot  /storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/ \\\n",
    "--name cycle_hela63_scalecrop_1024_v2 --model test \\\n",
    "--no_dropout --preprocess scale_width --load_size 1024 --gpu_ids=2 --num_test=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unaligned training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                 CUT_mode: CUT                           \n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "                    beta2: 0.999                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Unaligned/train/\t[default: placeholder]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: None                          \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8087                          \t[default: 8097]\n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "               easy_label: experiment_name               \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "          evaluation_freq: 5000                          \n",
      "        flip_equivariance: False                         \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: xavier                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "               lambda_GAN: 1.0                           \n",
      "               lambda_NCE: 1.0                           \n",
      "                load_size: 256                           \t[default: 286]\n",
      "                       lr: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 40                            \t[default: 50]\n",
      "                lr_policy: step                          \t[default: linear]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: cut                           \n",
      "                 n_epochs: 150                           \t[default: 200]\n",
      "           n_epochs_decay: 200                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: cut_b18z40_63x_unaligned_resize\t[default: experiment_name]\n",
      "                    nce_T: 0.07                          \n",
      "                  nce_idt: True                          \n",
      "nce_includes_all_negatives_from_minibatch: False                         \n",
      "               nce_layers: 0,4,8,12,16                   \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netF: mlp_sample                    \n",
      "                  netF_nc: 256                           \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "             no_antialias: False                         \n",
      "          no_antialias_up: False                         \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                    normD: instance                      \n",
      "                    normG: instance                      \n",
      "              num_patches: 256                           \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize                        \t[default: resize_and_crop]\n",
      "          pretrained_name: None                          \n",
      "               print_freq: 100                           \n",
      "         random_scale_max: 3.0                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 60                            \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "stylegan2_G_num_downsampling: 1                             \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "model [CUTModel] was created\n",
      "The number of training images = 333\n",
      "Setting up a new session...\n",
      "Exception in user code:\n",
      "------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 170, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1285, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1331, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1280, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1046, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 984, in send\n",
      "    self.connect()\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 182, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f682e96ea90>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8087): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f682e96ea90>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/visdom/__init__.py\", line 711, in _send\n",
      "    data=json.dumps(msg),\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/visdom/__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8087): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f682e96ea90>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "[Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Could not connect to Visdom server. \n",
      " Trying to start a server....\n",
      "Command: /storage01/grexai/dev/envs/pix2pix/bin/python -m visdom.server -p 8087 &>/dev/null &\n",
      "create web directory ./checkpoints/cut_b18z40_63x_unaligned_resize/web...\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "[Network F] Total number of parameters : 0.560 M\n",
      "[Network D] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "(epoch: 1, iters: 100, time: 0.188, data: 0.361) G_GAN: 0.268 D_real: 0.202 D_fake: 0.293 G: 3.086 NCE: 2.997 NCE_Y: 2.639 \n",
      "(epoch: 1, iters: 200, time: 0.227, data: 0.005) G_GAN: 0.274 D_real: 0.242 D_fake: 0.243 G: 2.195 NCE: 2.118 NCE_Y: 1.723 \n",
      "(epoch: 1, iters: 300, time: 0.256, data: 0.001) G_GAN: 0.306 D_real: 0.199 D_fake: 0.249 G: 2.148 NCE: 2.034 NCE_Y: 1.651 \n",
      "End of epoch 1 / 350 \t Time Taken: 102 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 2, iters: 67, time: 0.270, data: 0.002) G_GAN: 0.279 D_real: 0.257 D_fake: 0.234 G: 1.816 NCE: 1.679 NCE_Y: 1.395 \n",
      "(epoch: 2, iters: 167, time: 0.278, data: 0.002) G_GAN: 0.283 D_real: 0.229 D_fake: 0.274 G: 1.754 NCE: 1.485 NCE_Y: 1.456 \n",
      "(epoch: 2, iters: 267, time: 0.284, data: 0.002) G_GAN: 0.297 D_real: 0.198 D_fake: 0.262 G: 6.788 NCE: 5.702 NCE_Y: 7.281 \n",
      "End of epoch 2 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 3, iters: 34, time: 0.287, data: 0.002) G_GAN: 0.253 D_real: 0.259 D_fake: 0.267 G: 1.731 NCE: 1.617 NCE_Y: 1.340 \n",
      "(epoch: 3, iters: 134, time: 0.292, data: 0.002) G_GAN: 0.971 D_real: 0.334 D_fake: 0.066 G: 2.664 NCE: 1.688 NCE_Y: 1.699 \n",
      "(epoch: 3, iters: 234, time: 0.292, data: 0.002) G_GAN: 0.382 D_real: 0.190 D_fake: 0.249 G: 1.603 NCE: 1.306 NCE_Y: 1.136 \n",
      "End of epoch 3 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 4, iters: 1, time: 0.294, data: 0.002) G_GAN: 1.080 D_real: 0.051 D_fake: 0.200 G: 2.725 NCE: 1.709 NCE_Y: 1.582 \n",
      "(epoch: 4, iters: 101, time: 0.295, data: 0.001) G_GAN: 1.204 D_real: 0.611 D_fake: 0.168 G: 3.017 NCE: 2.181 NCE_Y: 1.444 \n",
      "(epoch: 4, iters: 201, time: 0.296, data: 0.002) G_GAN: 0.633 D_real: 0.050 D_fake: 0.051 G: 2.025 NCE: 1.561 NCE_Y: 1.224 \n",
      "(epoch: 4, iters: 301, time: 0.296, data: 0.002) G_GAN: 0.843 D_real: 0.352 D_fake: 0.021 G: 2.491 NCE: 1.777 NCE_Y: 1.520 \n",
      "End of epoch 4 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 5, iters: 68, time: 0.290, data: 0.002) G_GAN: 0.297 D_real: 0.203 D_fake: 0.251 G: 1.571 NCE: 1.377 NCE_Y: 1.172 \n",
      "(epoch: 5, iters: 168, time: 0.293, data: 0.001) G_GAN: 0.308 D_real: 0.262 D_fake: 0.215 G: 1.483 NCE: 1.322 NCE_Y: 1.028 \n",
      "(epoch: 5, iters: 268, time: 0.293, data: 0.001) G_GAN: 0.339 D_real: 0.172 D_fake: 0.266 G: 1.554 NCE: 1.292 NCE_Y: 1.137 \n",
      "End of epoch 5 / 350 \t Time Taken: 97 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 6, iters: 35, time: 0.288, data: 0.003) G_GAN: 0.311 D_real: 0.205 D_fake: 0.246 G: 1.575 NCE: 1.321 NCE_Y: 1.207 \n",
      "(epoch: 6, iters: 135, time: 0.289, data: 0.002) G_GAN: 0.311 D_real: 0.164 D_fake: 0.273 G: 1.632 NCE: 1.363 NCE_Y: 1.279 \n",
      "(epoch: 6, iters: 235, time: 0.290, data: 0.002) G_GAN: 0.273 D_real: 0.272 D_fake: 0.243 G: 1.389 NCE: 1.274 NCE_Y: 0.959 \n",
      "End of epoch 6 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 7, iters: 2, time: 0.292, data: 0.002) G_GAN: 0.267 D_real: 0.232 D_fake: 0.230 G: 1.536 NCE: 1.481 NCE_Y: 1.056 \n",
      "(epoch: 7, iters: 102, time: 0.292, data: 0.001) G_GAN: 0.271 D_real: 0.224 D_fake: 0.230 G: 1.570 NCE: 1.311 NCE_Y: 1.286 \n",
      "(epoch: 7, iters: 202, time: 0.293, data: 0.002) G_GAN: 0.310 D_real: 0.228 D_fake: 0.289 G: 1.418 NCE: 1.242 NCE_Y: 0.973 \n",
      "(epoch: 7, iters: 302, time: 0.293, data: 0.002) G_GAN: 0.366 D_real: 0.285 D_fake: 0.240 G: 1.502 NCE: 1.317 NCE_Y: 0.955 \n",
      "End of epoch 7 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 8, iters: 69, time: 0.293, data: 0.002) G_GAN: 0.288 D_real: 0.282 D_fake: 0.254 G: 1.407 NCE: 1.256 NCE_Y: 0.982 \n",
      "(epoch: 8, iters: 169, time: 0.292, data: 0.007) G_GAN: 0.381 D_real: 0.053 D_fake: 0.319 G: 1.811 NCE: 1.598 NCE_Y: 1.262 \n",
      "(epoch: 8, iters: 269, time: 0.293, data: 0.002) G_GAN: 0.250 D_real: 0.207 D_fake: 0.301 G: 1.344 NCE: 1.241 NCE_Y: 0.946 \n",
      "End of epoch 8 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 9, iters: 36, time: 0.293, data: 0.002) G_GAN: 0.359 D_real: 0.230 D_fake: 0.239 G: 1.557 NCE: 1.304 NCE_Y: 1.092 \n",
      "(epoch: 9, iters: 136, time: 0.294, data: 0.002) G_GAN: 0.344 D_real: 0.192 D_fake: 0.273 G: 1.468 NCE: 1.255 NCE_Y: 0.993 \n",
      "(epoch: 9, iters: 236, time: 0.295, data: 0.002) G_GAN: 0.494 D_real: 0.339 D_fake: 0.170 G: 1.894 NCE: 1.763 NCE_Y: 1.038 \n",
      "End of epoch 9 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 10, iters: 3, time: 0.296, data: 0.002) G_GAN: 0.457 D_real: 0.461 D_fake: 0.398 G: 1.953 NCE: 1.889 NCE_Y: 1.103 \n",
      "(epoch: 10, iters: 103, time: 0.294, data: 0.002) G_GAN: 0.401 D_real: 0.199 D_fake: 0.236 G: 1.510 NCE: 1.231 NCE_Y: 0.986 \n",
      "(epoch: 10, iters: 203, time: 0.295, data: 0.002) G_GAN: 0.357 D_real: 0.425 D_fake: 0.198 G: 1.498 NCE: 1.227 NCE_Y: 1.056 \n",
      "(epoch: 10, iters: 303, time: 0.295, data: 0.002) G_GAN: 0.290 D_real: 0.592 D_fake: 0.171 G: 1.538 NCE: 1.378 NCE_Y: 1.117 \n",
      "End of epoch 10 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 11, iters: 70, time: 0.294, data: 0.002) G_GAN: 0.278 D_real: 0.245 D_fake: 0.221 G: 1.342 NCE: 1.197 NCE_Y: 0.931 \n",
      "(epoch: 11, iters: 170, time: 0.294, data: 0.002) G_GAN: 0.318 D_real: 0.270 D_fake: 0.175 G: 1.535 NCE: 1.471 NCE_Y: 0.963 \n",
      "(epoch: 11, iters: 270, time: 0.293, data: 0.002) G_GAN: 0.320 D_real: 0.548 D_fake: 0.135 G: 1.549 NCE: 1.401 NCE_Y: 1.057 \n",
      "End of epoch 11 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 12, iters: 37, time: 0.295, data: 0.007) G_GAN: 0.352 D_real: 0.054 D_fake: 0.436 G: 1.483 NCE: 1.261 NCE_Y: 1.001 \n",
      "(epoch: 12, iters: 137, time: 0.294, data: 0.002) G_GAN: 0.530 D_real: 0.023 D_fake: 0.256 G: 2.000 NCE: 1.189 NCE_Y: 1.751 \n",
      "(epoch: 12, iters: 237, time: 0.295, data: 0.002) G_GAN: 0.262 D_real: 0.346 D_fake: 0.163 G: 1.391 NCE: 1.302 NCE_Y: 0.956 \n",
      "End of epoch 12 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 13, iters: 4, time: 0.295, data: 0.002) G_GAN: 0.404 D_real: 0.224 D_fake: 0.211 G: 1.548 NCE: 1.362 NCE_Y: 0.926 \n",
      "(epoch: 13, iters: 104, time: 0.295, data: 0.004) G_GAN: 0.461 D_real: 0.159 D_fake: 0.133 G: 1.627 NCE: 1.300 NCE_Y: 1.031 \n",
      "(epoch: 13, iters: 204, time: 0.295, data: 0.002) G_GAN: 0.460 D_real: 0.130 D_fake: 0.315 G: 1.592 NCE: 1.162 NCE_Y: 1.101 \n",
      "(epoch: 13, iters: 304, time: 0.295, data: 0.002) G_GAN: 0.299 D_real: 0.245 D_fake: 0.289 G: 1.376 NCE: 1.267 NCE_Y: 0.886 \n",
      "End of epoch 13 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 14, iters: 71, time: 0.295, data: 0.002) G_GAN: 0.266 D_real: 0.308 D_fake: 0.235 G: 1.352 NCE: 1.199 NCE_Y: 0.973 \n",
      "(epoch: 14, iters: 171, time: 0.295, data: 0.002) G_GAN: 0.357 D_real: 0.182 D_fake: 0.212 G: 1.465 NCE: 1.217 NCE_Y: 0.998 \n",
      "(epoch: 14, iters: 271, time: 0.296, data: 0.002) G_GAN: 0.342 D_real: 0.173 D_fake: 0.229 G: 1.458 NCE: 1.237 NCE_Y: 0.996 \n",
      "End of epoch 14 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 15, iters: 38, time: 0.296, data: 0.002) G_GAN: 0.263 D_real: 0.252 D_fake: 0.283 G: 1.295 NCE: 1.161 NCE_Y: 0.903 \n",
      "(epoch: 15, iters: 138, time: 0.297, data: 0.002) G_GAN: 0.333 D_real: 0.262 D_fake: 0.195 G: 1.348 NCE: 1.126 NCE_Y: 0.903 \n",
      "(epoch: 15, iters: 238, time: 0.296, data: 0.002) G_GAN: 0.416 D_real: 0.059 D_fake: 0.217 G: 1.585 NCE: 1.239 NCE_Y: 1.098 \n",
      "End of epoch 15 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 16, iters: 5, time: 0.297, data: 0.002) G_GAN: 0.253 D_real: 0.306 D_fake: 0.215 G: 1.345 NCE: 1.199 NCE_Y: 0.985 \n",
      "saving the latest model (epoch 16, total_iters 5000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 16, iters: 105, time: 0.296, data: 0.001) G_GAN: 0.587 D_real: 0.067 D_fake: 0.105 G: 1.799 NCE: 1.202 NCE_Y: 1.224 \n",
      "(epoch: 16, iters: 205, time: 0.296, data: 0.001) G_GAN: 0.289 D_real: 0.463 D_fake: 0.216 G: 1.435 NCE: 1.292 NCE_Y: 1.000 \n",
      "(epoch: 16, iters: 305, time: 0.296, data: 0.002) G_GAN: 0.626 D_real: 0.051 D_fake: 0.082 G: 1.762 NCE: 1.233 NCE_Y: 1.038 \n",
      "End of epoch 16 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 17, iters: 72, time: 0.296, data: 0.002) G_GAN: 0.219 D_real: 0.436 D_fake: 0.122 G: 1.414 NCE: 1.317 NCE_Y: 1.074 \n",
      "(epoch: 17, iters: 172, time: 0.297, data: 0.001) G_GAN: 0.877 D_real: 0.050 D_fake: 0.063 G: 2.135 NCE: 1.250 NCE_Y: 1.266 \n",
      "(epoch: 17, iters: 272, time: 0.297, data: 0.007) G_GAN: 0.257 D_real: 0.282 D_fake: 0.304 G: 1.323 NCE: 1.175 NCE_Y: 0.957 \n",
      "End of epoch 17 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 18, iters: 39, time: 0.297, data: 0.002) G_GAN: 0.250 D_real: 0.238 D_fake: 0.291 G: 1.287 NCE: 1.174 NCE_Y: 0.900 \n",
      "(epoch: 18, iters: 139, time: 0.298, data: 0.002) G_GAN: 0.385 D_real: 0.024 D_fake: 0.281 G: 1.745 NCE: 1.246 NCE_Y: 1.474 \n",
      "(epoch: 18, iters: 239, time: 0.297, data: 0.002) G_GAN: 0.305 D_real: 0.285 D_fake: 0.192 G: 1.323 NCE: 1.142 NCE_Y: 0.892 \n",
      "End of epoch 18 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 19, iters: 6, time: 0.297, data: 0.001) G_GAN: 0.244 D_real: 0.219 D_fake: 0.277 G: 1.321 NCE: 1.199 NCE_Y: 0.955 \n",
      "(epoch: 19, iters: 106, time: 0.296, data: 0.001) G_GAN: 0.253 D_real: 0.236 D_fake: 0.406 G: 1.370 NCE: 1.287 NCE_Y: 0.946 \n",
      "(epoch: 19, iters: 206, time: 0.297, data: 0.001) G_GAN: 0.329 D_real: 0.421 D_fake: 0.185 G: 1.451 NCE: 1.184 NCE_Y: 1.059 \n",
      "(epoch: 19, iters: 306, time: 0.297, data: 0.001) G_GAN: 0.257 D_real: 0.331 D_fake: 0.269 G: 1.316 NCE: 1.175 NCE_Y: 0.942 \n",
      "End of epoch 19 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 20, iters: 73, time: 0.296, data: 0.002) G_GAN: 0.383 D_real: 0.260 D_fake: 0.160 G: 1.473 NCE: 1.190 NCE_Y: 0.991 \n",
      "(epoch: 20, iters: 173, time: 0.297, data: 0.002) G_GAN: 0.288 D_real: 0.423 D_fake: 0.154 G: 1.350 NCE: 1.174 NCE_Y: 0.950 \n",
      "(epoch: 20, iters: 273, time: 0.295, data: 0.002) G_GAN: 0.680 D_real: 0.126 D_fake: 0.233 G: 1.780 NCE: 1.173 NCE_Y: 1.026 \n",
      "End of epoch 20 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 21, iters: 40, time: 0.296, data: 0.002) G_GAN: 0.355 D_real: 0.219 D_fake: 0.227 G: 1.459 NCE: 1.161 NCE_Y: 1.046 \n",
      "(epoch: 21, iters: 140, time: 0.295, data: 0.002) G_GAN: 0.571 D_real: 0.316 D_fake: 0.222 G: 1.678 NCE: 1.172 NCE_Y: 1.043 \n",
      "(epoch: 21, iters: 240, time: 0.296, data: 0.002) G_GAN: 0.279 D_real: 0.229 D_fake: 0.256 G: 1.312 NCE: 1.158 NCE_Y: 0.907 \n",
      "End of epoch 21 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 22, iters: 7, time: 0.296, data: 0.003) G_GAN: 0.297 D_real: 0.276 D_fake: 0.297 G: 1.358 NCE: 1.203 NCE_Y: 0.918 \n",
      "(epoch: 22, iters: 107, time: 0.297, data: 0.000) G_GAN: 0.339 D_real: 0.329 D_fake: 0.179 G: 1.653 NCE: 1.656 NCE_Y: 0.972 \n",
      "(epoch: 22, iters: 207, time: 0.296, data: 0.003) G_GAN: 0.242 D_real: 0.273 D_fake: 0.506 G: 1.326 NCE: 1.193 NCE_Y: 0.974 \n",
      "(epoch: 22, iters: 307, time: 0.296, data: 0.001) G_GAN: 0.331 D_real: 0.298 D_fake: 0.205 G: 1.377 NCE: 1.172 NCE_Y: 0.919 \n",
      "End of epoch 22 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 23, iters: 74, time: 0.295, data: 0.002) G_GAN: 0.310 D_real: 0.344 D_fake: 0.171 G: 1.444 NCE: 1.323 NCE_Y: 0.945 \n",
      "(epoch: 23, iters: 174, time: 0.297, data: 0.002) G_GAN: 0.267 D_real: 0.246 D_fake: 0.315 G: 1.303 NCE: 1.163 NCE_Y: 0.910 \n",
      "(epoch: 23, iters: 274, time: 0.297, data: 0.002) G_GAN: 0.335 D_real: 0.268 D_fake: 0.188 G: 1.473 NCE: 1.319 NCE_Y: 0.957 \n",
      "End of epoch 23 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 24, iters: 41, time: 0.295, data: 0.003) G_GAN: 0.250 D_real: 0.153 D_fake: 0.302 G: 1.377 NCE: 1.319 NCE_Y: 0.935 \n",
      "(epoch: 24, iters: 141, time: 0.295, data: 0.002) G_GAN: 0.264 D_real: 0.231 D_fake: 0.261 G: 1.290 NCE: 1.163 NCE_Y: 0.890 \n",
      "(epoch: 24, iters: 241, time: 0.294, data: 0.002) G_GAN: 0.263 D_real: 0.242 D_fake: 0.268 G: 1.299 NCE: 1.170 NCE_Y: 0.902 \n",
      "End of epoch 24 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 25, iters: 8, time: 0.292, data: 0.001) G_GAN: 0.374 D_real: 0.166 D_fake: 0.194 G: 1.503 NCE: 1.217 NCE_Y: 1.042 \n",
      "(epoch: 25, iters: 108, time: 0.294, data: 0.002) G_GAN: 0.349 D_real: 0.392 D_fake: 0.158 G: 1.369 NCE: 1.152 NCE_Y: 0.886 \n",
      "(epoch: 25, iters: 208, time: 0.293, data: 0.002) G_GAN: 0.270 D_real: 0.243 D_fake: 0.260 G: 1.285 NCE: 1.157 NCE_Y: 0.873 \n",
      "(epoch: 25, iters: 308, time: 0.293, data: 0.002) G_GAN: 0.342 D_real: 0.053 D_fake: 0.286 G: 1.395 NCE: 1.173 NCE_Y: 0.934 \n",
      "End of epoch 25 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 26, iters: 75, time: 0.295, data: 0.001) G_GAN: 0.383 D_real: 0.318 D_fake: 0.150 G: 1.452 NCE: 1.177 NCE_Y: 0.959 \n",
      "(epoch: 26, iters: 175, time: 0.294, data: 0.002) G_GAN: 0.344 D_real: 0.236 D_fake: 0.184 G: 1.355 NCE: 1.152 NCE_Y: 0.871 \n",
      "(epoch: 26, iters: 275, time: 0.296, data: 0.001) G_GAN: 0.483 D_real: 0.006 D_fake: 0.216 G: 1.589 NCE: 1.216 NCE_Y: 0.994 \n",
      "End of epoch 26 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 27, iters: 42, time: 0.296, data: 0.002) G_GAN: 0.303 D_real: 0.186 D_fake: 0.298 G: 1.362 NCE: 1.187 NCE_Y: 0.932 \n",
      "(epoch: 27, iters: 142, time: 0.296, data: 0.002) G_GAN: 0.219 D_real: 0.302 D_fake: 0.345 G: 1.252 NCE: 1.213 NCE_Y: 0.852 \n",
      "(epoch: 27, iters: 242, time: 0.296, data: 0.002) G_GAN: 0.287 D_real: 0.218 D_fake: 0.232 G: 1.344 NCE: 1.191 NCE_Y: 0.922 \n",
      "End of epoch 27 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 28, iters: 9, time: 0.294, data: 0.001) G_GAN: 0.399 D_real: 0.361 D_fake: 0.150 G: 1.739 NCE: 1.371 NCE_Y: 1.307 \n",
      "(epoch: 28, iters: 109, time: 0.297, data: 0.002) G_GAN: 0.291 D_real: 0.296 D_fake: 0.247 G: 1.362 NCE: 1.229 NCE_Y: 0.914 \n",
      "(epoch: 28, iters: 209, time: 0.296, data: 0.002) G_GAN: 0.458 D_real: 0.038 D_fake: 0.161 G: 1.642 NCE: 1.234 NCE_Y: 1.135 \n",
      "(epoch: 28, iters: 309, time: 0.296, data: 0.001) G_GAN: 0.279 D_real: 0.263 D_fake: 0.222 G: 1.316 NCE: 1.173 NCE_Y: 0.901 \n",
      "End of epoch 28 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 29, iters: 76, time: 0.298, data: 0.002) G_GAN: 0.280 D_real: 0.379 D_fake: 0.182 G: 1.393 NCE: 1.267 NCE_Y: 0.960 \n",
      "(epoch: 29, iters: 176, time: 0.296, data: 0.002) G_GAN: 0.324 D_real: 0.153 D_fake: 0.324 G: 1.401 NCE: 1.240 NCE_Y: 0.914 \n",
      "(epoch: 29, iters: 276, time: 0.297, data: 0.002) G_GAN: 0.361 D_real: 0.270 D_fake: 0.175 G: 1.418 NCE: 1.238 NCE_Y: 0.875 \n",
      "End of epoch 29 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 30, iters: 43, time: 0.294, data: 0.002) G_GAN: 0.356 D_real: 0.063 D_fake: 0.261 G: 1.462 NCE: 1.180 NCE_Y: 1.032 \n",
      "(epoch: 30, iters: 143, time: 0.295, data: 0.002) G_GAN: 0.263 D_real: 0.282 D_fake: 0.237 G: 1.311 NCE: 1.176 NCE_Y: 0.921 \n",
      "(epoch: 30, iters: 243, time: 0.297, data: 0.002) G_GAN: 0.285 D_real: 0.210 D_fake: 0.217 G: 1.325 NCE: 1.155 NCE_Y: 0.926 \n",
      "End of epoch 30 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 31, iters: 10, time: 0.295, data: 0.002) G_GAN: 0.670 D_real: 0.164 D_fake: 0.049 G: 1.773 NCE: 1.183 NCE_Y: 1.024 \n",
      "saving the latest model (epoch 31, total_iters 10000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 31, iters: 110, time: 0.297, data: 0.002) G_GAN: 0.271 D_real: 0.262 D_fake: 0.241 G: 1.306 NCE: 1.170 NCE_Y: 0.901 \n",
      "(epoch: 31, iters: 210, time: 0.296, data: 0.002) G_GAN: 0.362 D_real: 0.112 D_fake: 0.231 G: 1.414 NCE: 1.153 NCE_Y: 0.951 \n",
      "(epoch: 31, iters: 310, time: 0.297, data: 0.002) G_GAN: 0.495 D_real: 0.339 D_fake: 0.100 G: 1.604 NCE: 1.193 NCE_Y: 1.025 \n",
      "End of epoch 31 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 32, iters: 77, time: 0.296, data: 0.002) G_GAN: 0.274 D_real: 0.344 D_fake: 0.203 G: 1.372 NCE: 1.236 NCE_Y: 0.960 \n",
      "(epoch: 32, iters: 177, time: 0.296, data: 0.002) G_GAN: 0.604 D_real: 0.073 D_fake: 0.254 G: 1.736 NCE: 1.167 NCE_Y: 1.098 \n",
      "(epoch: 32, iters: 277, time: 0.297, data: 0.005) G_GAN: 0.405 D_real: 0.278 D_fake: 0.295 G: 1.444 NCE: 1.137 NCE_Y: 0.941 \n",
      "End of epoch 32 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 33, iters: 44, time: 0.296, data: 0.002) G_GAN: 0.266 D_real: 0.256 D_fake: 0.258 G: 1.355 NCE: 1.256 NCE_Y: 0.921 \n",
      "(epoch: 33, iters: 144, time: 0.296, data: 0.002) G_GAN: 0.353 D_real: 0.348 D_fake: 0.182 G: 1.453 NCE: 1.215 NCE_Y: 0.986 \n",
      "(epoch: 33, iters: 244, time: 0.295, data: 0.002) G_GAN: 0.367 D_real: 0.267 D_fake: 0.208 G: 1.400 NCE: 1.176 NCE_Y: 0.891 \n",
      "End of epoch 33 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 34, iters: 11, time: 0.296, data: 0.002) G_GAN: 0.399 D_real: 0.518 D_fake: 0.110 G: 1.433 NCE: 1.153 NCE_Y: 0.913 \n",
      "(epoch: 34, iters: 111, time: 0.296, data: 0.001) G_GAN: 0.414 D_real: 0.075 D_fake: 0.190 G: 1.499 NCE: 1.171 NCE_Y: 0.999 \n",
      "(epoch: 34, iters: 211, time: 0.296, data: 0.002) G_GAN: 0.618 D_real: 0.244 D_fake: 0.257 G: 1.685 NCE: 1.181 NCE_Y: 0.954 \n",
      "(epoch: 34, iters: 311, time: 0.298, data: 0.001) G_GAN: 0.345 D_real: 0.366 D_fake: 0.181 G: 1.367 NCE: 1.144 NCE_Y: 0.901 \n",
      "End of epoch 34 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 35, iters: 78, time: 0.298, data: 0.002) G_GAN: 0.300 D_real: 0.327 D_fake: 0.210 G: 1.420 NCE: 1.350 NCE_Y: 0.891 \n",
      "(epoch: 35, iters: 178, time: 0.297, data: 0.002) G_GAN: 0.890 D_real: 0.264 D_fake: 0.518 G: 2.025 NCE: 1.320 NCE_Y: 0.950 \n",
      "(epoch: 35, iters: 278, time: 0.297, data: 0.003) G_GAN: 0.390 D_real: 0.371 D_fake: 0.127 G: 1.445 NCE: 1.207 NCE_Y: 0.903 \n",
      "End of epoch 35 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 36, iters: 45, time: 0.294, data: 0.002) G_GAN: 0.346 D_real: 0.440 D_fake: 0.140 G: 1.411 NCE: 1.179 NCE_Y: 0.952 \n",
      "(epoch: 36, iters: 145, time: 0.295, data: 0.001) G_GAN: 0.372 D_real: 0.389 D_fake: 0.151 G: 1.508 NCE: 1.323 NCE_Y: 0.948 \n",
      "(epoch: 36, iters: 245, time: 0.296, data: 0.002) G_GAN: 0.305 D_real: 0.461 D_fake: 0.142 G: 1.364 NCE: 1.175 NCE_Y: 0.945 \n",
      "End of epoch 36 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 37, iters: 12, time: 0.295, data: 0.002) G_GAN: 0.278 D_real: 0.333 D_fake: 0.247 G: 1.372 NCE: 1.202 NCE_Y: 0.986 \n",
      "(epoch: 37, iters: 112, time: 0.295, data: 0.002) G_GAN: 0.535 D_real: 0.057 D_fake: 0.310 G: 1.554 NCE: 1.111 NCE_Y: 0.928 \n",
      "(epoch: 37, iters: 212, time: 0.295, data: 0.002) G_GAN: 0.271 D_real: 0.182 D_fake: 0.272 G: 1.279 NCE: 1.122 NCE_Y: 0.892 \n",
      "(epoch: 37, iters: 312, time: 0.295, data: 0.002) G_GAN: 0.424 D_real: 0.019 D_fake: 0.313 G: 1.572 NCE: 1.155 NCE_Y: 1.142 \n",
      "End of epoch 37 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 38, iters: 79, time: 0.296, data: 0.001) G_GAN: 0.381 D_real: 0.400 D_fake: 0.154 G: 1.411 NCE: 1.165 NCE_Y: 0.893 \n",
      "(epoch: 38, iters: 179, time: 0.295, data: 0.001) G_GAN: 0.339 D_real: 0.277 D_fake: 0.207 G: 1.394 NCE: 1.202 NCE_Y: 0.910 \n",
      "(epoch: 38, iters: 279, time: 0.296, data: 0.002) G_GAN: 0.458 D_real: 0.192 D_fake: 0.369 G: 1.476 NCE: 1.132 NCE_Y: 0.904 \n",
      "End of epoch 38 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 39, iters: 46, time: 0.295, data: 0.002) G_GAN: 0.354 D_real: 0.174 D_fake: 0.255 G: 1.477 NCE: 1.236 NCE_Y: 1.009 \n",
      "(epoch: 39, iters: 146, time: 0.296, data: 0.002) G_GAN: 0.368 D_real: 0.199 D_fake: 0.533 G: 1.440 NCE: 1.243 NCE_Y: 0.901 \n",
      "(epoch: 39, iters: 246, time: 0.296, data: 0.001) G_GAN: 0.387 D_real: 0.268 D_fake: 0.158 G: 1.509 NCE: 1.286 NCE_Y: 0.959 \n",
      "End of epoch 39 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0001000\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 40, iters: 13, time: 0.295, data: 0.002) G_GAN: 0.389 D_real: 0.037 D_fake: 0.386 G: 1.428 NCE: 1.143 NCE_Y: 0.936 \n",
      "(epoch: 40, iters: 113, time: 0.294, data: 0.002) G_GAN: 0.326 D_real: 0.238 D_fake: 0.430 G: 1.318 NCE: 1.116 NCE_Y: 0.867 \n",
      "(epoch: 40, iters: 213, time: 0.295, data: 0.001) G_GAN: 0.359 D_real: 0.218 D_fake: 0.183 G: 1.381 NCE: 1.137 NCE_Y: 0.908 \n",
      "(epoch: 40, iters: 313, time: 0.294, data: 0.002) G_GAN: 0.256 D_real: 0.240 D_fake: 0.262 G: 1.274 NCE: 1.143 NCE_Y: 0.894 \n",
      "End of epoch 40 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 41, iters: 80, time: 0.293, data: 0.001) G_GAN: 0.354 D_real: 0.315 D_fake: 0.164 G: 1.403 NCE: 1.183 NCE_Y: 0.915 \n",
      "(epoch: 41, iters: 180, time: 0.292, data: 0.002) G_GAN: 0.280 D_real: 0.235 D_fake: 0.229 G: 1.308 NCE: 1.174 NCE_Y: 0.882 \n",
      "(epoch: 41, iters: 280, time: 0.294, data: 0.001) G_GAN: 0.275 D_real: 0.261 D_fake: 0.231 G: 1.279 NCE: 1.125 NCE_Y: 0.883 \n",
      "End of epoch 41 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 42, iters: 47, time: 0.293, data: 0.002) G_GAN: 0.255 D_real: 0.236 D_fake: 0.251 G: 1.270 NCE: 1.134 NCE_Y: 0.896 \n",
      "(epoch: 42, iters: 147, time: 0.294, data: 0.002) G_GAN: 0.260 D_real: 0.283 D_fake: 0.243 G: 1.309 NCE: 1.187 NCE_Y: 0.911 \n",
      "(epoch: 42, iters: 247, time: 0.295, data: 0.002) G_GAN: 0.352 D_real: 0.175 D_fake: 0.176 G: 1.463 NCE: 1.268 NCE_Y: 0.953 \n",
      "End of epoch 42 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 43, iters: 14, time: 0.295, data: 0.002) G_GAN: 0.266 D_real: 0.276 D_fake: 0.240 G: 1.300 NCE: 1.133 NCE_Y: 0.934 \n",
      "(epoch: 43, iters: 114, time: 0.296, data: 0.001) G_GAN: 0.301 D_real: 0.305 D_fake: 0.206 G: 1.331 NCE: 1.170 NCE_Y: 0.890 \n",
      "(epoch: 43, iters: 214, time: 0.294, data: 0.002) G_GAN: 0.248 D_real: 0.255 D_fake: 0.269 G: 1.302 NCE: 1.196 NCE_Y: 0.913 \n",
      "(epoch: 43, iters: 314, time: 0.295, data: 0.002) G_GAN: 0.276 D_real: 0.222 D_fake: 0.237 G: 1.281 NCE: 1.118 NCE_Y: 0.890 \n",
      "End of epoch 43 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 44, iters: 81, time: 0.294, data: 0.001) G_GAN: 0.272 D_real: 0.260 D_fake: 0.235 G: 1.287 NCE: 1.133 NCE_Y: 0.897 \n",
      "(epoch: 44, iters: 181, time: 0.294, data: 0.002) G_GAN: 0.250 D_real: 0.248 D_fake: 0.258 G: 1.282 NCE: 1.165 NCE_Y: 0.900 \n",
      "(epoch: 44, iters: 281, time: 0.294, data: 0.002) G_GAN: 0.284 D_real: 0.363 D_fake: 0.220 G: 1.379 NCE: 1.142 NCE_Y: 1.048 \n",
      "End of epoch 44 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 45, iters: 48, time: 0.295, data: 0.001) G_GAN: 0.295 D_real: 0.197 D_fake: 0.225 G: 1.361 NCE: 1.193 NCE_Y: 0.938 \n",
      "(epoch: 45, iters: 148, time: 0.294, data: 0.001) G_GAN: 0.298 D_real: 0.206 D_fake: 0.271 G: 1.331 NCE: 1.187 NCE_Y: 0.879 \n",
      "(epoch: 45, iters: 248, time: 0.296, data: 0.001) G_GAN: 0.220 D_real: 0.283 D_fake: 0.301 G: 1.260 NCE: 1.158 NCE_Y: 0.922 \n",
      "End of epoch 45 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 46, iters: 15, time: 0.294, data: 0.002) G_GAN: 0.241 D_real: 0.160 D_fake: 0.304 G: 1.272 NCE: 1.173 NCE_Y: 0.888 \n",
      "saving the latest model (epoch 46, total_iters 15000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 46, iters: 115, time: 0.294, data: 0.002) G_GAN: 0.210 D_real: 0.275 D_fake: 0.379 G: 1.224 NCE: 1.164 NCE_Y: 0.863 \n",
      "(epoch: 46, iters: 215, time: 0.293, data: 0.001) G_GAN: 0.279 D_real: 0.257 D_fake: 0.244 G: 1.298 NCE: 1.148 NCE_Y: 0.889 \n",
      "(epoch: 46, iters: 315, time: 0.293, data: 0.001) G_GAN: 0.347 D_real: 0.277 D_fake: 0.180 G: 1.350 NCE: 1.114 NCE_Y: 0.893 \n",
      "End of epoch 46 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 47, iters: 82, time: 0.293, data: 0.001) G_GAN: 0.237 D_real: 0.220 D_fake: 0.272 G: 1.248 NCE: 1.131 NCE_Y: 0.892 \n",
      "(epoch: 47, iters: 182, time: 0.294, data: 0.002) G_GAN: 0.250 D_real: 0.290 D_fake: 0.253 G: 1.278 NCE: 1.158 NCE_Y: 0.897 \n",
      "(epoch: 47, iters: 282, time: 0.294, data: 0.002) G_GAN: 0.282 D_real: 0.172 D_fake: 0.230 G: 1.304 NCE: 1.134 NCE_Y: 0.910 \n",
      "End of epoch 47 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 48, iters: 49, time: 0.294, data: 0.001) G_GAN: 0.324 D_real: 0.266 D_fake: 0.194 G: 1.325 NCE: 1.128 NCE_Y: 0.874 \n",
      "(epoch: 48, iters: 149, time: 0.294, data: 0.002) G_GAN: 0.245 D_real: 0.240 D_fake: 0.260 G: 1.243 NCE: 1.137 NCE_Y: 0.860 \n",
      "(epoch: 48, iters: 249, time: 0.295, data: 0.002) G_GAN: 0.256 D_real: 0.250 D_fake: 0.253 G: 1.241 NCE: 1.128 NCE_Y: 0.843 \n",
      "End of epoch 48 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 49, iters: 16, time: 0.295, data: 0.002) G_GAN: 0.296 D_real: 0.049 D_fake: 0.233 G: 1.527 NCE: 1.180 NCE_Y: 1.282 \n",
      "(epoch: 49, iters: 116, time: 0.296, data: 0.002) G_GAN: 0.256 D_real: 0.237 D_fake: 0.252 G: 1.270 NCE: 1.164 NCE_Y: 0.865 \n",
      "(epoch: 49, iters: 216, time: 0.292, data: 0.002) G_GAN: 0.271 D_real: 0.129 D_fake: 0.241 G: 1.306 NCE: 1.143 NCE_Y: 0.927 \n",
      "(epoch: 49, iters: 316, time: 0.292, data: 0.001) G_GAN: 0.258 D_real: 0.255 D_fake: 0.250 G: 1.312 NCE: 1.151 NCE_Y: 0.957 \n",
      "End of epoch 49 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 50, iters: 83, time: 0.293, data: 0.002) G_GAN: 0.263 D_real: 0.273 D_fake: 0.253 G: 1.303 NCE: 1.181 NCE_Y: 0.899 \n",
      "(epoch: 50, iters: 183, time: 0.293, data: 0.002) G_GAN: 0.266 D_real: 0.215 D_fake: 0.239 G: 1.258 NCE: 1.127 NCE_Y: 0.858 \n",
      "(epoch: 50, iters: 283, time: 0.293, data: 0.001) G_GAN: 0.286 D_real: 0.282 D_fake: 0.232 G: 1.298 NCE: 1.116 NCE_Y: 0.907 \n",
      "End of epoch 50 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 51, iters: 50, time: 0.293, data: 0.002) G_GAN: 0.270 D_real: 0.156 D_fake: 0.239 G: 1.325 NCE: 1.134 NCE_Y: 0.977 \n",
      "(epoch: 51, iters: 150, time: 0.293, data: 0.001) G_GAN: 0.274 D_real: 0.210 D_fake: 0.242 G: 1.313 NCE: 1.114 NCE_Y: 0.965 \n",
      "(epoch: 51, iters: 250, time: 0.295, data: 0.001) G_GAN: 0.247 D_real: 0.254 D_fake: 0.258 G: 1.282 NCE: 1.160 NCE_Y: 0.908 \n",
      "End of epoch 51 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 52, iters: 17, time: 0.294, data: 0.001) G_GAN: 0.289 D_real: 0.259 D_fake: 0.226 G: 1.362 NCE: 1.238 NCE_Y: 0.907 \n",
      "(epoch: 52, iters: 117, time: 0.294, data: 0.002) G_GAN: 0.256 D_real: 0.233 D_fake: 0.251 G: 1.260 NCE: 1.130 NCE_Y: 0.877 \n",
      "(epoch: 52, iters: 217, time: 0.294, data: 0.002) G_GAN: 0.313 D_real: 0.259 D_fake: 0.207 G: 1.313 NCE: 1.101 NCE_Y: 0.899 \n",
      "(epoch: 52, iters: 317, time: 0.293, data: 0.001) G_GAN: 0.293 D_real: 0.257 D_fake: 0.211 G: 1.314 NCE: 1.143 NCE_Y: 0.898 \n",
      "End of epoch 52 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 53, iters: 84, time: 0.295, data: 0.001) G_GAN: 0.283 D_real: 0.180 D_fake: 0.238 G: 1.310 NCE: 1.106 NCE_Y: 0.948 \n",
      "(epoch: 53, iters: 184, time: 0.295, data: 0.002) G_GAN: 0.253 D_real: 0.269 D_fake: 0.250 G: 1.254 NCE: 1.125 NCE_Y: 0.877 \n",
      "(epoch: 53, iters: 284, time: 0.295, data: 0.002) G_GAN: 0.242 D_real: 0.272 D_fake: 0.275 G: 1.283 NCE: 1.186 NCE_Y: 0.897 \n",
      "End of epoch 53 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 54, iters: 51, time: 0.291, data: 0.001) G_GAN: 0.390 D_real: 0.266 D_fake: 0.148 G: 1.402 NCE: 1.179 NCE_Y: 0.845 \n",
      "(epoch: 54, iters: 151, time: 0.294, data: 0.002) G_GAN: 0.236 D_real: 0.193 D_fake: 0.286 G: 1.365 NCE: 1.105 NCE_Y: 1.153 \n",
      "(epoch: 54, iters: 251, time: 0.293, data: 0.001) G_GAN: 0.268 D_real: 0.202 D_fake: 0.242 G: 1.250 NCE: 1.146 NCE_Y: 0.817 \n",
      "End of epoch 54 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 55, iters: 18, time: 0.291, data: 0.002) G_GAN: 0.283 D_real: 0.262 D_fake: 0.224 G: 1.296 NCE: 1.182 NCE_Y: 0.844 \n",
      "(epoch: 55, iters: 118, time: 0.292, data: 0.001) G_GAN: 0.261 D_real: 0.253 D_fake: 0.244 G: 1.259 NCE: 1.132 NCE_Y: 0.865 \n",
      "(epoch: 55, iters: 218, time: 0.294, data: 0.002) G_GAN: 0.258 D_real: 0.261 D_fake: 0.244 G: 1.276 NCE: 1.199 NCE_Y: 0.837 \n",
      "(epoch: 55, iters: 318, time: 0.295, data: 0.002) G_GAN: 0.272 D_real: 0.274 D_fake: 0.239 G: 1.279 NCE: 1.095 NCE_Y: 0.919 \n",
      "End of epoch 55 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 56, iters: 85, time: 0.296, data: 0.002) G_GAN: 0.245 D_real: 0.264 D_fake: 0.279 G: 1.287 NCE: 1.200 NCE_Y: 0.883 \n",
      "(epoch: 56, iters: 185, time: 0.295, data: 0.001) G_GAN: 0.317 D_real: 0.238 D_fake: 0.224 G: 1.340 NCE: 1.140 NCE_Y: 0.906 \n",
      "(epoch: 56, iters: 285, time: 0.296, data: 0.002) G_GAN: 0.263 D_real: 0.251 D_fake: 0.244 G: 1.272 NCE: 1.128 NCE_Y: 0.890 \n",
      "End of epoch 56 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 57, iters: 52, time: 0.295, data: 0.002) G_GAN: 0.269 D_real: 0.267 D_fake: 0.238 G: 1.303 NCE: 1.159 NCE_Y: 0.909 \n",
      "(epoch: 57, iters: 152, time: 0.295, data: 0.001) G_GAN: 0.271 D_real: 0.310 D_fake: 0.247 G: 1.277 NCE: 1.128 NCE_Y: 0.883 \n",
      "(epoch: 57, iters: 252, time: 0.293, data: 0.002) G_GAN: 0.259 D_real: 0.228 D_fake: 0.248 G: 1.249 NCE: 1.114 NCE_Y: 0.865 \n",
      "End of epoch 57 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 58, iters: 19, time: 0.292, data: 0.001) G_GAN: 0.267 D_real: 0.245 D_fake: 0.237 G: 1.326 NCE: 1.219 NCE_Y: 0.900 \n",
      "(epoch: 58, iters: 119, time: 0.293, data: 0.002) G_GAN: 0.277 D_real: 0.273 D_fake: 0.226 G: 1.294 NCE: 1.145 NCE_Y: 0.889 \n",
      "(epoch: 58, iters: 219, time: 0.293, data: 0.002) G_GAN: 0.238 D_real: 0.281 D_fake: 0.284 G: 1.230 NCE: 1.105 NCE_Y: 0.879 \n",
      "(epoch: 58, iters: 319, time: 0.293, data: 0.001) G_GAN: 0.268 D_real: 0.254 D_fake: 0.239 G: 1.283 NCE: 1.167 NCE_Y: 0.864 \n",
      "End of epoch 58 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 59, iters: 86, time: 0.293, data: 0.002) G_GAN: 0.261 D_real: 0.220 D_fake: 0.249 G: 1.311 NCE: 1.173 NCE_Y: 0.927 \n",
      "(epoch: 59, iters: 186, time: 0.295, data: 0.002) G_GAN: 0.256 D_real: 0.271 D_fake: 0.247 G: 1.225 NCE: 1.135 NCE_Y: 0.804 \n",
      "(epoch: 59, iters: 286, time: 0.294, data: 0.001) G_GAN: 0.311 D_real: 0.174 D_fake: 0.208 G: 1.297 NCE: 1.119 NCE_Y: 0.855 \n",
      "End of epoch 59 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 60, iters: 53, time: 0.293, data: 0.001) G_GAN: 0.287 D_real: 0.267 D_fake: 0.216 G: 1.295 NCE: 1.126 NCE_Y: 0.890 \n",
      "(epoch: 60, iters: 153, time: 0.293, data: 0.002) G_GAN: 0.273 D_real: 0.237 D_fake: 0.235 G: 1.296 NCE: 1.119 NCE_Y: 0.926 \n",
      "(epoch: 60, iters: 253, time: 0.292, data: 0.001) G_GAN: 0.316 D_real: 0.135 D_fake: 0.205 G: 1.306 NCE: 1.129 NCE_Y: 0.851 \n",
      "saving the model at the end of epoch 60, iters 19980\n",
      "End of epoch 60 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 61, iters: 20, time: 0.291, data: 0.002) G_GAN: 0.282 D_real: 0.218 D_fake: 0.250 G: 1.341 NCE: 1.175 NCE_Y: 0.942 \n",
      "saving the latest model (epoch 61, total_iters 20000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 61, iters: 120, time: 0.293, data: 0.001) G_GAN: 0.246 D_real: 0.229 D_fake: 0.260 G: 1.229 NCE: 1.145 NCE_Y: 0.821 \n",
      "(epoch: 61, iters: 220, time: 0.290, data: 0.001) G_GAN: 0.217 D_real: 0.261 D_fake: 0.309 G: 1.244 NCE: 1.134 NCE_Y: 0.919 \n",
      "(epoch: 61, iters: 320, time: 0.290, data: 0.002) G_GAN: 0.243 D_real: 0.233 D_fake: 0.273 G: 1.244 NCE: 1.119 NCE_Y: 0.883 \n",
      "End of epoch 61 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 62, iters: 87, time: 0.291, data: 0.001) G_GAN: 0.240 D_real: 0.241 D_fake: 0.274 G: 1.267 NCE: 1.203 NCE_Y: 0.853 \n",
      "(epoch: 62, iters: 187, time: 0.291, data: 0.001) G_GAN: 0.261 D_real: 0.241 D_fake: 0.248 G: 1.279 NCE: 1.205 NCE_Y: 0.833 \n",
      "(epoch: 62, iters: 287, time: 0.291, data: 0.002) G_GAN: 0.248 D_real: 0.258 D_fake: 0.256 G: 1.258 NCE: 1.130 NCE_Y: 0.889 \n",
      "End of epoch 62 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 63, iters: 54, time: 0.293, data: 0.001) G_GAN: 0.293 D_real: 0.253 D_fake: 0.224 G: 1.321 NCE: 1.133 NCE_Y: 0.925 \n",
      "(epoch: 63, iters: 154, time: 0.294, data: 0.002) G_GAN: 0.281 D_real: 0.274 D_fake: 0.229 G: 1.262 NCE: 1.144 NCE_Y: 0.817 \n",
      "(epoch: 63, iters: 254, time: 0.294, data: 0.001) G_GAN: 0.295 D_real: 0.265 D_fake: 0.215 G: 1.277 NCE: 1.075 NCE_Y: 0.889 \n",
      "End of epoch 63 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 64, iters: 21, time: 0.290, data: 0.001) G_GAN: 0.278 D_real: 0.269 D_fake: 0.225 G: 1.289 NCE: 1.147 NCE_Y: 0.875 \n",
      "(epoch: 64, iters: 121, time: 0.292, data: 0.002) G_GAN: 0.294 D_real: 0.250 D_fake: 0.229 G: 1.318 NCE: 1.138 NCE_Y: 0.911 \n",
      "(epoch: 64, iters: 221, time: 0.292, data: 0.002) G_GAN: 0.307 D_real: 0.241 D_fake: 0.211 G: 1.288 NCE: 1.148 NCE_Y: 0.815 \n",
      "(epoch: 64, iters: 321, time: 0.290, data: 0.001) G_GAN: 0.289 D_real: 0.282 D_fake: 0.227 G: 1.307 NCE: 1.174 NCE_Y: 0.863 \n",
      "End of epoch 64 / 350 \t Time Taken: 97 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 65, iters: 88, time: 0.291, data: 0.001) G_GAN: 0.285 D_real: 0.286 D_fake: 0.214 G: 1.278 NCE: 1.124 NCE_Y: 0.862 \n",
      "(epoch: 65, iters: 188, time: 0.293, data: 0.001) G_GAN: 0.257 D_real: 0.242 D_fake: 0.249 G: 1.282 NCE: 1.146 NCE_Y: 0.906 \n",
      "(epoch: 65, iters: 288, time: 0.292, data: 0.002) G_GAN: 0.269 D_real: 0.200 D_fake: 0.238 G: 1.271 NCE: 1.120 NCE_Y: 0.883 \n",
      "End of epoch 65 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 66, iters: 55, time: 0.294, data: 0.001) G_GAN: 0.286 D_real: 0.238 D_fake: 0.240 G: 1.303 NCE: 1.117 NCE_Y: 0.916 \n",
      "(epoch: 66, iters: 155, time: 0.292, data: 0.002) G_GAN: 0.270 D_real: 0.193 D_fake: 0.236 G: 1.259 NCE: 1.119 NCE_Y: 0.859 \n",
      "(epoch: 66, iters: 255, time: 0.293, data: 0.002) G_GAN: 0.254 D_real: 0.264 D_fake: 0.264 G: 1.220 NCE: 1.102 NCE_Y: 0.830 \n",
      "End of epoch 66 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 67, iters: 22, time: 0.293, data: 0.002) G_GAN: 0.287 D_real: 0.283 D_fake: 0.216 G: 1.307 NCE: 1.117 NCE_Y: 0.923 \n",
      "(epoch: 67, iters: 122, time: 0.294, data: 0.001) G_GAN: 0.289 D_real: 0.242 D_fake: 0.220 G: 1.310 NCE: 1.116 NCE_Y: 0.926 \n",
      "(epoch: 67, iters: 222, time: 0.291, data: 0.001) G_GAN: 0.265 D_real: 0.256 D_fake: 0.242 G: 1.284 NCE: 1.130 NCE_Y: 0.907 \n",
      "(epoch: 67, iters: 322, time: 0.291, data: 0.002) G_GAN: 0.300 D_real: 0.250 D_fake: 0.215 G: 1.322 NCE: 1.143 NCE_Y: 0.900 \n",
      "End of epoch 67 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 68, iters: 89, time: 0.292, data: 0.001) G_GAN: 0.293 D_real: 0.269 D_fake: 0.214 G: 1.316 NCE: 1.124 NCE_Y: 0.922 \n",
      "(epoch: 68, iters: 189, time: 0.292, data: 0.001) G_GAN: 0.285 D_real: 0.272 D_fake: 0.222 G: 1.295 NCE: 1.129 NCE_Y: 0.891 \n",
      "(epoch: 68, iters: 289, time: 0.292, data: 0.002) G_GAN: 0.279 D_real: 0.225 D_fake: 0.229 G: 1.349 NCE: 1.209 NCE_Y: 0.933 \n",
      "End of epoch 68 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 69, iters: 56, time: 0.294, data: 0.001) G_GAN: 0.313 D_real: 0.231 D_fake: 0.209 G: 1.290 NCE: 1.088 NCE_Y: 0.865 \n",
      "(epoch: 69, iters: 156, time: 0.291, data: 0.001) G_GAN: 0.267 D_real: 0.220 D_fake: 0.241 G: 1.266 NCE: 1.137 NCE_Y: 0.860 \n",
      "(epoch: 69, iters: 256, time: 0.292, data: 0.002) G_GAN: 0.252 D_real: 0.236 D_fake: 0.277 G: 1.272 NCE: 1.152 NCE_Y: 0.888 \n",
      "End of epoch 69 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 70, iters: 23, time: 0.293, data: 0.002) G_GAN: 0.251 D_real: 0.241 D_fake: 0.256 G: 1.272 NCE: 1.152 NCE_Y: 0.890 \n",
      "(epoch: 70, iters: 123, time: 0.294, data: 0.001) G_GAN: 0.265 D_real: 0.221 D_fake: 0.243 G: 1.304 NCE: 1.189 NCE_Y: 0.887 \n",
      "(epoch: 70, iters: 223, time: 0.290, data: 0.002) G_GAN: 0.272 D_real: 0.250 D_fake: 0.243 G: 1.356 NCE: 1.267 NCE_Y: 0.901 \n",
      "(epoch: 70, iters: 323, time: 0.292, data: 0.002) G_GAN: 0.280 D_real: 0.235 D_fake: 0.228 G: 1.336 NCE: 1.220 NCE_Y: 0.891 \n",
      "End of epoch 70 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 71, iters: 90, time: 0.292, data: 0.001) G_GAN: 0.348 D_real: 0.288 D_fake: 0.180 G: 1.319 NCE: 1.107 NCE_Y: 0.834 \n",
      "(epoch: 71, iters: 190, time: 0.293, data: 0.002) G_GAN: 0.276 D_real: 0.256 D_fake: 0.239 G: 1.280 NCE: 1.136 NCE_Y: 0.872 \n",
      "(epoch: 71, iters: 290, time: 0.293, data: 0.002) G_GAN: 0.252 D_real: 0.270 D_fake: 0.248 G: 1.237 NCE: 1.128 NCE_Y: 0.843 \n",
      "End of epoch 71 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 72, iters: 57, time: 0.293, data: 0.001) G_GAN: 0.274 D_real: 0.006 D_fake: 0.264 G: 1.338 NCE: 1.123 NCE_Y: 1.004 \n",
      "(epoch: 72, iters: 157, time: 0.293, data: 0.002) G_GAN: 0.267 D_real: 0.170 D_fake: 0.240 G: 1.266 NCE: 1.144 NCE_Y: 0.855 \n",
      "(epoch: 72, iters: 257, time: 0.295, data: 0.002) G_GAN: 0.321 D_real: 0.273 D_fake: 0.196 G: 1.304 NCE: 1.116 NCE_Y: 0.851 \n",
      "End of epoch 72 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 73, iters: 24, time: 0.294, data: 0.002) G_GAN: 0.279 D_real: 0.272 D_fake: 0.230 G: 1.278 NCE: 1.148 NCE_Y: 0.849 \n",
      "(epoch: 73, iters: 124, time: 0.292, data: 0.002) G_GAN: 0.238 D_real: 0.241 D_fake: 0.269 G: 1.257 NCE: 1.140 NCE_Y: 0.897 \n",
      "(epoch: 73, iters: 224, time: 0.293, data: 0.002) G_GAN: 0.355 D_real: 0.279 D_fake: 0.192 G: 1.370 NCE: 1.177 NCE_Y: 0.853 \n",
      "(epoch: 73, iters: 324, time: 0.292, data: 0.001) G_GAN: 0.262 D_real: 0.246 D_fake: 0.248 G: 1.291 NCE: 1.153 NCE_Y: 0.905 \n",
      "End of epoch 73 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 74, iters: 91, time: 0.293, data: 0.002) G_GAN: 0.256 D_real: 0.230 D_fake: 0.246 G: 1.260 NCE: 1.117 NCE_Y: 0.891 \n",
      "(epoch: 74, iters: 191, time: 0.293, data: 0.002) G_GAN: 0.279 D_real: 0.255 D_fake: 0.232 G: 1.301 NCE: 1.156 NCE_Y: 0.889 \n",
      "(epoch: 74, iters: 291, time: 0.293, data: 0.002) G_GAN: 0.261 D_real: 0.273 D_fake: 0.245 G: 1.255 NCE: 1.133 NCE_Y: 0.856 \n",
      "End of epoch 74 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 75, iters: 58, time: 0.293, data: 0.002) G_GAN: 0.284 D_real: 0.277 D_fake: 0.240 G: 1.308 NCE: 1.163 NCE_Y: 0.885 \n",
      "(epoch: 75, iters: 158, time: 0.293, data: 0.001) G_GAN: 0.266 D_real: 0.246 D_fake: 0.243 G: 1.307 NCE: 1.174 NCE_Y: 0.908 \n",
      "(epoch: 75, iters: 258, time: 0.292, data: 0.001) G_GAN: 0.262 D_real: 0.256 D_fake: 0.268 G: 1.291 NCE: 1.149 NCE_Y: 0.910 \n",
      "End of epoch 75 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 76, iters: 25, time: 0.292, data: 0.001) G_GAN: 0.288 D_real: 0.254 D_fake: 0.219 G: 1.373 NCE: 1.259 NCE_Y: 0.912 \n",
      "saving the latest model (epoch 76, total_iters 25000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 76, iters: 125, time: 0.292, data: 0.001) G_GAN: 0.216 D_real: 0.272 D_fake: 0.309 G: 1.248 NCE: 1.165 NCE_Y: 0.898 \n",
      "(epoch: 76, iters: 225, time: 0.295, data: 0.001) G_GAN: 0.265 D_real: 0.258 D_fake: 0.241 G: 1.286 NCE: 1.130 NCE_Y: 0.912 \n",
      "(epoch: 76, iters: 325, time: 0.294, data: 0.001) G_GAN: 0.275 D_real: 0.251 D_fake: 0.233 G: 1.327 NCE: 1.227 NCE_Y: 0.877 \n",
      "End of epoch 76 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 77, iters: 92, time: 0.293, data: 0.002) G_GAN: 0.325 D_real: 0.216 D_fake: 0.212 G: 1.331 NCE: 1.102 NCE_Y: 0.909 \n",
      "(epoch: 77, iters: 192, time: 0.293, data: 0.001) G_GAN: 0.324 D_real: 0.257 D_fake: 0.189 G: 1.353 NCE: 1.137 NCE_Y: 0.920 \n",
      "(epoch: 77, iters: 292, time: 0.294, data: 0.001) G_GAN: 0.293 D_real: 0.278 D_fake: 0.210 G: 1.310 NCE: 1.126 NCE_Y: 0.907 \n",
      "End of epoch 77 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 78, iters: 59, time: 0.293, data: 0.002) G_GAN: 0.270 D_real: 0.261 D_fake: 0.254 G: 1.274 NCE: 1.168 NCE_Y: 0.839 \n",
      "(epoch: 78, iters: 159, time: 0.292, data: 0.002) G_GAN: 0.274 D_real: 0.218 D_fake: 0.237 G: 1.295 NCE: 1.147 NCE_Y: 0.896 \n",
      "(epoch: 78, iters: 259, time: 0.294, data: 0.001) G_GAN: 0.266 D_real: 0.265 D_fake: 0.240 G: 1.258 NCE: 1.118 NCE_Y: 0.867 \n",
      "End of epoch 78 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 79, iters: 26, time: 0.296, data: 0.001) G_GAN: 0.301 D_real: 0.276 D_fake: 0.213 G: 1.313 NCE: 1.133 NCE_Y: 0.892 \n",
      "(epoch: 79, iters: 126, time: 0.296, data: 0.002) G_GAN: 0.280 D_real: 0.270 D_fake: 0.224 G: 1.296 NCE: 1.137 NCE_Y: 0.894 \n",
      "(epoch: 79, iters: 226, time: 0.293, data: 0.002) G_GAN: 0.298 D_real: 0.243 D_fake: 0.220 G: 1.329 NCE: 1.161 NCE_Y: 0.901 \n",
      "(epoch: 79, iters: 326, time: 0.294, data: 0.002) G_GAN: 0.324 D_real: 0.273 D_fake: 0.209 G: 1.451 NCE: 1.387 NCE_Y: 0.867 \n",
      "End of epoch 79 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000100\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 80, iters: 93, time: 0.295, data: 0.002) G_GAN: 0.311 D_real: 0.134 D_fake: 0.211 G: 1.442 NCE: 1.144 NCE_Y: 1.119 \n",
      "(epoch: 80, iters: 193, time: 0.296, data: 0.002) G_GAN: 0.289 D_real: 0.203 D_fake: 0.223 G: 1.303 NCE: 1.126 NCE_Y: 0.902 \n",
      "(epoch: 80, iters: 293, time: 0.295, data: 0.002) G_GAN: 0.318 D_real: 0.283 D_fake: 0.194 G: 1.324 NCE: 1.130 NCE_Y: 0.882 \n",
      "End of epoch 80 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 81, iters: 60, time: 0.295, data: 0.001) G_GAN: 0.333 D_real: 0.272 D_fake: 0.195 G: 1.316 NCE: 1.128 NCE_Y: 0.838 \n",
      "(epoch: 81, iters: 160, time: 0.294, data: 0.002) G_GAN: 0.302 D_real: 0.219 D_fake: 0.207 G: 1.322 NCE: 1.156 NCE_Y: 0.885 \n",
      "(epoch: 81, iters: 260, time: 0.291, data: 0.001) G_GAN: 0.269 D_real: 0.147 D_fake: 0.237 G: 1.416 NCE: 1.197 NCE_Y: 1.096 \n",
      "End of epoch 81 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 82, iters: 27, time: 0.292, data: 0.002) G_GAN: 0.291 D_real: 0.271 D_fake: 0.223 G: 1.298 NCE: 1.106 NCE_Y: 0.907 \n",
      "(epoch: 82, iters: 127, time: 0.294, data: 0.001) G_GAN: 0.275 D_real: 0.240 D_fake: 0.230 G: 1.291 NCE: 1.128 NCE_Y: 0.904 \n",
      "(epoch: 82, iters: 227, time: 0.294, data: 0.002) G_GAN: 0.257 D_real: 0.249 D_fake: 0.248 G: 1.255 NCE: 1.136 NCE_Y: 0.860 \n",
      "(epoch: 82, iters: 327, time: 0.293, data: 0.002) G_GAN: 0.273 D_real: 0.254 D_fake: 0.240 G: 1.279 NCE: 1.118 NCE_Y: 0.894 \n",
      "End of epoch 82 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 83, iters: 94, time: 0.293, data: 0.002) G_GAN: 0.275 D_real: 0.211 D_fake: 0.230 G: 1.306 NCE: 1.123 NCE_Y: 0.941 \n",
      "(epoch: 83, iters: 194, time: 0.293, data: 0.002) G_GAN: 0.271 D_real: 0.259 D_fake: 0.234 G: 1.289 NCE: 1.150 NCE_Y: 0.887 \n",
      "(epoch: 83, iters: 294, time: 0.295, data: 0.001) G_GAN: 0.273 D_real: 0.221 D_fake: 0.232 G: 1.317 NCE: 1.132 NCE_Y: 0.956 \n",
      "End of epoch 83 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 84, iters: 61, time: 0.292, data: 0.001) G_GAN: 0.282 D_real: 0.116 D_fake: 0.225 G: 1.302 NCE: 1.125 NCE_Y: 0.915 \n",
      "(epoch: 84, iters: 161, time: 0.293, data: 0.002) G_GAN: 0.270 D_real: 0.265 D_fake: 0.235 G: 1.270 NCE: 1.130 NCE_Y: 0.869 \n",
      "(epoch: 84, iters: 261, time: 0.293, data: 0.002) G_GAN: 0.279 D_real: 0.248 D_fake: 0.226 G: 1.253 NCE: 1.125 NCE_Y: 0.823 \n",
      "End of epoch 84 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 85, iters: 28, time: 0.294, data: 0.002) G_GAN: 0.260 D_real: 0.246 D_fake: 0.243 G: 1.247 NCE: 1.118 NCE_Y: 0.856 \n",
      "(epoch: 85, iters: 128, time: 0.292, data: 0.002) G_GAN: 0.274 D_real: 0.189 D_fake: 0.231 G: 1.283 NCE: 1.138 NCE_Y: 0.880 \n",
      "(epoch: 85, iters: 228, time: 0.293, data: 0.002) G_GAN: 0.319 D_real: 0.256 D_fake: 0.199 G: 1.341 NCE: 1.153 NCE_Y: 0.890 \n",
      "(epoch: 85, iters: 328, time: 0.293, data: 0.001) G_GAN: 0.267 D_real: 0.273 D_fake: 0.243 G: 1.257 NCE: 1.101 NCE_Y: 0.880 \n",
      "End of epoch 85 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 86, iters: 95, time: 0.294, data: 0.001) G_GAN: 0.258 D_real: 0.223 D_fake: 0.249 G: 1.233 NCE: 1.121 NCE_Y: 0.829 \n",
      "(epoch: 86, iters: 195, time: 0.294, data: 0.001) G_GAN: 0.266 D_real: 0.245 D_fake: 0.238 G: 1.282 NCE: 1.190 NCE_Y: 0.841 \n",
      "(epoch: 86, iters: 295, time: 0.294, data: 0.002) G_GAN: 0.266 D_real: 0.228 D_fake: 0.240 G: 1.280 NCE: 1.131 NCE_Y: 0.898 \n",
      "End of epoch 86 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 87, iters: 62, time: 0.294, data: 0.002) G_GAN: 0.274 D_real: 0.244 D_fake: 0.231 G: 1.304 NCE: 1.149 NCE_Y: 0.909 \n",
      "(epoch: 87, iters: 162, time: 0.291, data: 0.002) G_GAN: 0.303 D_real: 0.237 D_fake: 0.213 G: 1.293 NCE: 1.090 NCE_Y: 0.890 \n",
      "(epoch: 87, iters: 262, time: 0.293, data: 0.002) G_GAN: 0.271 D_real: 0.110 D_fake: 0.235 G: 1.300 NCE: 1.146 NCE_Y: 0.911 \n",
      "End of epoch 87 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 88, iters: 29, time: 0.295, data: 0.002) G_GAN: 0.278 D_real: 0.007 D_fake: 0.231 G: 1.345 NCE: 1.130 NCE_Y: 1.003 \n",
      "(epoch: 88, iters: 129, time: 0.293, data: 0.001) G_GAN: 0.265 D_real: 0.273 D_fake: 0.248 G: 1.264 NCE: 1.142 NCE_Y: 0.856 \n",
      "(epoch: 88, iters: 229, time: 0.293, data: 0.001) G_GAN: 0.273 D_real: 0.226 D_fake: 0.232 G: 1.236 NCE: 1.125 NCE_Y: 0.799 \n",
      "(epoch: 88, iters: 329, time: 0.293, data: 0.002) G_GAN: 0.276 D_real: 0.217 D_fake: 0.230 G: 1.238 NCE: 1.124 NCE_Y: 0.800 \n",
      "End of epoch 88 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 89, iters: 96, time: 0.290, data: 0.001) G_GAN: 0.297 D_real: 0.264 D_fake: 0.212 G: 1.295 NCE: 1.140 NCE_Y: 0.855 \n",
      "(epoch: 89, iters: 196, time: 0.293, data: 0.002) G_GAN: 0.269 D_real: 0.263 D_fake: 0.237 G: 1.266 NCE: 1.108 NCE_Y: 0.885 \n",
      "(epoch: 89, iters: 296, time: 0.295, data: 0.001) G_GAN: 0.281 D_real: 0.256 D_fake: 0.225 G: 1.359 NCE: 1.259 NCE_Y: 0.898 \n",
      "End of epoch 89 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 90, iters: 63, time: 0.293, data: 0.002) G_GAN: 0.253 D_real: 0.226 D_fake: 0.258 G: 1.223 NCE: 1.131 NCE_Y: 0.809 \n",
      "(epoch: 90, iters: 163, time: 0.293, data: 0.002) G_GAN: 0.285 D_real: 0.254 D_fake: 0.222 G: 1.319 NCE: 1.171 NCE_Y: 0.897 \n",
      "(epoch: 90, iters: 263, time: 0.294, data: 0.001) G_GAN: 0.276 D_real: 0.225 D_fake: 0.229 G: 1.270 NCE: 1.113 NCE_Y: 0.873 \n",
      "End of epoch 90 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 91, iters: 30, time: 0.296, data: 0.002) G_GAN: 0.295 D_real: 0.251 D_fake: 0.218 G: 1.323 NCE: 1.124 NCE_Y: 0.934 \n",
      "saving the latest model (epoch 91, total_iters 30000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 91, iters: 130, time: 0.296, data: 0.002) G_GAN: 0.276 D_real: 0.248 D_fake: 0.229 G: 1.235 NCE: 1.096 NCE_Y: 0.822 \n",
      "(epoch: 91, iters: 230, time: 0.296, data: 0.001) G_GAN: 0.278 D_real: 0.265 D_fake: 0.228 G: 1.298 NCE: 1.126 NCE_Y: 0.913 \n",
      "(epoch: 91, iters: 330, time: 0.293, data: 0.001) G_GAN: 0.283 D_real: 0.268 D_fake: 0.223 G: 1.278 NCE: 1.116 NCE_Y: 0.874 \n",
      "End of epoch 91 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 92, iters: 97, time: 0.294, data: 0.002) G_GAN: 0.274 D_real: 0.238 D_fake: 0.242 G: 1.297 NCE: 1.174 NCE_Y: 0.872 \n",
      "(epoch: 92, iters: 197, time: 0.295, data: 0.001) G_GAN: 0.274 D_real: 0.298 D_fake: 0.230 G: 1.243 NCE: 1.133 NCE_Y: 0.806 \n",
      "(epoch: 92, iters: 297, time: 0.295, data: 0.001) G_GAN: 0.270 D_real: 0.250 D_fake: 0.234 G: 1.274 NCE: 1.129 NCE_Y: 0.878 \n",
      "End of epoch 92 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 93, iters: 64, time: 0.295, data: 0.002) G_GAN: 0.286 D_real: 0.264 D_fake: 0.220 G: 1.303 NCE: 1.142 NCE_Y: 0.890 \n",
      "(epoch: 93, iters: 164, time: 0.294, data: 0.001) G_GAN: 0.255 D_real: 0.266 D_fake: 0.249 G: 1.249 NCE: 1.143 NCE_Y: 0.846 \n",
      "(epoch: 93, iters: 264, time: 0.292, data: 0.002) G_GAN: 0.273 D_real: 0.255 D_fake: 0.231 G: 1.268 NCE: 1.145 NCE_Y: 0.845 \n",
      "End of epoch 93 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 94, iters: 31, time: 0.294, data: 0.002) G_GAN: 0.264 D_real: 0.250 D_fake: 0.241 G: 1.267 NCE: 1.138 NCE_Y: 0.869 \n",
      "(epoch: 94, iters: 131, time: 0.293, data: 0.001) G_GAN: 0.263 D_real: 0.248 D_fake: 0.241 G: 1.274 NCE: 1.121 NCE_Y: 0.901 \n",
      "(epoch: 94, iters: 231, time: 0.294, data: 0.002) G_GAN: 0.292 D_real: 0.256 D_fake: 0.216 G: 1.325 NCE: 1.210 NCE_Y: 0.856 \n",
      "(epoch: 94, iters: 331, time: 0.294, data: 0.002) G_GAN: 0.281 D_real: 0.306 D_fake: 0.226 G: 1.304 NCE: 1.126 NCE_Y: 0.920 \n",
      "End of epoch 94 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 95, iters: 98, time: 0.292, data: 0.002) G_GAN: 0.312 D_real: 0.238 D_fake: 0.205 G: 1.293 NCE: 1.131 NCE_Y: 0.830 \n",
      "(epoch: 95, iters: 198, time: 0.294, data: 0.002) G_GAN: 0.270 D_real: 0.267 D_fake: 0.234 G: 1.273 NCE: 1.123 NCE_Y: 0.883 \n",
      "(epoch: 95, iters: 298, time: 0.295, data: 0.001) G_GAN: 0.264 D_real: 0.239 D_fake: 0.241 G: 1.254 NCE: 1.122 NCE_Y: 0.857 \n",
      "End of epoch 95 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 96, iters: 65, time: 0.293, data: 0.002) G_GAN: 0.290 D_real: 0.033 D_fake: 0.220 G: 1.406 NCE: 1.108 NCE_Y: 1.124 \n",
      "(epoch: 96, iters: 165, time: 0.295, data: 0.002) G_GAN: 0.273 D_real: 0.272 D_fake: 0.232 G: 1.279 NCE: 1.123 NCE_Y: 0.890 \n",
      "(epoch: 96, iters: 265, time: 0.292, data: 0.001) G_GAN: 0.248 D_real: 0.264 D_fake: 0.262 G: 1.293 NCE: 1.177 NCE_Y: 0.913 \n",
      "End of epoch 96 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 97, iters: 32, time: 0.292, data: 0.002) G_GAN: 0.261 D_real: 0.213 D_fake: 0.243 G: 1.266 NCE: 1.124 NCE_Y: 0.885 \n",
      "(epoch: 97, iters: 132, time: 0.291, data: 0.001) G_GAN: 0.287 D_real: 0.255 D_fake: 0.220 G: 1.281 NCE: 1.136 NCE_Y: 0.853 \n",
      "(epoch: 97, iters: 232, time: 0.291, data: 0.002) G_GAN: 0.232 D_real: 0.267 D_fake: 0.277 G: 1.236 NCE: 1.110 NCE_Y: 0.898 \n",
      "(epoch: 97, iters: 332, time: 0.293, data: 0.001) G_GAN: 0.296 D_real: 0.214 D_fake: 0.221 G: 1.322 NCE: 1.157 NCE_Y: 0.894 \n",
      "End of epoch 97 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 98, iters: 99, time: 0.293, data: 0.001) G_GAN: 0.272 D_real: 0.185 D_fake: 0.233 G: 1.346 NCE: 1.229 NCE_Y: 0.918 \n",
      "(epoch: 98, iters: 199, time: 0.293, data: 0.002) G_GAN: 0.267 D_real: 0.271 D_fake: 0.237 G: 1.259 NCE: 1.171 NCE_Y: 0.813 \n",
      "(epoch: 98, iters: 299, time: 0.293, data: 0.001) G_GAN: 0.286 D_real: 0.258 D_fake: 0.219 G: 1.291 NCE: 1.124 NCE_Y: 0.886 \n",
      "End of epoch 98 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 99, iters: 66, time: 0.293, data: 0.002) G_GAN: 0.273 D_real: 0.226 D_fake: 0.231 G: 1.307 NCE: 1.141 NCE_Y: 0.927 \n",
      "(epoch: 99, iters: 166, time: 0.293, data: 0.001) G_GAN: 0.288 D_real: 0.260 D_fake: 0.221 G: 1.276 NCE: 1.096 NCE_Y: 0.881 \n",
      "(epoch: 99, iters: 266, time: 0.293, data: 0.001) G_GAN: 0.270 D_real: 0.261 D_fake: 0.235 G: 1.262 NCE: 1.117 NCE_Y: 0.868 \n",
      "End of epoch 99 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 100, iters: 33, time: 0.293, data: 0.002) G_GAN: 0.278 D_real: 0.229 D_fake: 0.228 G: 1.322 NCE: 1.283 NCE_Y: 0.805 \n",
      "(epoch: 100, iters: 133, time: 0.293, data: 0.002) G_GAN: 0.295 D_real: 0.263 D_fake: 0.219 G: 1.294 NCE: 1.105 NCE_Y: 0.892 \n",
      "(epoch: 100, iters: 233, time: 0.292, data: 0.001) G_GAN: 0.242 D_real: 0.244 D_fake: 0.271 G: 1.219 NCE: 1.107 NCE_Y: 0.849 \n",
      "(epoch: 100, iters: 333, time: 0.293, data: 0.002) G_GAN: 0.263 D_real: 0.253 D_fake: 0.246 G: 1.303 NCE: 1.165 NCE_Y: 0.913 \n",
      "End of epoch 100 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 101, iters: 100, time: 0.295, data: 0.257) G_GAN: 0.273 D_real: 0.261 D_fake: 0.238 G: 1.283 NCE: 1.146 NCE_Y: 0.874 \n",
      "(epoch: 101, iters: 200, time: 0.295, data: 0.002) G_GAN: 0.266 D_real: 0.247 D_fake: 0.239 G: 1.299 NCE: 1.149 NCE_Y: 0.916 \n",
      "(epoch: 101, iters: 300, time: 0.296, data: 0.001) G_GAN: 0.272 D_real: 0.240 D_fake: 0.239 G: 1.272 NCE: 1.129 NCE_Y: 0.871 \n",
      "End of epoch 101 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 102, iters: 67, time: 0.294, data: 0.002) G_GAN: 0.284 D_real: 0.269 D_fake: 0.222 G: 1.298 NCE: 1.118 NCE_Y: 0.910 \n",
      "(epoch: 102, iters: 167, time: 0.293, data: 0.002) G_GAN: 0.272 D_real: 0.240 D_fake: 0.233 G: 1.301 NCE: 1.109 NCE_Y: 0.950 \n",
      "(epoch: 102, iters: 267, time: 0.292, data: 0.001) G_GAN: 0.248 D_real: 0.182 D_fake: 0.260 G: 1.355 NCE: 1.119 NCE_Y: 1.096 \n",
      "End of epoch 102 / 350 \t Time Taken: 97 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 103, iters: 34, time: 0.289, data: 0.002) G_GAN: 0.270 D_real: 0.234 D_fake: 0.235 G: 1.239 NCE: 1.103 NCE_Y: 0.834 \n",
      "(epoch: 103, iters: 134, time: 0.291, data: 0.002) G_GAN: 0.274 D_real: 0.243 D_fake: 0.231 G: 1.286 NCE: 1.132 NCE_Y: 0.892 \n",
      "(epoch: 103, iters: 234, time: 0.292, data: 0.001) G_GAN: 0.270 D_real: 0.010 D_fake: 0.238 G: 1.342 NCE: 1.135 NCE_Y: 1.009 \n",
      "End of epoch 103 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 104, iters: 1, time: 0.294, data: 0.001) G_GAN: 0.272 D_real: 0.253 D_fake: 0.233 G: 1.277 NCE: 1.115 NCE_Y: 0.895 \n",
      "(epoch: 104, iters: 101, time: 0.293, data: 0.000) G_GAN: 0.272 D_real: 0.247 D_fake: 0.232 G: 1.264 NCE: 1.113 NCE_Y: 0.871 \n",
      "(epoch: 104, iters: 201, time: 0.292, data: 0.002) G_GAN: 0.271 D_real: 0.147 D_fake: 0.234 G: 1.287 NCE: 1.137 NCE_Y: 0.894 \n",
      "(epoch: 104, iters: 301, time: 0.291, data: 0.002) G_GAN: 0.274 D_real: 0.213 D_fake: 0.238 G: 1.308 NCE: 1.229 NCE_Y: 0.838 \n",
      "End of epoch 104 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 105, iters: 68, time: 0.292, data: 0.002) G_GAN: 0.279 D_real: 0.250 D_fake: 0.227 G: 1.292 NCE: 1.171 NCE_Y: 0.856 \n",
      "(epoch: 105, iters: 168, time: 0.295, data: 0.001) G_GAN: 0.275 D_real: 0.260 D_fake: 0.229 G: 1.278 NCE: 1.124 NCE_Y: 0.882 \n",
      "(epoch: 105, iters: 268, time: 0.293, data: 0.001) G_GAN: 0.275 D_real: 0.256 D_fake: 0.230 G: 1.296 NCE: 1.154 NCE_Y: 0.887 \n",
      "End of epoch 105 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 106, iters: 35, time: 0.295, data: 0.002) G_GAN: 0.275 D_real: 0.216 D_fake: 0.231 G: 1.306 NCE: 1.156 NCE_Y: 0.906 \n",
      "saving the latest model (epoch 106, total_iters 35000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 106, iters: 135, time: 0.295, data: 0.002) G_GAN: 0.277 D_real: 0.262 D_fake: 0.229 G: 1.303 NCE: 1.134 NCE_Y: 0.918 \n",
      "(epoch: 106, iters: 235, time: 0.293, data: 0.002) G_GAN: 0.284 D_real: 0.273 D_fake: 0.222 G: 1.316 NCE: 1.179 NCE_Y: 0.884 \n",
      "End of epoch 106 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 107, iters: 2, time: 0.293, data: 0.002) G_GAN: 0.277 D_real: 0.259 D_fake: 0.229 G: 1.304 NCE: 1.127 NCE_Y: 0.927 \n",
      "(epoch: 107, iters: 102, time: 0.292, data: 0.001) G_GAN: 0.271 D_real: 0.247 D_fake: 0.238 G: 1.233 NCE: 1.077 NCE_Y: 0.848 \n",
      "(epoch: 107, iters: 202, time: 0.293, data: 0.001) G_GAN: 0.310 D_real: 0.203 D_fake: 0.205 G: 1.310 NCE: 1.124 NCE_Y: 0.876 \n",
      "(epoch: 107, iters: 302, time: 0.293, data: 0.001) G_GAN: 0.268 D_real: 0.266 D_fake: 0.237 G: 1.298 NCE: 1.145 NCE_Y: 0.915 \n",
      "End of epoch 107 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 108, iters: 69, time: 0.292, data: 0.001) G_GAN: 0.174 D_real: 0.240 D_fake: 0.370 G: 1.256 NCE: 1.285 NCE_Y: 0.880 \n",
      "(epoch: 108, iters: 169, time: 0.291, data: 0.002) G_GAN: 0.265 D_real: 0.189 D_fake: 0.240 G: 1.282 NCE: 1.119 NCE_Y: 0.915 \n",
      "(epoch: 108, iters: 269, time: 0.293, data: 0.002) G_GAN: 0.266 D_real: 0.264 D_fake: 0.242 G: 1.303 NCE: 1.198 NCE_Y: 0.875 \n",
      "End of epoch 108 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 109, iters: 36, time: 0.293, data: 0.001) G_GAN: 0.275 D_real: 0.252 D_fake: 0.231 G: 1.290 NCE: 1.132 NCE_Y: 0.899 \n",
      "(epoch: 109, iters: 136, time: 0.293, data: 0.003) G_GAN: 0.255 D_real: 0.250 D_fake: 0.249 G: 1.283 NCE: 1.128 NCE_Y: 0.929 \n",
      "(epoch: 109, iters: 236, time: 0.292, data: 0.001) G_GAN: 0.273 D_real: 0.248 D_fake: 0.232 G: 1.284 NCE: 1.134 NCE_Y: 0.888 \n",
      "End of epoch 109 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 110, iters: 3, time: 0.292, data: 0.001) G_GAN: 0.268 D_real: 0.268 D_fake: 0.236 G: 1.272 NCE: 1.127 NCE_Y: 0.881 \n",
      "(epoch: 110, iters: 103, time: 0.292, data: 0.002) G_GAN: 0.262 D_real: 0.246 D_fake: 0.243 G: 1.284 NCE: 1.137 NCE_Y: 0.909 \n",
      "(epoch: 110, iters: 203, time: 0.292, data: 0.001) G_GAN: 0.260 D_real: 0.240 D_fake: 0.245 G: 1.254 NCE: 1.141 NCE_Y: 0.847 \n",
      "(epoch: 110, iters: 303, time: 0.295, data: 0.002) G_GAN: 0.274 D_real: 0.253 D_fake: 0.231 G: 1.297 NCE: 1.137 NCE_Y: 0.909 \n",
      "End of epoch 110 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 111, iters: 70, time: 0.296, data: 0.001) G_GAN: 0.261 D_real: 0.261 D_fake: 0.251 G: 1.255 NCE: 1.108 NCE_Y: 0.880 \n",
      "(epoch: 111, iters: 170, time: 0.296, data: 0.001) G_GAN: 0.261 D_real: 0.228 D_fake: 0.249 G: 1.285 NCE: 1.105 NCE_Y: 0.944 \n",
      "(epoch: 111, iters: 270, time: 0.293, data: 0.001) G_GAN: 0.269 D_real: 0.246 D_fake: 0.236 G: 1.268 NCE: 1.137 NCE_Y: 0.860 \n",
      "End of epoch 111 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 112, iters: 37, time: 0.293, data: 0.001) G_GAN: 0.300 D_real: 0.191 D_fake: 0.219 G: 1.342 NCE: 1.220 NCE_Y: 0.864 \n",
      "(epoch: 112, iters: 137, time: 0.295, data: 0.001) G_GAN: 0.252 D_real: 0.187 D_fake: 0.261 G: 1.310 NCE: 1.116 NCE_Y: 1.000 \n",
      "(epoch: 112, iters: 237, time: 0.295, data: 0.001) G_GAN: 0.267 D_real: 0.251 D_fake: 0.239 G: 1.279 NCE: 1.174 NCE_Y: 0.852 \n",
      "End of epoch 112 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 113, iters: 4, time: 0.294, data: 0.001) G_GAN: 0.255 D_real: 0.246 D_fake: 0.249 G: 1.234 NCE: 1.108 NCE_Y: 0.850 \n",
      "(epoch: 113, iters: 104, time: 0.295, data: 0.002) G_GAN: 0.274 D_real: 0.169 D_fake: 0.236 G: 1.211 NCE: 1.069 NCE_Y: 0.806 \n",
      "(epoch: 113, iters: 204, time: 0.294, data: 0.001) G_GAN: 0.269 D_real: 0.248 D_fake: 0.241 G: 1.280 NCE: 1.101 NCE_Y: 0.919 \n",
      "(epoch: 113, iters: 304, time: 0.293, data: 0.001) G_GAN: 0.268 D_real: 0.256 D_fake: 0.237 G: 1.283 NCE: 1.107 NCE_Y: 0.923 \n",
      "End of epoch 113 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 114, iters: 71, time: 0.294, data: 0.001) G_GAN: 0.258 D_real: 0.244 D_fake: 0.247 G: 1.286 NCE: 1.139 NCE_Y: 0.916 \n",
      "(epoch: 114, iters: 171, time: 0.295, data: 0.002) G_GAN: 0.272 D_real: 0.262 D_fake: 0.239 G: 1.323 NCE: 1.229 NCE_Y: 0.873 \n",
      "(epoch: 114, iters: 271, time: 0.296, data: 0.002) G_GAN: 0.295 D_real: 0.213 D_fake: 0.213 G: 1.338 NCE: 1.145 NCE_Y: 0.941 \n",
      "End of epoch 114 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 115, iters: 38, time: 0.297, data: 0.002) G_GAN: 0.254 D_real: 0.221 D_fake: 0.250 G: 1.255 NCE: 1.160 NCE_Y: 0.841 \n",
      "(epoch: 115, iters: 138, time: 0.296, data: 0.002) G_GAN: 0.271 D_real: 0.266 D_fake: 0.234 G: 1.278 NCE: 1.126 NCE_Y: 0.888 \n",
      "(epoch: 115, iters: 238, time: 0.295, data: 0.001) G_GAN: 0.268 D_real: 0.264 D_fake: 0.236 G: 1.305 NCE: 1.155 NCE_Y: 0.917 \n",
      "End of epoch 115 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 116, iters: 5, time: 0.296, data: 0.001) G_GAN: 0.281 D_real: 0.230 D_fake: 0.226 G: 1.352 NCE: 1.198 NCE_Y: 0.945 \n",
      "(epoch: 116, iters: 105, time: 0.294, data: 0.002) G_GAN: 0.261 D_real: 0.265 D_fake: 0.243 G: 1.254 NCE: 1.129 NCE_Y: 0.857 \n",
      "(epoch: 116, iters: 205, time: 0.294, data: 0.001) G_GAN: 0.227 D_real: 0.266 D_fake: 0.282 G: 1.245 NCE: 1.115 NCE_Y: 0.922 \n",
      "(epoch: 116, iters: 305, time: 0.293, data: 0.002) G_GAN: 0.286 D_real: 0.241 D_fake: 0.222 G: 1.296 NCE: 1.149 NCE_Y: 0.871 \n",
      "End of epoch 116 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 117, iters: 72, time: 0.291, data: 0.001) G_GAN: 0.279 D_real: 0.259 D_fake: 0.235 G: 1.257 NCE: 1.085 NCE_Y: 0.871 \n",
      "(epoch: 117, iters: 172, time: 0.294, data: 0.001) G_GAN: 0.253 D_real: 0.250 D_fake: 0.262 G: 1.256 NCE: 1.108 NCE_Y: 0.899 \n",
      "(epoch: 117, iters: 272, time: 0.295, data: 0.002) G_GAN: 0.269 D_real: 0.258 D_fake: 0.235 G: 1.289 NCE: 1.139 NCE_Y: 0.899 \n",
      "End of epoch 117 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 118, iters: 39, time: 0.291, data: 0.002) G_GAN: 0.263 D_real: 0.204 D_fake: 0.241 G: 1.269 NCE: 1.116 NCE_Y: 0.895 \n",
      "(epoch: 118, iters: 139, time: 0.291, data: 0.002) G_GAN: 0.285 D_real: 0.194 D_fake: 0.226 G: 1.269 NCE: 1.116 NCE_Y: 0.851 \n",
      "(epoch: 118, iters: 239, time: 0.294, data: 0.002) G_GAN: 0.276 D_real: 0.237 D_fake: 0.230 G: 1.326 NCE: 1.213 NCE_Y: 0.886 \n",
      "End of epoch 118 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 119, iters: 6, time: 0.295, data: 0.002) G_GAN: 0.262 D_real: 0.263 D_fake: 0.246 G: 1.302 NCE: 1.201 NCE_Y: 0.879 \n",
      "(epoch: 119, iters: 106, time: 0.296, data: 0.002) G_GAN: 0.265 D_real: 0.253 D_fake: 0.239 G: 1.335 NCE: 1.212 NCE_Y: 0.928 \n",
      "(epoch: 119, iters: 206, time: 0.295, data: 0.002) G_GAN: 0.265 D_real: 0.223 D_fake: 0.240 G: 1.279 NCE: 1.123 NCE_Y: 0.905 \n",
      "(epoch: 119, iters: 306, time: 0.294, data: 0.002) G_GAN: 0.279 D_real: 0.264 D_fake: 0.227 G: 1.282 NCE: 1.130 NCE_Y: 0.877 \n",
      "End of epoch 119 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000010\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 120, iters: 73, time: 0.294, data: 0.002) G_GAN: 0.258 D_real: 0.232 D_fake: 0.246 G: 1.230 NCE: 1.099 NCE_Y: 0.845 \n",
      "(epoch: 120, iters: 173, time: 0.293, data: 0.002) G_GAN: 0.272 D_real: 0.266 D_fake: 0.234 G: 1.292 NCE: 1.146 NCE_Y: 0.893 \n",
      "(epoch: 120, iters: 273, time: 0.294, data: 0.001) G_GAN: 0.270 D_real: 0.263 D_fake: 0.238 G: 1.271 NCE: 1.099 NCE_Y: 0.904 \n",
      "saving the model at the end of epoch 120, iters 39960\n",
      "End of epoch 120 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 121, iters: 40, time: 0.294, data: 0.002) G_GAN: 0.299 D_real: 0.255 D_fake: 0.215 G: 1.333 NCE: 1.154 NCE_Y: 0.913 \n",
      "saving the latest model (epoch 121, total_iters 40000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 121, iters: 140, time: 0.293, data: 0.002) G_GAN: 0.272 D_real: 0.258 D_fake: 0.234 G: 1.289 NCE: 1.146 NCE_Y: 0.888 \n",
      "(epoch: 121, iters: 240, time: 0.293, data: 0.001) G_GAN: 0.273 D_real: 0.253 D_fake: 0.231 G: 1.292 NCE: 1.128 NCE_Y: 0.908 \n",
      "End of epoch 121 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 122, iters: 7, time: 0.295, data: 0.002) G_GAN: 0.259 D_real: 0.197 D_fake: 0.246 G: 1.205 NCE: 1.106 NCE_Y: 0.787 \n",
      "(epoch: 122, iters: 107, time: 0.294, data: 0.000) G_GAN: 0.271 D_real: 0.269 D_fake: 0.239 G: 1.266 NCE: 1.126 NCE_Y: 0.865 \n",
      "(epoch: 122, iters: 207, time: 0.293, data: 0.001) G_GAN: 0.273 D_real: 0.261 D_fake: 0.232 G: 1.266 NCE: 1.096 NCE_Y: 0.889 \n",
      "(epoch: 122, iters: 307, time: 0.294, data: 0.002) G_GAN: 0.266 D_real: 0.246 D_fake: 0.238 G: 1.270 NCE: 1.112 NCE_Y: 0.897 \n",
      "End of epoch 122 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 123, iters: 74, time: 0.294, data: 0.002) G_GAN: 0.269 D_real: 0.245 D_fake: 0.235 G: 1.258 NCE: 1.137 NCE_Y: 0.841 \n",
      "(epoch: 123, iters: 174, time: 0.291, data: 0.002) G_GAN: 0.256 D_real: 0.256 D_fake: 0.248 G: 1.275 NCE: 1.130 NCE_Y: 0.907 \n",
      "(epoch: 123, iters: 274, time: 0.294, data: 0.002) G_GAN: 0.276 D_real: 0.248 D_fake: 0.229 G: 1.284 NCE: 1.121 NCE_Y: 0.895 \n",
      "End of epoch 123 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 124, iters: 41, time: 0.295, data: 0.001) G_GAN: 0.277 D_real: 0.258 D_fake: 0.229 G: 1.260 NCE: 1.131 NCE_Y: 0.834 \n",
      "(epoch: 124, iters: 141, time: 0.294, data: 0.002) G_GAN: 0.256 D_real: 0.251 D_fake: 0.248 G: 1.268 NCE: 1.112 NCE_Y: 0.911 \n",
      "(epoch: 124, iters: 241, time: 0.294, data: 0.002) G_GAN: 0.279 D_real: 0.257 D_fake: 0.234 G: 1.255 NCE: 1.095 NCE_Y: 0.857 \n",
      "End of epoch 124 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 125, iters: 8, time: 0.292, data: 0.002) G_GAN: 0.277 D_real: 0.239 D_fake: 0.229 G: 1.237 NCE: 1.110 NCE_Y: 0.810 \n",
      "(epoch: 125, iters: 108, time: 0.295, data: 0.002) G_GAN: 0.256 D_real: 0.268 D_fake: 0.249 G: 1.256 NCE: 1.137 NCE_Y: 0.862 \n",
      "(epoch: 125, iters: 208, time: 0.296, data: 0.002) G_GAN: 0.272 D_real: 0.251 D_fake: 0.233 G: 1.304 NCE: 1.140 NCE_Y: 0.923 \n",
      "(epoch: 125, iters: 308, time: 0.296, data: 0.002) G_GAN: 0.272 D_real: 0.225 D_fake: 0.234 G: 1.273 NCE: 1.115 NCE_Y: 0.888 \n",
      "End of epoch 125 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 126, iters: 75, time: 0.295, data: 0.002) G_GAN: 0.269 D_real: 0.263 D_fake: 0.236 G: 1.299 NCE: 1.146 NCE_Y: 0.913 \n",
      "(epoch: 126, iters: 175, time: 0.294, data: 0.002) G_GAN: 0.272 D_real: 0.235 D_fake: 0.236 G: 1.256 NCE: 1.133 NCE_Y: 0.835 \n",
      "(epoch: 126, iters: 275, time: 0.294, data: 0.002) G_GAN: 0.260 D_real: 0.196 D_fake: 0.246 G: 1.304 NCE: 1.234 NCE_Y: 0.854 \n",
      "End of epoch 126 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 127, iters: 42, time: 0.294, data: 0.001) G_GAN: 0.290 D_real: 0.255 D_fake: 0.218 G: 1.331 NCE: 1.196 NCE_Y: 0.887 \n",
      "(epoch: 127, iters: 142, time: 0.291, data: 0.002) G_GAN: 0.277 D_real: 0.231 D_fake: 0.229 G: 1.380 NCE: 1.213 NCE_Y: 0.992 \n",
      "(epoch: 127, iters: 242, time: 0.294, data: 0.002) G_GAN: 0.283 D_real: 0.217 D_fake: 0.223 G: 1.258 NCE: 1.129 NCE_Y: 0.820 \n",
      "End of epoch 127 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 128, iters: 9, time: 0.294, data: 0.002) G_GAN: 0.259 D_real: 0.259 D_fake: 0.248 G: 1.291 NCE: 1.186 NCE_Y: 0.877 \n",
      "(epoch: 128, iters: 109, time: 0.294, data: 0.002) G_GAN: 0.277 D_real: 0.248 D_fake: 0.229 G: 1.271 NCE: 1.125 NCE_Y: 0.862 \n",
      "(epoch: 128, iters: 209, time: 0.293, data: 0.001) G_GAN: 0.286 D_real: 0.251 D_fake: 0.224 G: 1.321 NCE: 1.150 NCE_Y: 0.920 \n",
      "(epoch: 128, iters: 309, time: 0.293, data: 0.001) G_GAN: 0.268 D_real: 0.230 D_fake: 0.239 G: 1.346 NCE: 1.167 NCE_Y: 0.989 \n",
      "End of epoch 128 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 129, iters: 76, time: 0.294, data: 0.002) G_GAN: 0.276 D_real: 0.253 D_fake: 0.229 G: 1.276 NCE: 1.110 NCE_Y: 0.889 \n",
      "(epoch: 129, iters: 176, time: 0.295, data: 0.002) G_GAN: 0.271 D_real: 0.240 D_fake: 0.235 G: 1.240 NCE: 1.140 NCE_Y: 0.798 \n",
      "(epoch: 129, iters: 276, time: 0.294, data: 0.002) G_GAN: 0.294 D_real: 0.241 D_fake: 0.216 G: 1.308 NCE: 1.136 NCE_Y: 0.893 \n",
      "End of epoch 129 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 130, iters: 43, time: 0.295, data: 0.002) G_GAN: 0.247 D_real: 0.259 D_fake: 0.258 G: 1.270 NCE: 1.147 NCE_Y: 0.900 \n",
      "(epoch: 130, iters: 143, time: 0.294, data: 0.002) G_GAN: 0.267 D_real: 0.249 D_fake: 0.237 G: 1.251 NCE: 1.115 NCE_Y: 0.852 \n",
      "(epoch: 130, iters: 243, time: 0.292, data: 0.002) G_GAN: 0.266 D_real: 0.235 D_fake: 0.239 G: 1.285 NCE: 1.132 NCE_Y: 0.907 \n",
      "End of epoch 130 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 131, iters: 10, time: 0.293, data: 0.002) G_GAN: 0.270 D_real: 0.220 D_fake: 0.235 G: 1.224 NCE: 1.109 NCE_Y: 0.798 \n",
      "(epoch: 131, iters: 110, time: 0.292, data: 0.002) G_GAN: 0.280 D_real: 0.173 D_fake: 0.227 G: 1.269 NCE: 1.116 NCE_Y: 0.862 \n",
      "(epoch: 131, iters: 210, time: 0.293, data: 0.002) G_GAN: 0.283 D_real: 0.257 D_fake: 0.227 G: 1.292 NCE: 1.135 NCE_Y: 0.883 \n",
      "(epoch: 131, iters: 310, time: 0.295, data: 0.002) G_GAN: 0.281 D_real: 0.250 D_fake: 0.230 G: 1.308 NCE: 1.189 NCE_Y: 0.864 \n",
      "End of epoch 131 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 132, iters: 77, time: 0.295, data: 0.001) G_GAN: 0.269 D_real: 0.253 D_fake: 0.235 G: 1.287 NCE: 1.132 NCE_Y: 0.903 \n",
      "(epoch: 132, iters: 177, time: 0.295, data: 0.001) G_GAN: 0.295 D_real: 0.235 D_fake: 0.218 G: 1.290 NCE: 1.094 NCE_Y: 0.895 \n",
      "(epoch: 132, iters: 277, time: 0.293, data: 0.002) G_GAN: 0.264 D_real: 0.254 D_fake: 0.240 G: 1.292 NCE: 1.147 NCE_Y: 0.909 \n",
      "End of epoch 132 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 133, iters: 44, time: 0.293, data: 0.002) G_GAN: 0.268 D_real: 0.256 D_fake: 0.236 G: 1.269 NCE: 1.126 NCE_Y: 0.876 \n",
      "(epoch: 133, iters: 144, time: 0.293, data: 0.003) G_GAN: 0.269 D_real: 0.248 D_fake: 0.236 G: 1.273 NCE: 1.111 NCE_Y: 0.898 \n",
      "(epoch: 133, iters: 244, time: 0.295, data: 0.002) G_GAN: 0.267 D_real: 0.236 D_fake: 0.240 G: 1.224 NCE: 1.074 NCE_Y: 0.840 \n",
      "End of epoch 133 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 134, iters: 11, time: 0.296, data: 0.002) G_GAN: 0.274 D_real: 0.254 D_fake: 0.232 G: 1.274 NCE: 1.126 NCE_Y: 0.873 \n",
      "(epoch: 134, iters: 111, time: 0.295, data: 0.002) G_GAN: 0.256 D_real: 0.219 D_fake: 0.259 G: 1.288 NCE: 1.170 NCE_Y: 0.894 \n",
      "(epoch: 134, iters: 211, time: 0.296, data: 0.002) G_GAN: 0.261 D_real: 0.036 D_fake: 0.244 G: 1.392 NCE: 1.126 NCE_Y: 1.138 \n",
      "(epoch: 134, iters: 311, time: 0.297, data: 0.003) G_GAN: 0.266 D_real: 0.260 D_fake: 0.238 G: 1.292 NCE: 1.137 NCE_Y: 0.915 \n",
      "End of epoch 134 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 135, iters: 78, time: 0.296, data: 0.001) G_GAN: 0.275 D_real: 0.249 D_fake: 0.230 G: 1.257 NCE: 1.111 NCE_Y: 0.853 \n",
      "(epoch: 135, iters: 178, time: 0.297, data: 0.001) G_GAN: 0.273 D_real: 0.256 D_fake: 0.231 G: 1.243 NCE: 1.128 NCE_Y: 0.812 \n",
      "(epoch: 135, iters: 278, time: 0.296, data: 0.002) G_GAN: 0.275 D_real: 0.269 D_fake: 0.232 G: 1.311 NCE: 1.156 NCE_Y: 0.916 \n",
      "End of epoch 135 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 136, iters: 45, time: 0.295, data: 0.002) G_GAN: 0.274 D_real: 0.203 D_fake: 0.235 G: 1.212 NCE: 1.084 NCE_Y: 0.791 \n",
      "saving the latest model (epoch 136, total_iters 45000)\n",
      "cut_b18z40_63x_unaligned_resize\n",
      "(epoch: 136, iters: 145, time: 0.295, data: 0.002) G_GAN: 0.275 D_real: 0.227 D_fake: 0.235 G: 1.298 NCE: 1.203 NCE_Y: 0.845 \n",
      "(epoch: 136, iters: 245, time: 0.294, data: 0.001) G_GAN: 0.274 D_real: 0.244 D_fake: 0.231 G: 1.257 NCE: 1.111 NCE_Y: 0.855 \n",
      "End of epoch 136 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 137, iters: 12, time: 0.295, data: 0.001) G_GAN: 0.260 D_real: 0.267 D_fake: 0.265 G: 1.406 NCE: 1.401 NCE_Y: 0.890 \n",
      "(epoch: 137, iters: 112, time: 0.295, data: 0.001) G_GAN: 0.258 D_real: 0.248 D_fake: 0.254 G: 1.254 NCE: 1.103 NCE_Y: 0.890 \n",
      "(epoch: 137, iters: 212, time: 0.293, data: 0.002) G_GAN: 0.243 D_real: 0.249 D_fake: 0.266 G: 1.234 NCE: 1.095 NCE_Y: 0.887 \n",
      "(epoch: 137, iters: 312, time: 0.295, data: 0.002) G_GAN: 0.238 D_real: 0.271 D_fake: 0.273 G: 1.205 NCE: 1.113 NCE_Y: 0.821 \n",
      "End of epoch 137 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 138, iters: 79, time: 0.291, data: 0.002) G_GAN: 0.273 D_real: 0.215 D_fake: 0.232 G: 1.276 NCE: 1.124 NCE_Y: 0.881 \n",
      "(epoch: 138, iters: 179, time: 0.293, data: 0.002) G_GAN: 0.267 D_real: 0.214 D_fake: 0.246 G: 1.332 NCE: 1.151 NCE_Y: 0.979 \n",
      "(epoch: 138, iters: 279, time: 0.293, data: 0.001) G_GAN: 0.255 D_real: 0.247 D_fake: 0.256 G: 1.249 NCE: 1.090 NCE_Y: 0.899 \n",
      "End of epoch 138 / 350 \t Time Taken: 98 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 139, iters: 46, time: 0.292, data: 0.003) G_GAN: 0.293 D_real: 0.252 D_fake: 0.218 G: 1.296 NCE: 1.097 NCE_Y: 0.910 \n",
      "(epoch: 139, iters: 146, time: 0.292, data: 0.001) G_GAN: 0.268 D_real: 0.245 D_fake: 0.237 G: 1.286 NCE: 1.136 NCE_Y: 0.899 \n",
      "(epoch: 139, iters: 246, time: 0.294, data: 0.002) G_GAN: 0.274 D_real: 0.252 D_fake: 0.236 G: 1.268 NCE: 1.128 NCE_Y: 0.861 \n",
      "End of epoch 139 / 350 \t Time Taken: 99 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 140, iters: 13, time: 0.294, data: 0.001) G_GAN: 0.277 D_real: 0.251 D_fake: 0.229 G: 1.305 NCE: 1.133 NCE_Y: 0.924 \n",
      "(epoch: 140, iters: 113, time: 0.296, data: 0.002) G_GAN: 0.277 D_real: 0.296 D_fake: 0.229 G: 1.338 NCE: 1.200 NCE_Y: 0.922 \n",
      "(epoch: 140, iters: 213, time: 0.297, data: 0.001) G_GAN: 0.251 D_real: 0.256 D_fake: 0.257 G: 1.294 NCE: 1.197 NCE_Y: 0.889 \n",
      "(epoch: 140, iters: 313, time: 0.295, data: 0.001) G_GAN: 0.260 D_real: 0.263 D_fake: 0.244 G: 1.267 NCE: 1.142 NCE_Y: 0.872 \n",
      "End of epoch 140 / 350 \t Time Taken: 100 sec\n",
      "learning rate = 0.0000001\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "(epoch: 141, iters: 80, time: 0.295, data: 0.001) G_GAN: 0.282 D_real: 0.196 D_fake: 0.226 G: 1.323 NCE: 1.140 NCE_Y: 0.942 \n",
      "(epoch: 141, iters: 180, time: 0.295, data: 0.002) G_GAN: 0.266 D_real: 0.237 D_fake: 0.239 G: 1.270 NCE: 1.194 NCE_Y: 0.813 \n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Unaligned/train/ \\\n",
    "--name cut_b18z40_63x_unaligned_resize --lr=0.0001 --lr_decay_iters=40 --lr_policy step --batch_size=1 \\\n",
    "--preprocess resize --load_size=256 --save_epoch_freq=60 --n_epochs=150  --display_port=8087 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test unaligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                 CUT_mode: CUT                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/\t[default: placeholder]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "               easy_label: experiment_name               \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "        flip_equivariance: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: xavier                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "               lambda_GAN: 1.0                           \n",
      "               lambda_NCE: 1.0                           \n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cut                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: cut_b18z40_63x_unaligned_resize\t[default: experiment_name]\n",
      "                    nce_T: 0.07                          \n",
      "                  nce_idt: True                          \n",
      "nce_includes_all_negatives_from_minibatch: False                         \n",
      "               nce_layers: 0,4,8,12,16                   \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netF: mlp_sample                    \n",
      "                  netF_nc: 256                           \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "             no_antialias: False                         \n",
      "          no_antialias_up: False                         \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                    normD: instance                      \n",
      "                    normG: instance                      \n",
      "              num_patches: 256                           \n",
      "                 num_test: 99999                         \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \t[default: test]\n",
      "                pool_size: 0                             \n",
      "               preprocess: resize                        \t[default: resize_and_crop]\n",
      "         random_scale_max: 3.0                           \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "stylegan2_G_num_downsampling: 1                             \n",
      "                   suffix:                               \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "dataset [UnalignedDataset] was created\n",
      "model [CUTModel] was created\n",
      "creating web directory ./results/cut_b18z40_63x_unaligned_resize/train_latest\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "loading the model from ./checkpoints/cut_b18z40_63x_unaligned_resize/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "processing (0000)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m1700_c1_z1_l1_o0_1.png']\n",
      "processing (0005)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m1800_c1_z1_l1_o0_3.png']\n",
      "processing (0010)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m2030_c1_z1_l1_o0_2.png']\n",
      "processing (0015)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m2720_c1_z1_l1_o0_1.png']\n",
      "processing (0020)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m2800_c1_z1_l1_o0_3.png']\n",
      "processing (0025)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m3500_c1_z1_l1_o0_2.png']\n",
      "processing (0030)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m4010_c1_z1_l1_o0_1.png']\n",
      "processing (0035)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m4320_c1_z1_l1_o0_3.png']\n",
      "processing (0040)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m4810_c1_z1_l1_o0_2.png']\n",
      "processing (0045)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m5030_c1_z1_l1_o0_1.png']\n",
      "processing (0050)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m5110_c1_z1_l1_o0_3.png']\n",
      "processing (0055)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m5220_c1_z1_l1_o0_2.png']\n",
      "processing (0060)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m5710_c1_z1_l1_o0_1.png']\n",
      "processing (0065)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m6010_c1_z1_l1_o0_3.png']\n",
      "processing (0070)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m6030_c1_z1_l1_o0_2.png']\n",
      "processing (0075)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m6220_c1_z1_l1_o0_1.png']\n",
      "processing (0080)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/trainA/p1_wA1_t1_m6300_c1_z1_l1_o0_3.png']\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/test/ \\\n",
    "--preprocess resize --load_size=256 --name cut_b18z40_63x_unaligned_resize --CUT_mode CUT --phase train --num_test=99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on image resize ALGINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                augment_a: False                         \n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "               brightness: True                          \t[default: False]\n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                 contrast: True                          \t[default: False]\n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/train/\t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8087                          \t[default: 8097]\n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                    gauss: True                          \t[default: False]\n",
      "                  gpu_ids: 2                             \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.45                          \t[default: 0.5]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \t[default: 286]\n",
      "                       lr: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 40                            \t[default: 50]\n",
      "                lr_policy: step                          \t[default: linear]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 200                           \t[default: 100]\n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: cycle_b18z40_63x_3f_resize_aligned\t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: resize                        \t[default: resize_and_crop]\n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 60                            \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                sharpness: True                          \t[default: False]\n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 333\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "Setting up a new session...\n",
      "create web directory ./checkpoints/cycle_b18z40_63x_3f_resize_aligned/web...\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 1, iters: 100, time: 0.288, data: 0.315) D_A: 0.346 G_A: 0.403 cycle_A: 1.567 idt_A: 1.550 D_B: 0.358 G_B: 0.397 cycle_B: 3.565 idt_B: 0.588 \n",
      "(epoch: 1, iters: 200, time: 0.307, data: 0.002) D_A: 0.378 G_A: 0.319 cycle_A: 0.781 idt_A: 2.247 D_B: 0.258 G_B: 0.368 cycle_B: 4.876 idt_B: 0.364 \n",
      "(epoch: 1, iters: 300, time: 0.304, data: 0.003) D_A: 0.341 G_A: 0.458 cycle_A: 1.101 idt_A: 0.587 D_B: 0.297 G_B: 0.411 cycle_B: 1.486 idt_B: 0.370 \n",
      "End of epoch 1 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 2, iters: 67, time: 6.940, data: 0.002) D_A: 0.337 G_A: 0.282 cycle_A: 1.339 idt_A: 0.700 D_B: 0.276 G_B: 0.264 cycle_B: 1.411 idt_B: 0.738 \n",
      "(epoch: 2, iters: 167, time: 0.308, data: 0.002) D_A: 0.293 G_A: 0.435 cycle_A: 0.903 idt_A: 0.617 D_B: 0.272 G_B: 0.349 cycle_B: 1.262 idt_B: 0.432 \n",
      "(epoch: 2, iters: 267, time: 0.304, data: 0.002) D_A: 0.282 G_A: 0.226 cycle_A: 1.610 idt_A: 0.990 D_B: 0.199 G_B: 0.546 cycle_B: 2.022 idt_B: 0.723 \n",
      "End of epoch 2 / 300 \t Time Taken: 100 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 3, iters: 34, time: 0.311, data: 0.002) D_A: 0.170 G_A: 0.653 cycle_A: 1.313 idt_A: 1.091 D_B: 0.312 G_B: 0.144 cycle_B: 2.595 idt_B: 0.580 \n",
      "(epoch: 3, iters: 134, time: 2.433, data: 0.002) D_A: 0.088 G_A: 0.479 cycle_A: 0.967 idt_A: 2.127 D_B: 0.271 G_B: 0.361 cycle_B: 4.584 idt_B: 0.456 \n",
      "(epoch: 3, iters: 234, time: 0.305, data: 0.003) D_A: 0.115 G_A: 0.625 cycle_A: 1.483 idt_A: 1.688 D_B: 0.289 G_B: 0.194 cycle_B: 3.936 idt_B: 0.629 \n",
      "End of epoch 3 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 4, iters: 1, time: 0.277, data: 0.002) D_A: 0.132 G_A: 0.649 cycle_A: 2.841 idt_A: 2.758 D_B: 0.130 G_B: 0.404 cycle_B: 6.351 idt_B: 1.309 \n",
      "(epoch: 4, iters: 101, time: 0.283, data: 0.001) D_A: 0.057 G_A: 0.270 cycle_A: 2.335 idt_A: 1.908 D_B: 0.109 G_B: 0.604 cycle_B: 4.144 idt_B: 1.049 \n",
      "(epoch: 4, iters: 201, time: 1.047, data: 0.002) D_A: 0.178 G_A: 0.346 cycle_A: 1.085 idt_A: 2.224 D_B: 0.274 G_B: 0.383 cycle_B: 5.119 idt_B: 0.454 \n",
      "(epoch: 4, iters: 301, time: 0.303, data: 0.002) D_A: 0.140 G_A: 0.334 cycle_A: 1.380 idt_A: 0.558 D_B: 0.309 G_B: 0.096 cycle_B: 1.160 idt_B: 0.592 \n",
      "End of epoch 4 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 5, iters: 68, time: 0.287, data: 0.002) D_A: 0.243 G_A: 0.818 cycle_A: 1.707 idt_A: 1.162 D_B: 0.125 G_B: 0.596 cycle_B: 2.613 idt_B: 0.742 \n",
      "(epoch: 5, iters: 168, time: 0.310, data: 0.002) D_A: 0.049 G_A: 0.749 cycle_A: 2.768 idt_A: 1.396 D_B: 0.531 G_B: 0.044 cycle_B: 2.915 idt_B: 1.265 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 5, iters: 268, time: 0.782, data: 0.002) D_A: 0.263 G_A: 0.223 cycle_A: 1.393 idt_A: 0.516 D_B: 0.235 G_B: 0.715 cycle_B: 1.253 idt_B: 0.603 \n",
      "End of epoch 5 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 6, iters: 35, time: 0.296, data: 0.002) D_A: 0.070 G_A: 0.561 cycle_A: 0.832 idt_A: 1.310 D_B: 0.105 G_B: 0.614 cycle_B: 2.847 idt_B: 0.384 \n",
      "(epoch: 6, iters: 135, time: 0.305, data: 0.002) D_A: 0.082 G_A: 0.699 cycle_A: 1.310 idt_A: 1.708 D_B: 0.275 G_B: 0.295 cycle_B: 3.864 idt_B: 0.446 \n",
      "(epoch: 6, iters: 235, time: 0.312, data: 0.002) D_A: 0.283 G_A: 0.566 cycle_A: 1.579 idt_A: 0.569 D_B: 0.201 G_B: 0.472 cycle_B: 1.557 idt_B: 0.707 \n",
      "End of epoch 6 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 7, iters: 2, time: 0.812, data: 0.002) D_A: 0.092 G_A: 0.394 cycle_A: 0.631 idt_A: 2.435 D_B: 0.267 G_B: 0.212 cycle_B: 5.726 idt_B: 0.286 \n",
      "(epoch: 7, iters: 102, time: 0.306, data: 0.002) D_A: 0.084 G_A: 0.499 cycle_A: 1.282 idt_A: 1.552 D_B: 0.144 G_B: 0.566 cycle_B: 3.229 idt_B: 0.585 \n",
      "(epoch: 7, iters: 202, time: 0.291, data: 0.001) D_A: 0.063 G_A: 0.567 cycle_A: 1.197 idt_A: 0.877 D_B: 0.351 G_B: 0.109 cycle_B: 1.884 idt_B: 0.496 \n",
      "(epoch: 7, iters: 302, time: 0.299, data: 0.002) D_A: 0.192 G_A: 0.259 cycle_A: 1.542 idt_A: 0.709 D_B: 0.163 G_B: 0.268 cycle_B: 1.502 idt_B: 0.691 \n",
      "End of epoch 7 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 8, iters: 69, time: 0.735, data: 0.002) D_A: 0.149 G_A: 0.635 cycle_A: 3.233 idt_A: 1.106 D_B: 0.084 G_B: 0.303 cycle_B: 2.382 idt_B: 1.432 \n",
      "(epoch: 8, iters: 169, time: 0.308, data: 0.002) D_A: 0.171 G_A: 0.355 cycle_A: 1.686 idt_A: 0.620 D_B: 0.204 G_B: 0.242 cycle_B: 1.184 idt_B: 0.696 \n",
      "(epoch: 8, iters: 269, time: 0.303, data: 0.002) D_A: 0.074 G_A: 0.493 cycle_A: 1.436 idt_A: 1.144 D_B: 0.178 G_B: 0.256 cycle_B: 2.619 idt_B: 0.609 \n",
      "End of epoch 8 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 9, iters: 36, time: 0.288, data: 0.003) D_A: 0.025 G_A: 0.796 cycle_A: 1.351 idt_A: 2.362 D_B: 0.254 G_B: 0.244 cycle_B: 5.166 idt_B: 0.544 \n",
      "(epoch: 9, iters: 136, time: 0.887, data: 0.002) D_A: 0.030 G_A: 0.803 cycle_A: 1.426 idt_A: 1.933 D_B: 0.293 G_B: 0.435 cycle_B: 4.649 idt_B: 0.675 \n",
      "(epoch: 9, iters: 236, time: 0.299, data: 0.002) D_A: 0.190 G_A: 0.245 cycle_A: 0.897 idt_A: 2.292 D_B: 0.198 G_B: 0.563 cycle_B: 6.228 idt_B: 0.392 \n",
      "End of epoch 9 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 10, iters: 3, time: 0.303, data: 0.001) D_A: 0.058 G_A: 0.532 cycle_A: 1.087 idt_A: 2.175 D_B: 0.242 G_B: 0.348 cycle_B: 4.617 idt_B: 0.491 \n",
      "(epoch: 10, iters: 103, time: 0.308, data: 0.002) D_A: 0.329 G_A: 0.763 cycle_A: 1.390 idt_A: 0.532 D_B: 0.246 G_B: 0.259 cycle_B: 1.225 idt_B: 0.433 \n",
      "(epoch: 10, iters: 203, time: 0.842, data: 0.002) D_A: 0.218 G_A: 0.372 cycle_A: 2.366 idt_A: 1.011 D_B: 0.234 G_B: 0.300 cycle_B: 2.399 idt_B: 0.976 \n",
      "(epoch: 10, iters: 303, time: 0.311, data: 0.002) D_A: 0.087 G_A: 0.489 cycle_A: 1.754 idt_A: 1.645 D_B: 0.242 G_B: 0.151 cycle_B: 4.161 idt_B: 0.823 \n",
      "End of epoch 10 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 11, iters: 70, time: 0.306, data: 0.002) D_A: 0.096 G_A: 0.251 cycle_A: 1.272 idt_A: 1.391 D_B: 0.239 G_B: 0.379 cycle_B: 3.050 idt_B: 0.705 \n",
      "(epoch: 11, iters: 170, time: 0.293, data: 0.002) D_A: 0.064 G_A: 0.546 cycle_A: 2.148 idt_A: 1.786 D_B: 0.130 G_B: 0.341 cycle_B: 4.038 idt_B: 1.012 \n",
      "(epoch: 11, iters: 270, time: 0.855, data: 0.002) D_A: 0.281 G_A: 0.683 cycle_A: 0.765 idt_A: 0.530 D_B: 0.220 G_B: 0.442 cycle_B: 1.406 idt_B: 0.303 \n",
      "End of epoch 11 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 12, iters: 37, time: 0.312, data: 0.002) D_A: 0.339 G_A: 0.496 cycle_A: 0.926 idt_A: 0.596 D_B: 0.541 G_B: 0.864 cycle_B: 1.551 idt_B: 0.322 \n",
      "(epoch: 12, iters: 137, time: 0.310, data: 0.001) D_A: 0.220 G_A: 1.015 cycle_A: 1.900 idt_A: 1.577 D_B: 0.287 G_B: 0.104 cycle_B: 3.821 idt_B: 0.694 \n",
      "(epoch: 12, iters: 237, time: 0.306, data: 0.002) D_A: 0.393 G_A: 0.713 cycle_A: 3.640 idt_A: 0.452 D_B: 0.084 G_B: 0.432 cycle_B: 1.189 idt_B: 1.627 \n",
      "End of epoch 12 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 13, iters: 4, time: 0.829, data: 0.002) D_A: 0.059 G_A: 0.486 cycle_A: 1.423 idt_A: 1.946 D_B: 0.215 G_B: 0.356 cycle_B: 4.141 idt_B: 0.650 \n",
      "(epoch: 13, iters: 104, time: 0.307, data: 0.002) D_A: 0.049 G_A: 0.899 cycle_A: 0.923 idt_A: 1.126 D_B: 0.154 G_B: 0.875 cycle_B: 2.472 idt_B: 0.342 \n",
      "(epoch: 13, iters: 204, time: 0.310, data: 0.002) D_A: 0.308 G_A: 0.443 cycle_A: 1.367 idt_A: 0.514 D_B: 0.037 G_B: 0.636 cycle_B: 1.081 idt_B: 0.626 \n",
      "(epoch: 13, iters: 304, time: 0.293, data: 0.002) D_A: 0.147 G_A: 0.268 cycle_A: 1.311 idt_A: 1.383 D_B: 0.184 G_B: 0.547 cycle_B: 3.245 idt_B: 0.544 \n",
      "End of epoch 13 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 14, iters: 71, time: 0.852, data: 0.002) D_A: 0.388 G_A: 0.497 cycle_A: 2.632 idt_A: 0.485 D_B: 0.065 G_B: 0.115 cycle_B: 1.088 idt_B: 1.154 \n",
      "(epoch: 14, iters: 171, time: 0.310, data: 0.002) D_A: 0.145 G_A: 0.370 cycle_A: 1.038 idt_A: 0.930 D_B: 0.153 G_B: 0.176 cycle_B: 2.124 idt_B: 0.355 \n",
      "(epoch: 14, iters: 271, time: 0.311, data: 0.002) D_A: 0.066 G_A: 0.796 cycle_A: 1.824 idt_A: 1.925 D_B: 0.106 G_B: 0.345 cycle_B: 4.524 idt_B: 0.732 \n",
      "End of epoch 14 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 15, iters: 38, time: 0.293, data: 0.001) D_A: 0.224 G_A: 1.002 cycle_A: 0.994 idt_A: 1.181 D_B: 0.230 G_B: 0.272 cycle_B: 2.835 idt_B: 0.443 \n",
      "(epoch: 15, iters: 138, time: 0.866, data: 0.001) D_A: 0.130 G_A: 0.515 cycle_A: 1.765 idt_A: 0.422 D_B: 0.133 G_B: 0.074 cycle_B: 0.970 idt_B: 0.818 \n",
      "(epoch: 15, iters: 238, time: 0.310, data: 0.002) D_A: 0.318 G_A: 0.297 cycle_A: 10.463 idt_A: 0.546 D_B: 0.043 G_B: 0.421 cycle_B: 1.268 idt_B: 4.477 \n",
      "End of epoch 15 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 16, iters: 5, time: 0.306, data: 0.001) D_A: 0.349 G_A: 0.278 cycle_A: 2.703 idt_A: 0.643 D_B: 0.166 G_B: 0.264 cycle_B: 1.573 idt_B: 1.194 \n",
      "saving the latest model (epoch 16, total_iters 5000)\n",
      "(epoch: 16, iters: 105, time: 0.302, data: 0.002) D_A: 0.186 G_A: 0.291 cycle_A: 2.786 idt_A: 0.721 D_B: 0.066 G_B: 0.682 cycle_B: 1.741 idt_B: 1.233 \n",
      "(epoch: 16, iters: 205, time: 0.858, data: 0.002) D_A: 0.032 G_A: 0.795 cycle_A: 1.114 idt_A: 1.789 D_B: 0.494 G_B: 0.634 cycle_B: 3.900 idt_B: 0.501 \n",
      "(epoch: 16, iters: 305, time: 0.311, data: 0.002) D_A: 0.190 G_A: 0.291 cycle_A: 1.887 idt_A: 0.305 D_B: 0.187 G_B: 0.346 cycle_B: 0.819 idt_B: 0.689 \n",
      "End of epoch 16 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 17, iters: 72, time: 0.301, data: 0.001) D_A: 0.245 G_A: 0.319 cycle_A: 1.392 idt_A: 0.834 D_B: 0.053 G_B: 0.387 cycle_B: 1.675 idt_B: 0.584 \n",
      "(epoch: 17, iters: 172, time: 0.282, data: 0.002) D_A: 0.158 G_A: 0.394 cycle_A: 0.840 idt_A: 0.572 D_B: 0.171 G_B: 0.534 cycle_B: 1.498 idt_B: 0.327 \n",
      "(epoch: 17, iters: 272, time: 0.900, data: 0.001) D_A: 0.297 G_A: 0.587 cycle_A: 1.698 idt_A: 0.338 D_B: 0.114 G_B: 0.387 cycle_B: 0.960 idt_B: 0.657 \n",
      "End of epoch 17 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 18, iters: 39, time: 0.305, data: 0.001) D_A: 0.123 G_A: 0.370 cycle_A: 1.295 idt_A: 1.539 D_B: 0.192 G_B: 0.213 cycle_B: 3.697 idt_B: 0.530 \n",
      "(epoch: 18, iters: 139, time: 0.284, data: 0.002) D_A: 0.294 G_A: 0.529 cycle_A: 2.302 idt_A: 0.462 D_B: 0.455 G_B: 0.041 cycle_B: 1.159 idt_B: 1.093 \n",
      "(epoch: 18, iters: 239, time: 0.287, data: 0.002) D_A: 0.212 G_A: 0.181 cycle_A: 1.467 idt_A: 1.443 D_B: 0.244 G_B: 0.143 cycle_B: 3.113 idt_B: 0.637 \n",
      "End of epoch 18 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 19, iters: 6, time: 0.897, data: 0.002) D_A: 0.172 G_A: 0.448 cycle_A: 1.714 idt_A: 0.856 D_B: 0.064 G_B: 0.393 cycle_B: 2.071 idt_B: 0.797 \n",
      "(epoch: 19, iters: 106, time: 0.306, data: 0.002) D_A: 0.054 G_A: 0.391 cycle_A: 2.707 idt_A: 0.793 D_B: 0.369 G_B: 0.227 cycle_B: 1.963 idt_B: 1.368 \n",
      "(epoch: 19, iters: 206, time: 0.310, data: 0.002) D_A: 0.279 G_A: 0.213 cycle_A: 1.143 idt_A: 0.457 D_B: 0.475 G_B: 0.539 cycle_B: 1.311 idt_B: 0.480 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 19, iters: 306, time: 0.294, data: 0.002) D_A: 0.081 G_A: 0.463 cycle_A: 2.859 idt_A: 1.555 D_B: 0.122 G_B: 0.125 cycle_B: 3.484 idt_B: 1.069 \n",
      "End of epoch 19 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 20, iters: 73, time: 1.012, data: 0.001) D_A: 0.088 G_A: 0.478 cycle_A: 1.019 idt_A: 0.775 D_B: 0.248 G_B: 0.708 cycle_B: 2.170 idt_B: 0.352 \n",
      "(epoch: 20, iters: 173, time: 0.310, data: 0.003) D_A: 0.214 G_A: 0.150 cycle_A: 2.982 idt_A: 1.499 D_B: 0.137 G_B: 0.307 cycle_B: 2.993 idt_B: 1.208 \n",
      "(epoch: 20, iters: 273, time: 0.293, data: 0.002) D_A: 0.164 G_A: 0.288 cycle_A: 0.415 idt_A: 0.858 D_B: 0.228 G_B: 0.217 cycle_B: 2.111 idt_B: 0.178 \n",
      "End of epoch 20 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 21, iters: 40, time: 0.311, data: 0.002) D_A: 0.196 G_A: 0.215 cycle_A: 1.583 idt_A: 1.200 D_B: 0.257 G_B: 0.270 cycle_B: 2.414 idt_B: 0.721 \n",
      "(epoch: 21, iters: 140, time: 0.921, data: 0.001) D_A: 0.245 G_A: 0.440 cycle_A: 0.866 idt_A: 1.333 D_B: 0.144 G_B: 0.314 cycle_B: 3.439 idt_B: 0.415 \n",
      "(epoch: 21, iters: 240, time: 0.307, data: 0.002) D_A: 0.596 G_A: 0.480 cycle_A: 2.428 idt_A: 1.254 D_B: 0.283 G_B: 0.651 cycle_B: 2.901 idt_B: 0.934 \n",
      "End of epoch 21 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 22, iters: 7, time: 0.313, data: 0.002) D_A: 0.259 G_A: 0.370 cycle_A: 6.732 idt_A: 0.515 D_B: 0.179 G_B: 0.221 cycle_B: 1.164 idt_B: 2.685 \n",
      "(epoch: 22, iters: 107, time: 0.291, data: 0.002) D_A: 0.252 G_A: 0.233 cycle_A: 0.875 idt_A: 0.930 D_B: 0.232 G_B: 0.296 cycle_B: 2.380 idt_B: 0.338 \n",
      "(epoch: 22, iters: 207, time: 0.859, data: 0.002) D_A: 0.023 G_A: 0.414 cycle_A: 2.065 idt_A: 1.260 D_B: 0.176 G_B: 0.676 cycle_B: 3.448 idt_B: 1.118 \n",
      "(epoch: 22, iters: 307, time: 0.309, data: 0.002) D_A: 0.052 G_A: 0.786 cycle_A: 0.810 idt_A: 2.056 D_B: 0.303 G_B: 0.101 cycle_B: 4.502 idt_B: 0.338 \n",
      "End of epoch 22 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 23, iters: 74, time: 0.306, data: 0.002) D_A: 0.214 G_A: 0.445 cycle_A: 0.368 idt_A: 0.911 D_B: 0.133 G_B: 0.109 cycle_B: 1.489 idt_B: 0.192 \n",
      "(epoch: 23, iters: 174, time: 0.306, data: 0.002) D_A: 0.283 G_A: 0.610 cycle_A: 1.533 idt_A: 0.556 D_B: 0.102 G_B: 0.372 cycle_B: 1.297 idt_B: 0.787 \n",
      "(epoch: 23, iters: 274, time: 0.928, data: 0.002) D_A: 0.094 G_A: 0.913 cycle_A: 1.336 idt_A: 1.243 D_B: 0.324 G_B: 0.327 cycle_B: 3.013 idt_B: 0.614 \n",
      "End of epoch 23 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 24, iters: 41, time: 0.307, data: 0.002) D_A: 0.531 G_A: 0.584 cycle_A: 1.205 idt_A: 0.892 D_B: 0.288 G_B: 0.401 cycle_B: 1.634 idt_B: 0.422 \n",
      "(epoch: 24, iters: 141, time: 0.296, data: 0.001) D_A: 0.224 G_A: 0.603 cycle_A: 1.704 idt_A: 0.885 D_B: 0.100 G_B: 0.225 cycle_B: 2.582 idt_B: 0.896 \n",
      "(epoch: 24, iters: 241, time: 0.306, data: 0.002) D_A: 0.077 G_A: 0.667 cycle_A: 0.620 idt_A: 0.999 D_B: 0.250 G_B: 0.408 cycle_B: 2.017 idt_B: 0.264 \n",
      "End of epoch 24 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 25, iters: 8, time: 0.934, data: 0.002) D_A: 0.342 G_A: 0.396 cycle_A: 2.001 idt_A: 0.961 D_B: 0.169 G_B: 0.274 cycle_B: 1.635 idt_B: 0.583 \n",
      "(epoch: 25, iters: 108, time: 0.309, data: 0.002) D_A: 0.089 G_A: 0.533 cycle_A: 1.236 idt_A: 1.995 D_B: 0.205 G_B: 0.326 cycle_B: 4.301 idt_B: 0.479 \n",
      "(epoch: 25, iters: 208, time: 0.308, data: 0.002) D_A: 0.064 G_A: 0.487 cycle_A: 2.420 idt_A: 1.727 D_B: 0.186 G_B: 0.184 cycle_B: 4.391 idt_B: 0.999 \n",
      "(epoch: 25, iters: 308, time: 0.310, data: 0.002) D_A: 0.682 G_A: 1.085 cycle_A: 3.374 idt_A: 0.322 D_B: 0.474 G_B: 0.036 cycle_B: 0.859 idt_B: 1.625 \n",
      "End of epoch 25 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 26, iters: 75, time: 0.914, data: 0.001) D_A: 0.025 G_A: 0.525 cycle_A: 1.498 idt_A: 1.991 D_B: 0.262 G_B: 0.129 cycle_B: 5.467 idt_B: 0.491 \n",
      "(epoch: 26, iters: 175, time: 0.309, data: 0.002) D_A: 0.053 G_A: 0.533 cycle_A: 1.370 idt_A: 1.578 D_B: 0.321 G_B: 0.383 cycle_B: 4.536 idt_B: 0.551 \n",
      "(epoch: 26, iters: 275, time: 0.304, data: 0.002) D_A: 0.121 G_A: 0.509 cycle_A: 1.334 idt_A: 1.292 D_B: 0.086 G_B: 0.471 cycle_B: 2.289 idt_B: 0.960 \n",
      "End of epoch 26 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 27, iters: 42, time: 0.277, data: 0.002) D_A: 0.100 G_A: 0.315 cycle_A: 1.349 idt_A: 1.413 D_B: 0.154 G_B: 0.352 cycle_B: 3.006 idt_B: 0.586 \n",
      "(epoch: 27, iters: 142, time: 0.900, data: 0.002) D_A: 0.026 G_A: 0.799 cycle_A: 7.077 idt_A: 1.325 D_B: 0.126 G_B: 0.224 cycle_B: 3.253 idt_B: 3.217 \n",
      "(epoch: 27, iters: 242, time: 0.308, data: 0.002) D_A: 0.305 G_A: 0.379 cycle_A: 1.056 idt_A: 1.706 D_B: 0.308 G_B: 0.373 cycle_B: 3.562 idt_B: 0.500 \n",
      "End of epoch 27 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 28, iters: 9, time: 0.291, data: 0.002) D_A: 0.251 G_A: 0.308 cycle_A: 1.024 idt_A: 0.426 D_B: 0.237 G_B: 0.216 cycle_B: 1.128 idt_B: 0.406 \n",
      "(epoch: 28, iters: 109, time: 0.309, data: 0.002) D_A: 0.411 G_A: 0.308 cycle_A: 0.727 idt_A: 0.887 D_B: 0.281 G_B: 0.263 cycle_B: 2.442 idt_B: 0.296 \n",
      "(epoch: 28, iters: 209, time: 0.951, data: 0.002) D_A: 0.233 G_A: 0.494 cycle_A: 1.864 idt_A: 1.019 D_B: 0.094 G_B: 0.424 cycle_B: 1.981 idt_B: 0.788 \n",
      "(epoch: 28, iters: 309, time: 0.307, data: 0.002) D_A: 0.230 G_A: 0.560 cycle_A: 1.935 idt_A: 1.147 D_B: 0.160 G_B: 0.257 cycle_B: 2.994 idt_B: 1.018 \n",
      "End of epoch 28 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 29, iters: 76, time: 0.303, data: 0.002) D_A: 0.140 G_A: 0.625 cycle_A: 1.501 idt_A: 1.169 D_B: 0.092 G_B: 0.458 cycle_B: 2.502 idt_B: 0.627 \n",
      "(epoch: 29, iters: 176, time: 0.300, data: 0.001) D_A: 0.081 G_A: 0.531 cycle_A: 0.964 idt_A: 0.735 D_B: 0.234 G_B: 0.351 cycle_B: 1.388 idt_B: 0.317 \n",
      "(epoch: 29, iters: 276, time: 1.158, data: 0.002) D_A: 0.231 G_A: 0.489 cycle_A: 1.085 idt_A: 0.711 D_B: 0.254 G_B: 0.471 cycle_B: 1.875 idt_B: 0.433 \n",
      "End of epoch 29 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 30, iters: 43, time: 0.307, data: 0.001) D_A: 0.126 G_A: 0.428 cycle_A: 1.243 idt_A: 0.927 D_B: 0.238 G_B: 0.286 cycle_B: 2.203 idt_B: 0.533 \n",
      "(epoch: 30, iters: 143, time: 0.307, data: 0.002) D_A: 0.019 G_A: 0.508 cycle_A: 1.326 idt_A: 2.013 D_B: 0.183 G_B: 0.292 cycle_B: 4.209 idt_B: 0.514 \n",
      "(epoch: 30, iters: 243, time: 0.307, data: 0.002) D_A: 0.570 G_A: 0.103 cycle_A: 1.587 idt_A: 1.559 D_B: 0.110 G_B: 0.366 cycle_B: 3.728 idt_B: 0.898 \n",
      "End of epoch 30 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 31, iters: 10, time: 1.029, data: 0.002) D_A: 0.118 G_A: 0.958 cycle_A: 0.902 idt_A: 1.427 D_B: 0.429 G_B: 1.286 cycle_B: 2.149 idt_B: 0.373 \n",
      "saving the latest model (epoch 31, total_iters 10000)\n",
      "(epoch: 31, iters: 110, time: 0.299, data: 0.002) D_A: 0.093 G_A: 0.614 cycle_A: 0.685 idt_A: 1.052 D_B: 0.295 G_B: 0.153 cycle_B: 2.883 idt_B: 0.293 \n",
      "(epoch: 31, iters: 210, time: 0.308, data: 0.002) D_A: 0.148 G_A: 0.378 cycle_A: 1.801 idt_A: 0.449 D_B: 0.098 G_B: 0.239 cycle_B: 1.252 idt_B: 0.826 \n",
      "(epoch: 31, iters: 310, time: 0.311, data: 0.002) D_A: 0.117 G_A: 0.327 cycle_A: 0.890 idt_A: 0.850 D_B: 0.365 G_B: 0.319 cycle_B: 2.242 idt_B: 0.271 \n",
      "End of epoch 31 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 32, iters: 77, time: 1.059, data: 0.002) D_A: 0.138 G_A: 0.569 cycle_A: 0.934 idt_A: 0.597 D_B: 0.375 G_B: 0.401 cycle_B: 1.841 idt_B: 0.385 \n",
      "(epoch: 32, iters: 177, time: 0.306, data: 0.001) D_A: 0.373 G_A: 0.641 cycle_A: 1.068 idt_A: 0.967 D_B: 0.288 G_B: 0.265 cycle_B: 1.777 idt_B: 0.253 \n",
      "(epoch: 32, iters: 277, time: 0.297, data: 0.002) D_A: 0.037 G_A: 0.371 cycle_A: 1.430 idt_A: 0.483 D_B: 0.247 G_B: 0.340 cycle_B: 0.865 idt_B: 0.533 \n",
      "End of epoch 32 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 33, iters: 44, time: 0.303, data: 0.002) D_A: 0.270 G_A: 0.325 cycle_A: 2.635 idt_A: 0.524 D_B: 0.062 G_B: 0.237 cycle_B: 1.242 idt_B: 1.261 \n",
      "(epoch: 33, iters: 144, time: 1.001, data: 0.002) D_A: 0.059 G_A: 0.393 cycle_A: 1.768 idt_A: 1.020 D_B: 0.050 G_B: 0.376 cycle_B: 1.808 idt_B: 0.917 \n",
      "(epoch: 33, iters: 244, time: 0.310, data: 0.002) D_A: 0.361 G_A: 0.726 cycle_A: 0.868 idt_A: 1.437 D_B: 0.259 G_B: 0.393 cycle_B: 3.524 idt_B: 0.348 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 33 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 34, iters: 11, time: 0.307, data: 0.001) D_A: 0.124 G_A: 0.302 cycle_A: 1.036 idt_A: 2.336 D_B: 0.106 G_B: 0.353 cycle_B: 4.733 idt_B: 0.468 \n",
      "(epoch: 34, iters: 111, time: 0.303, data: 0.002) D_A: 0.258 G_A: 0.273 cycle_A: 1.266 idt_A: 1.725 D_B: 0.135 G_B: 0.059 cycle_B: 4.413 idt_B: 0.511 \n",
      "(epoch: 34, iters: 211, time: 0.980, data: 0.002) D_A: 0.051 G_A: 0.530 cycle_A: 5.845 idt_A: 2.032 D_B: 0.135 G_B: 0.329 cycle_B: 4.718 idt_B: 2.354 \n",
      "(epoch: 34, iters: 311, time: 0.310, data: 0.001) D_A: 0.149 G_A: 0.430 cycle_A: 1.246 idt_A: 1.310 D_B: 0.258 G_B: 0.116 cycle_B: 2.414 idt_B: 0.565 \n",
      "End of epoch 34 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 35, iters: 78, time: 0.291, data: 0.002) D_A: 0.237 G_A: 0.563 cycle_A: 0.893 idt_A: 0.534 D_B: 0.197 G_B: 0.613 cycle_B: 1.453 idt_B: 0.394 \n",
      "(epoch: 35, iters: 178, time: 0.298, data: 0.002) D_A: 0.258 G_A: 0.420 cycle_A: 1.043 idt_A: 0.409 D_B: 0.321 G_B: 0.229 cycle_B: 1.444 idt_B: 0.471 \n",
      "(epoch: 35, iters: 278, time: 1.055, data: 0.002) D_A: 0.134 G_A: 0.247 cycle_A: 1.901 idt_A: 0.691 D_B: 0.061 G_B: 0.600 cycle_B: 1.660 idt_B: 0.707 \n",
      "End of epoch 35 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 36, iters: 45, time: 0.306, data: 0.001) D_A: 0.342 G_A: 0.668 cycle_A: 1.130 idt_A: 1.654 D_B: 0.198 G_B: 0.614 cycle_B: 3.165 idt_B: 0.465 \n",
      "(epoch: 36, iters: 145, time: 0.306, data: 0.001) D_A: 0.150 G_A: 0.493 cycle_A: 1.334 idt_A: 1.216 D_B: 0.377 G_B: 1.005 cycle_B: 2.471 idt_B: 0.590 \n",
      "(epoch: 36, iters: 245, time: 0.309, data: 0.002) D_A: 0.196 G_A: 0.521 cycle_A: 0.912 idt_A: 0.572 D_B: 0.282 G_B: 0.199 cycle_B: 1.305 idt_B: 0.457 \n",
      "End of epoch 36 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 37, iters: 12, time: 1.021, data: 0.002) D_A: 0.239 G_A: 0.095 cycle_A: 1.038 idt_A: 1.835 D_B: 0.273 G_B: 0.654 cycle_B: 4.309 idt_B: 0.435 \n",
      "(epoch: 37, iters: 112, time: 0.310, data: 0.002) D_A: 0.087 G_A: 0.257 cycle_A: 2.801 idt_A: 1.883 D_B: 0.070 G_B: 0.645 cycle_B: 3.556 idt_B: 1.194 \n",
      "(epoch: 37, iters: 212, time: 0.306, data: 0.002) D_A: 0.121 G_A: 0.768 cycle_A: 1.411 idt_A: 1.067 D_B: 0.304 G_B: 0.611 cycle_B: 2.056 idt_B: 0.546 \n",
      "(epoch: 37, iters: 312, time: 0.299, data: 0.002) D_A: 0.024 G_A: 0.795 cycle_A: 1.733 idt_A: 1.254 D_B: 0.125 G_B: 0.370 cycle_B: 2.592 idt_B: 0.662 \n",
      "End of epoch 37 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 38, iters: 79, time: 0.980, data: 0.002) D_A: 0.016 G_A: 0.208 cycle_A: 3.109 idt_A: 0.850 D_B: 0.406 G_B: 0.184 cycle_B: 2.139 idt_B: 0.780 \n",
      "(epoch: 38, iters: 179, time: 0.310, data: 0.002) D_A: 0.246 G_A: 0.500 cycle_A: 1.241 idt_A: 0.372 D_B: 0.230 G_B: 0.207 cycle_B: 0.967 idt_B: 0.542 \n",
      "(epoch: 38, iters: 279, time: 0.293, data: 0.002) D_A: 0.087 G_A: 0.310 cycle_A: 2.186 idt_A: 1.021 D_B: 0.178 G_B: 0.240 cycle_B: 2.475 idt_B: 0.936 \n",
      "End of epoch 38 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 39, iters: 46, time: 0.303, data: 0.002) D_A: 0.082 G_A: 0.658 cycle_A: 1.209 idt_A: 1.261 D_B: 0.269 G_B: 0.644 cycle_B: 3.644 idt_B: 0.377 \n",
      "(epoch: 39, iters: 146, time: 1.068, data: 0.002) D_A: 0.087 G_A: 0.841 cycle_A: 1.033 idt_A: 1.246 D_B: 0.248 G_B: 0.314 cycle_B: 3.074 idt_B: 0.330 \n",
      "(epoch: 39, iters: 246, time: 0.310, data: 0.001) D_A: 0.068 G_A: 0.343 cycle_A: 1.165 idt_A: 1.341 D_B: 0.208 G_B: 0.263 cycle_B: 2.150 idt_B: 0.409 \n",
      "End of epoch 39 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0001000 -> 0.0000100\n",
      "(epoch: 40, iters: 13, time: 0.304, data: 0.002) D_A: 0.046 G_A: 0.385 cycle_A: 0.904 idt_A: 1.685 D_B: 0.268 G_B: 0.324 cycle_B: 4.582 idt_B: 0.363 \n",
      "(epoch: 40, iters: 113, time: 0.285, data: 0.002) D_A: 0.014 G_A: 0.841 cycle_A: 1.677 idt_A: 1.605 D_B: 0.132 G_B: 0.299 cycle_B: 3.188 idt_B: 0.616 \n",
      "(epoch: 40, iters: 213, time: 1.138, data: 0.002) D_A: 0.075 G_A: 0.427 cycle_A: 1.452 idt_A: 2.543 D_B: 0.088 G_B: 0.434 cycle_B: 5.788 idt_B: 0.621 \n",
      "(epoch: 40, iters: 313, time: 0.308, data: 0.001) D_A: 0.035 G_A: 0.630 cycle_A: 0.673 idt_A: 2.451 D_B: 0.230 G_B: 0.421 cycle_B: 5.557 idt_B: 0.238 \n",
      "End of epoch 40 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 41, iters: 80, time: 0.309, data: 0.002) D_A: 0.049 G_A: 0.573 cycle_A: 1.301 idt_A: 1.779 D_B: 0.103 G_B: 0.395 cycle_B: 3.856 idt_B: 0.653 \n",
      "(epoch: 41, iters: 180, time: 0.310, data: 0.002) D_A: 0.068 G_A: 0.585 cycle_A: 0.918 idt_A: 0.821 D_B: 0.135 G_B: 0.415 cycle_B: 2.286 idt_B: 0.389 \n",
      "(epoch: 41, iters: 280, time: 0.959, data: 0.002) D_A: 0.044 G_A: 0.553 cycle_A: 3.236 idt_A: 1.934 D_B: 0.072 G_B: 0.451 cycle_B: 3.837 idt_B: 1.438 \n",
      "End of epoch 41 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 42, iters: 47, time: 0.311, data: 0.001) D_A: 0.302 G_A: 0.321 cycle_A: 1.234 idt_A: 0.485 D_B: 0.217 G_B: 0.333 cycle_B: 0.873 idt_B: 0.560 \n",
      "(epoch: 42, iters: 147, time: 0.311, data: 0.001) D_A: 0.218 G_A: 0.633 cycle_A: 2.373 idt_A: 0.803 D_B: 0.208 G_B: 0.307 cycle_B: 1.003 idt_B: 1.035 \n",
      "(epoch: 42, iters: 247, time: 0.309, data: 0.002) D_A: 0.020 G_A: 0.545 cycle_A: 1.312 idt_A: 1.497 D_B: 0.112 G_B: 0.325 cycle_B: 2.292 idt_B: 0.785 \n",
      "End of epoch 42 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 43, iters: 14, time: 1.075, data: 0.001) D_A: 0.067 G_A: 0.457 cycle_A: 1.226 idt_A: 1.986 D_B: 0.300 G_B: 0.430 cycle_B: 4.253 idt_B: 0.576 \n",
      "(epoch: 43, iters: 114, time: 0.289, data: 0.001) D_A: 0.148 G_A: 0.569 cycle_A: 1.082 idt_A: 0.591 D_B: 0.187 G_B: 0.304 cycle_B: 1.045 idt_B: 0.451 \n",
      "(epoch: 43, iters: 214, time: 0.307, data: 0.002) D_A: 0.042 G_A: 0.605 cycle_A: 1.390 idt_A: 1.492 D_B: 0.148 G_B: 0.314 cycle_B: 3.629 idt_B: 0.443 \n",
      "(epoch: 43, iters: 314, time: 0.310, data: 0.002) D_A: 0.082 G_A: 0.459 cycle_A: 2.939 idt_A: 1.102 D_B: 0.064 G_B: 0.530 cycle_B: 2.347 idt_B: 1.083 \n",
      "End of epoch 43 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 44, iters: 81, time: 1.144, data: 0.002) D_A: 0.302 G_A: 0.778 cycle_A: 2.399 idt_A: 0.780 D_B: 0.113 G_B: 0.501 cycle_B: 2.933 idt_B: 1.120 \n",
      "(epoch: 44, iters: 181, time: 0.312, data: 0.002) D_A: 0.108 G_A: 0.389 cycle_A: 1.506 idt_A: 0.563 D_B: 0.162 G_B: 0.495 cycle_B: 1.487 idt_B: 0.577 \n",
      "(epoch: 44, iters: 281, time: 0.310, data: 0.002) D_A: 0.164 G_A: 0.359 cycle_A: 1.579 idt_A: 0.896 D_B: 0.019 G_B: 0.740 cycle_B: 2.222 idt_B: 0.747 \n",
      "End of epoch 44 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 45, iters: 48, time: 0.300, data: 0.002) D_A: 0.062 G_A: 0.471 cycle_A: 0.610 idt_A: 1.196 D_B: 0.231 G_B: 0.611 cycle_B: 2.196 idt_B: 0.279 \n",
      "(epoch: 45, iters: 148, time: 1.112, data: 0.002) D_A: 0.107 G_A: 0.443 cycle_A: 0.965 idt_A: 1.455 D_B: 0.170 G_B: 0.405 cycle_B: 3.511 idt_B: 0.413 \n",
      "(epoch: 45, iters: 248, time: 0.293, data: 0.002) D_A: 0.246 G_A: 0.533 cycle_A: 0.684 idt_A: 0.512 D_B: 0.171 G_B: 0.310 cycle_B: 0.951 idt_B: 0.257 \n",
      "End of epoch 45 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 46, iters: 15, time: 0.307, data: 0.002) D_A: 0.083 G_A: 0.380 cycle_A: 1.543 idt_A: 1.193 D_B: 0.092 G_B: 0.465 cycle_B: 2.461 idt_B: 0.772 \n",
      "saving the latest model (epoch 46, total_iters 15000)\n",
      "(epoch: 46, iters: 115, time: 0.309, data: 0.001) D_A: 0.073 G_A: 0.423 cycle_A: 0.890 idt_A: 1.880 D_B: 0.060 G_B: 0.546 cycle_B: 3.330 idt_B: 0.313 \n",
      "(epoch: 46, iters: 215, time: 1.007, data: 0.002) D_A: 0.154 G_A: 0.481 cycle_A: 2.331 idt_A: 0.531 D_B: 0.077 G_B: 0.412 cycle_B: 1.601 idt_B: 1.087 \n",
      "(epoch: 46, iters: 315, time: 0.304, data: 0.001) D_A: 0.254 G_A: 0.472 cycle_A: 1.524 idt_A: 0.467 D_B: 0.205 G_B: 0.391 cycle_B: 1.322 idt_B: 0.572 \n",
      "End of epoch 46 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 47, iters: 82, time: 0.310, data: 0.001) D_A: 0.249 G_A: 0.529 cycle_A: 1.210 idt_A: 0.420 D_B: 0.127 G_B: 0.615 cycle_B: 1.544 idt_B: 0.485 \n",
      "(epoch: 47, iters: 182, time: 0.310, data: 0.002) D_A: 0.254 G_A: 0.514 cycle_A: 0.804 idt_A: 0.415 D_B: 0.115 G_B: 0.206 cycle_B: 1.227 idt_B: 0.330 \n",
      "(epoch: 47, iters: 282, time: 1.082, data: 0.002) D_A: 0.329 G_A: 0.353 cycle_A: 0.856 idt_A: 1.174 D_B: 0.216 G_B: 0.564 cycle_B: 2.744 idt_B: 0.302 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 47 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 48, iters: 49, time: 0.310, data: 0.002) D_A: 0.115 G_A: 0.353 cycle_A: 1.868 idt_A: 1.512 D_B: 0.065 G_B: 0.396 cycle_B: 3.932 idt_B: 0.465 \n",
      "(epoch: 48, iters: 149, time: 0.306, data: 0.002) D_A: 0.058 G_A: 0.608 cycle_A: 1.612 idt_A: 1.881 D_B: 0.081 G_B: 0.433 cycle_B: 4.932 idt_B: 0.521 \n",
      "(epoch: 48, iters: 249, time: 0.302, data: 0.002) D_A: 0.067 G_A: 0.506 cycle_A: 3.650 idt_A: 0.344 D_B: 0.051 G_B: 0.418 cycle_B: 0.859 idt_B: 1.514 \n",
      "End of epoch 48 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 49, iters: 16, time: 1.230, data: 0.002) D_A: 0.030 G_A: 0.375 cycle_A: 0.589 idt_A: 2.994 D_B: 0.128 G_B: 0.763 cycle_B: 5.553 idt_B: 0.289 \n",
      "(epoch: 49, iters: 116, time: 0.310, data: 0.001) D_A: 0.108 G_A: 0.391 cycle_A: 2.897 idt_A: 1.521 D_B: 0.135 G_B: 0.429 cycle_B: 2.638 idt_B: 1.065 \n",
      "(epoch: 49, iters: 216, time: 0.306, data: 0.001) D_A: 0.154 G_A: 0.368 cycle_A: 1.167 idt_A: 1.959 D_B: 0.197 G_B: 0.464 cycle_B: 1.659 idt_B: 0.454 \n",
      "(epoch: 49, iters: 316, time: 0.298, data: 0.001) D_A: 0.179 G_A: 0.810 cycle_A: 2.061 idt_A: 0.676 D_B: 0.082 G_B: 0.523 cycle_B: 2.831 idt_B: 0.878 \n",
      "End of epoch 49 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 50, iters: 83, time: 1.163, data: 0.002) D_A: 0.190 G_A: 0.414 cycle_A: 0.837 idt_A: 0.436 D_B: 0.179 G_B: 0.595 cycle_B: 1.206 idt_B: 0.345 \n",
      "(epoch: 50, iters: 183, time: 0.300, data: 0.002) D_A: 0.240 G_A: 0.501 cycle_A: 2.051 idt_A: 0.791 D_B: 0.066 G_B: 0.366 cycle_B: 2.402 idt_B: 0.945 \n",
      "(epoch: 50, iters: 283, time: 0.299, data: 0.002) D_A: 0.080 G_A: 0.439 cycle_A: 0.788 idt_A: 1.845 D_B: 0.285 G_B: 0.857 cycle_B: 3.858 idt_B: 0.460 \n",
      "End of epoch 50 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 51, iters: 50, time: 0.305, data: 0.001) D_A: 0.099 G_A: 0.352 cycle_A: 1.932 idt_A: 3.357 D_B: 0.288 G_B: 0.788 cycle_B: 5.499 idt_B: 0.997 \n",
      "(epoch: 51, iters: 150, time: 1.046, data: 0.002) D_A: 0.039 G_A: 0.608 cycle_A: 0.769 idt_A: 1.761 D_B: 0.139 G_B: 0.623 cycle_B: 3.587 idt_B: 0.335 \n",
      "(epoch: 51, iters: 250, time: 0.310, data: 0.002) D_A: 0.118 G_A: 0.373 cycle_A: 1.371 idt_A: 1.228 D_B: 0.050 G_B: 0.652 cycle_B: 2.804 idt_B: 0.692 \n",
      "End of epoch 51 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 52, iters: 17, time: 0.302, data: 0.001) D_A: 0.160 G_A: 0.478 cycle_A: 1.543 idt_A: 0.672 D_B: 0.215 G_B: 0.319 cycle_B: 1.260 idt_B: 0.548 \n",
      "(epoch: 52, iters: 117, time: 0.310, data: 0.002) D_A: 0.211 G_A: 0.483 cycle_A: 2.011 idt_A: 0.816 D_B: 0.060 G_B: 0.577 cycle_B: 2.277 idt_B: 0.851 \n",
      "(epoch: 52, iters: 217, time: 1.200, data: 0.001) D_A: 0.031 G_A: 0.664 cycle_A: 0.618 idt_A: 0.801 D_B: 0.352 G_B: 0.527 cycle_B: 2.108 idt_B: 0.233 \n",
      "(epoch: 52, iters: 317, time: 0.312, data: 0.001) D_A: 0.280 G_A: 0.216 cycle_A: 0.476 idt_A: 0.592 D_B: 0.262 G_B: 0.399 cycle_B: 2.037 idt_B: 0.221 \n",
      "End of epoch 52 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 53, iters: 84, time: 0.311, data: 0.002) D_A: 0.159 G_A: 0.336 cycle_A: 0.561 idt_A: 0.351 D_B: 0.137 G_B: 0.430 cycle_B: 0.792 idt_B: 0.231 \n",
      "(epoch: 53, iters: 184, time: 0.310, data: 0.002) D_A: 0.243 G_A: 0.586 cycle_A: 1.473 idt_A: 0.342 D_B: 0.039 G_B: 0.461 cycle_B: 1.185 idt_B: 0.551 \n",
      "(epoch: 53, iters: 284, time: 1.067, data: 0.002) D_A: 0.051 G_A: 0.746 cycle_A: 1.622 idt_A: 0.703 D_B: 0.057 G_B: 0.548 cycle_B: 1.164 idt_B: 0.814 \n",
      "End of epoch 53 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 54, iters: 51, time: 0.307, data: 0.002) D_A: 0.178 G_A: 0.385 cycle_A: 1.033 idt_A: 0.988 D_B: 0.178 G_B: 0.382 cycle_B: 1.410 idt_B: 0.519 \n",
      "(epoch: 54, iters: 151, time: 0.309, data: 0.001) D_A: 0.094 G_A: 0.272 cycle_A: 0.642 idt_A: 0.670 D_B: 0.106 G_B: 0.407 cycle_B: 2.099 idt_B: 0.332 \n",
      "(epoch: 54, iters: 251, time: 0.309, data: 0.001) D_A: 0.154 G_A: 0.252 cycle_A: 0.733 idt_A: 2.753 D_B: 0.224 G_B: 0.311 cycle_B: 3.323 idt_B: 0.287 \n",
      "End of epoch 54 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 55, iters: 18, time: 1.212, data: 0.001) D_A: 0.257 G_A: 0.413 cycle_A: 0.609 idt_A: 0.456 D_B: 0.119 G_B: 0.240 cycle_B: 2.407 idt_B: 0.224 \n",
      "(epoch: 55, iters: 118, time: 0.285, data: 0.002) D_A: 0.034 G_A: 0.541 cycle_A: 0.797 idt_A: 1.508 D_B: 0.124 G_B: 0.414 cycle_B: 2.984 idt_B: 0.396 \n",
      "(epoch: 55, iters: 218, time: 0.308, data: 0.002) D_A: 0.235 G_A: 0.466 cycle_A: 1.034 idt_A: 0.807 D_B: 0.131 G_B: 0.460 cycle_B: 2.153 idt_B: 0.373 \n",
      "(epoch: 55, iters: 318, time: 0.285, data: 0.002) D_A: 0.221 G_A: 0.404 cycle_A: 1.786 idt_A: 0.287 D_B: 0.080 G_B: 0.516 cycle_B: 0.750 idt_B: 0.855 \n",
      "End of epoch 55 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 56, iters: 85, time: 1.163, data: 0.002) D_A: 0.182 G_A: 0.526 cycle_A: 2.983 idt_A: 1.047 D_B: 0.189 G_B: 0.289 cycle_B: 2.732 idt_B: 1.339 \n",
      "(epoch: 56, iters: 185, time: 0.308, data: 0.002) D_A: 0.152 G_A: 0.442 cycle_A: 0.791 idt_A: 0.736 D_B: 0.132 G_B: 0.527 cycle_B: 1.247 idt_B: 0.277 \n",
      "(epoch: 56, iters: 285, time: 0.306, data: 0.002) D_A: 0.277 G_A: 0.326 cycle_A: 0.658 idt_A: 0.522 D_B: 0.105 G_B: 0.304 cycle_B: 2.222 idt_B: 0.304 \n",
      "End of epoch 56 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 57, iters: 52, time: 0.311, data: 0.002) D_A: 0.128 G_A: 0.776 cycle_A: 3.256 idt_A: 0.917 D_B: 0.093 G_B: 0.474 cycle_B: 1.854 idt_B: 1.212 \n",
      "(epoch: 57, iters: 152, time: 1.153, data: 0.001) D_A: 0.234 G_A: 0.289 cycle_A: 1.189 idt_A: 0.958 D_B: 0.202 G_B: 0.695 cycle_B: 1.852 idt_B: 0.587 \n",
      "(epoch: 57, iters: 252, time: 0.303, data: 0.002) D_A: 0.179 G_A: 0.465 cycle_A: 3.116 idt_A: 1.396 D_B: 0.065 G_B: 0.811 cycle_B: 1.015 idt_B: 1.229 \n",
      "End of epoch 57 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 58, iters: 19, time: 0.310, data: 0.001) D_A: 0.133 G_A: 0.392 cycle_A: 2.073 idt_A: 0.328 D_B: 0.109 G_B: 0.406 cycle_B: 1.720 idt_B: 0.746 \n",
      "(epoch: 58, iters: 119, time: 0.308, data: 0.001) D_A: 0.156 G_A: 0.542 cycle_A: 2.212 idt_A: 1.393 D_B: 0.182 G_B: 0.429 cycle_B: 1.938 idt_B: 0.885 \n",
      "(epoch: 58, iters: 219, time: 1.214, data: 0.001) D_A: 0.237 G_A: 0.444 cycle_A: 0.747 idt_A: 0.459 D_B: 0.160 G_B: 0.495 cycle_B: 0.724 idt_B: 0.495 \n",
      "(epoch: 58, iters: 319, time: 0.301, data: 0.002) D_A: 0.275 G_A: 0.341 cycle_A: 0.987 idt_A: 0.328 D_B: 0.210 G_B: 0.484 cycle_B: 0.938 idt_B: 0.643 \n",
      "End of epoch 58 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 59, iters: 86, time: 0.309, data: 0.002) D_A: 0.039 G_A: 0.609 cycle_A: 2.985 idt_A: 1.453 D_B: 0.533 G_B: 0.387 cycle_B: 2.173 idt_B: 1.044 \n",
      "(epoch: 59, iters: 186, time: 0.306, data: 0.002) D_A: 0.130 G_A: 0.674 cycle_A: 0.673 idt_A: 1.135 D_B: 0.049 G_B: 0.638 cycle_B: 2.226 idt_B: 0.280 \n",
      "(epoch: 59, iters: 286, time: 1.126, data: 0.001) D_A: 0.112 G_A: 0.331 cycle_A: 1.355 idt_A: 0.845 D_B: 0.045 G_B: 0.290 cycle_B: 1.035 idt_B: 0.688 \n",
      "End of epoch 59 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 60, iters: 53, time: 0.311, data: 0.002) D_A: 0.163 G_A: 0.353 cycle_A: 0.797 idt_A: 0.539 D_B: 0.180 G_B: 0.299 cycle_B: 1.195 idt_B: 0.438 \n",
      "(epoch: 60, iters: 153, time: 0.309, data: 0.002) D_A: 0.143 G_A: 0.525 cycle_A: 1.672 idt_A: 0.424 D_B: 0.094 G_B: 0.389 cycle_B: 1.088 idt_B: 0.636 \n",
      "(epoch: 60, iters: 253, time: 0.302, data: 0.002) D_A: 0.024 G_A: 0.445 cycle_A: 0.654 idt_A: 3.544 D_B: 0.266 G_B: 0.509 cycle_B: 7.516 idt_B: 0.216 \n",
      "saving the model at the end of epoch 60, iters 19980\n",
      "End of epoch 60 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 61, iters: 20, time: 1.187, data: 0.002) D_A: 0.182 G_A: 0.515 cycle_A: 0.570 idt_A: 1.163 D_B: 0.182 G_B: 0.319 cycle_B: 3.160 idt_B: 0.247 \n",
      "saving the latest model (epoch 61, total_iters 20000)\n",
      "(epoch: 61, iters: 120, time: 0.310, data: 0.002) D_A: 0.218 G_A: 0.483 cycle_A: 1.215 idt_A: 0.391 D_B: 0.159 G_B: 0.445 cycle_B: 1.814 idt_B: 0.670 \n",
      "(epoch: 61, iters: 220, time: 0.308, data: 0.002) D_A: 0.060 G_A: 0.359 cycle_A: 0.667 idt_A: 1.295 D_B: 0.163 G_B: 0.455 cycle_B: 2.779 idt_B: 0.219 \n",
      "(epoch: 61, iters: 320, time: 0.295, data: 0.001) D_A: 0.174 G_A: 0.561 cycle_A: 1.073 idt_A: 0.460 D_B: 0.038 G_B: 0.314 cycle_B: 1.527 idt_B: 0.559 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 61 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 62, iters: 87, time: 1.314, data: 0.002) D_A: 0.112 G_A: 0.736 cycle_A: 2.313 idt_A: 1.112 D_B: 0.119 G_B: 0.528 cycle_B: 2.557 idt_B: 0.756 \n",
      "(epoch: 62, iters: 187, time: 0.309, data: 0.001) D_A: 0.142 G_A: 0.538 cycle_A: 2.115 idt_A: 0.949 D_B: 0.100 G_B: 0.570 cycle_B: 2.705 idt_B: 0.851 \n",
      "(epoch: 62, iters: 287, time: 0.293, data: 0.001) D_A: 0.125 G_A: 0.394 cycle_A: 3.192 idt_A: 0.736 D_B: 0.139 G_B: 0.276 cycle_B: 0.772 idt_B: 1.453 \n",
      "End of epoch 62 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 63, iters: 54, time: 0.306, data: 0.001) D_A: 0.052 G_A: 0.346 cycle_A: 0.500 idt_A: 0.586 D_B: 0.101 G_B: 0.475 cycle_B: 0.942 idt_B: 0.264 \n",
      "(epoch: 63, iters: 154, time: 1.066, data: 0.002) D_A: 0.139 G_A: 0.756 cycle_A: 1.007 idt_A: 2.132 D_B: 0.092 G_B: 0.219 cycle_B: 2.458 idt_B: 0.428 \n",
      "(epoch: 63, iters: 254, time: 0.296, data: 0.002) D_A: 0.054 G_A: 0.568 cycle_A: 0.478 idt_A: 2.031 D_B: 0.270 G_B: 0.638 cycle_B: 2.265 idt_B: 0.317 \n",
      "End of epoch 63 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 64, iters: 21, time: 0.291, data: 0.002) D_A: 0.097 G_A: 0.460 cycle_A: 0.643 idt_A: 0.784 D_B: 0.231 G_B: 0.611 cycle_B: 1.198 idt_B: 0.349 \n",
      "(epoch: 64, iters: 121, time: 0.307, data: 0.002) D_A: 0.181 G_A: 0.490 cycle_A: 1.582 idt_A: 0.495 D_B: 0.164 G_B: 0.587 cycle_B: 1.819 idt_B: 0.710 \n",
      "(epoch: 64, iters: 221, time: 1.153, data: 0.002) D_A: 0.115 G_A: 0.330 cycle_A: 1.420 idt_A: 1.052 D_B: 0.085 G_B: 0.550 cycle_B: 1.900 idt_B: 0.463 \n",
      "(epoch: 64, iters: 321, time: 0.307, data: 0.001) D_A: 0.207 G_A: 0.555 cycle_A: 2.290 idt_A: 0.769 D_B: 0.177 G_B: 0.408 cycle_B: 1.382 idt_B: 1.049 \n",
      "End of epoch 64 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 65, iters: 88, time: 0.304, data: 0.001) D_A: 0.181 G_A: 0.489 cycle_A: 1.162 idt_A: 0.584 D_B: 0.031 G_B: 0.434 cycle_B: 1.240 idt_B: 0.451 \n",
      "(epoch: 65, iters: 188, time: 0.310, data: 0.002) D_A: 0.205 G_A: 0.456 cycle_A: 1.662 idt_A: 0.573 D_B: 0.096 G_B: 0.451 cycle_B: 1.771 idt_B: 0.721 \n",
      "(epoch: 65, iters: 288, time: 1.119, data: 0.001) D_A: 0.167 G_A: 0.597 cycle_A: 1.636 idt_A: 0.833 D_B: 0.150 G_B: 0.623 cycle_B: 1.270 idt_B: 0.728 \n",
      "End of epoch 65 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 66, iters: 55, time: 0.308, data: 0.001) D_A: 0.257 G_A: 0.595 cycle_A: 0.571 idt_A: 0.773 D_B: 0.096 G_B: 0.226 cycle_B: 2.062 idt_B: 0.234 \n",
      "(epoch: 66, iters: 155, time: 0.301, data: 0.002) D_A: 0.020 G_A: 0.483 cycle_A: 2.112 idt_A: 1.716 D_B: 0.026 G_B: 0.594 cycle_B: 2.582 idt_B: 0.897 \n",
      "(epoch: 66, iters: 255, time: 0.310, data: 0.002) D_A: 0.036 G_A: 0.448 cycle_A: 0.999 idt_A: 1.383 D_B: 0.040 G_B: 0.456 cycle_B: 1.181 idt_B: 0.489 \n",
      "End of epoch 66 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 67, iters: 22, time: 1.266, data: 0.002) D_A: 0.214 G_A: 0.445 cycle_A: 0.591 idt_A: 0.372 D_B: 0.318 G_B: 0.528 cycle_B: 0.636 idt_B: 0.293 \n",
      "(epoch: 67, iters: 122, time: 0.311, data: 0.002) D_A: 0.089 G_A: 0.546 cycle_A: 4.055 idt_A: 0.493 D_B: 0.210 G_B: 0.816 cycle_B: 1.076 idt_B: 1.524 \n",
      "(epoch: 67, iters: 222, time: 0.310, data: 0.002) D_A: 0.138 G_A: 0.454 cycle_A: 0.839 idt_A: 1.823 D_B: 0.063 G_B: 0.287 cycle_B: 3.128 idt_B: 0.303 \n",
      "(epoch: 67, iters: 322, time: 0.306, data: 0.001) D_A: 0.132 G_A: 0.344 cycle_A: 1.665 idt_A: 0.323 D_B: 0.031 G_B: 0.548 cycle_B: 0.819 idt_B: 0.703 \n",
      "End of epoch 67 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 68, iters: 89, time: 1.293, data: 0.002) D_A: 0.196 G_A: 0.421 cycle_A: 1.126 idt_A: 0.555 D_B: 0.107 G_B: 0.286 cycle_B: 1.650 idt_B: 0.533 \n",
      "(epoch: 68, iters: 189, time: 0.308, data: 0.002) D_A: 0.218 G_A: 0.524 cycle_A: 0.894 idt_A: 0.429 D_B: 0.249 G_B: 0.440 cycle_B: 3.113 idt_B: 0.421 \n",
      "(epoch: 68, iters: 289, time: 0.299, data: 0.002) D_A: 0.176 G_A: 0.569 cycle_A: 2.415 idt_A: 0.629 D_B: 0.062 G_B: 0.551 cycle_B: 1.211 idt_B: 0.956 \n",
      "End of epoch 68 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 69, iters: 56, time: 0.296, data: 0.001) D_A: 0.104 G_A: 0.546 cycle_A: 1.075 idt_A: 0.328 D_B: 0.147 G_B: 0.543 cycle_B: 1.020 idt_B: 0.407 \n",
      "(epoch: 69, iters: 156, time: 1.264, data: 0.001) D_A: 0.289 G_A: 0.459 cycle_A: 1.955 idt_A: 0.998 D_B: 0.213 G_B: 0.877 cycle_B: 0.964 idt_B: 0.778 \n",
      "(epoch: 69, iters: 256, time: 0.296, data: 0.001) D_A: 0.060 G_A: 0.491 cycle_A: 0.951 idt_A: 1.238 D_B: 0.208 G_B: 0.526 cycle_B: 2.570 idt_B: 0.342 \n",
      "End of epoch 69 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 70, iters: 23, time: 0.311, data: 0.002) D_A: 0.163 G_A: 0.554 cycle_A: 0.705 idt_A: 0.264 D_B: 0.147 G_B: 0.599 cycle_B: 0.699 idt_B: 0.290 \n",
      "(epoch: 70, iters: 123, time: 0.311, data: 0.002) D_A: 0.022 G_A: 0.822 cycle_A: 1.784 idt_A: 0.759 D_B: 0.129 G_B: 0.560 cycle_B: 1.493 idt_B: 0.892 \n",
      "(epoch: 70, iters: 223, time: 1.172, data: 0.001) D_A: 0.205 G_A: 0.534 cycle_A: 2.033 idt_A: 0.399 D_B: 0.053 G_B: 0.643 cycle_B: 2.205 idt_B: 0.957 \n",
      "(epoch: 70, iters: 323, time: 0.312, data: 0.002) D_A: 0.246 G_A: 0.498 cycle_A: 2.218 idt_A: 0.491 D_B: 0.093 G_B: 0.323 cycle_B: 1.517 idt_B: 0.918 \n",
      "End of epoch 70 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 71, iters: 90, time: 0.310, data: 0.001) D_A: 0.033 G_A: 0.380 cycle_A: 0.775 idt_A: 2.225 D_B: 0.087 G_B: 0.367 cycle_B: 3.426 idt_B: 0.310 \n",
      "(epoch: 71, iters: 190, time: 0.293, data: 0.002) D_A: 0.062 G_A: 0.518 cycle_A: 3.530 idt_A: 1.476 D_B: 0.063 G_B: 0.428 cycle_B: 2.832 idt_B: 1.411 \n",
      "(epoch: 71, iters: 290, time: 1.255, data: 0.002) D_A: 0.057 G_A: 0.516 cycle_A: 1.381 idt_A: 1.377 D_B: 0.059 G_B: 0.490 cycle_B: 3.002 idt_B: 0.496 \n",
      "End of epoch 71 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 72, iters: 57, time: 0.301, data: 0.001) D_A: 0.133 G_A: 0.857 cycle_A: 1.406 idt_A: 1.224 D_B: 0.087 G_B: 0.705 cycle_B: 2.720 idt_B: 0.539 \n",
      "(epoch: 72, iters: 157, time: 0.308, data: 0.002) D_A: 0.195 G_A: 0.543 cycle_A: 1.212 idt_A: 0.379 D_B: 0.134 G_B: 0.319 cycle_B: 1.435 idt_B: 0.529 \n",
      "(epoch: 72, iters: 257, time: 0.307, data: 0.002) D_A: 0.068 G_A: 0.473 cycle_A: 2.236 idt_A: 1.771 D_B: 0.053 G_B: 0.501 cycle_B: 2.796 idt_B: 0.708 \n",
      "End of epoch 72 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 73, iters: 24, time: 1.149, data: 0.001) D_A: 0.222 G_A: 0.734 cycle_A: 0.982 idt_A: 1.236 D_B: 0.098 G_B: 0.457 cycle_B: 3.573 idt_B: 0.472 \n",
      "(epoch: 73, iters: 124, time: 0.307, data: 0.002) D_A: 0.280 G_A: 0.437 cycle_A: 0.611 idt_A: 0.398 D_B: 0.157 G_B: 0.424 cycle_B: 1.623 idt_B: 0.231 \n",
      "(epoch: 73, iters: 224, time: 0.291, data: 0.001) D_A: 0.173 G_A: 0.236 cycle_A: 1.182 idt_A: 0.563 D_B: 0.121 G_B: 0.603 cycle_B: 1.681 idt_B: 0.443 \n",
      "(epoch: 73, iters: 324, time: 0.310, data: 0.002) D_A: 0.179 G_A: 0.586 cycle_A: 0.796 idt_A: 0.596 D_B: 0.088 G_B: 0.321 cycle_B: 0.806 idt_B: 0.306 \n",
      "End of epoch 73 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 74, iters: 91, time: 1.304, data: 0.002) D_A: 0.096 G_A: 0.332 cycle_A: 1.239 idt_A: 2.276 D_B: 0.028 G_B: 0.754 cycle_B: 3.349 idt_B: 0.565 \n",
      "(epoch: 74, iters: 191, time: 0.310, data: 0.002) D_A: 0.185 G_A: 0.500 cycle_A: 0.927 idt_A: 0.858 D_B: 0.267 G_B: 0.229 cycle_B: 1.222 idt_B: 0.356 \n",
      "(epoch: 74, iters: 291, time: 0.309, data: 0.001) D_A: 0.075 G_A: 0.528 cycle_A: 2.262 idt_A: 1.155 D_B: 0.181 G_B: 0.477 cycle_B: 1.535 idt_B: 0.712 \n",
      "End of epoch 74 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 75, iters: 58, time: 0.310, data: 0.002) D_A: 0.200 G_A: 0.455 cycle_A: 0.980 idt_A: 0.775 D_B: 0.151 G_B: 0.488 cycle_B: 1.269 idt_B: 0.483 \n",
      "(epoch: 75, iters: 158, time: 1.252, data: 0.002) D_A: 0.232 G_A: 0.425 cycle_A: 2.588 idt_A: 1.311 D_B: 0.091 G_B: 0.437 cycle_B: 3.357 idt_B: 1.130 \n",
      "(epoch: 75, iters: 258, time: 0.311, data: 0.002) D_A: 0.066 G_A: 0.679 cycle_A: 1.240 idt_A: 0.249 D_B: 0.059 G_B: 0.597 cycle_B: 0.717 idt_B: 0.547 \n",
      "End of epoch 75 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 76, iters: 25, time: 0.309, data: 0.002) D_A: 0.240 G_A: 0.520 cycle_A: 1.510 idt_A: 0.781 D_B: 0.234 G_B: 0.154 cycle_B: 1.472 idt_B: 0.470 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the latest model (epoch 76, total_iters 25000)\n",
      "(epoch: 76, iters: 125, time: 0.308, data: 0.002) D_A: 0.158 G_A: 0.686 cycle_A: 1.519 idt_A: 0.429 D_B: 0.201 G_B: 0.247 cycle_B: 1.099 idt_B: 0.562 \n",
      "(epoch: 76, iters: 225, time: 1.240, data: 0.002) D_A: 0.060 G_A: 0.664 cycle_A: 1.099 idt_A: 1.670 D_B: 0.147 G_B: 0.269 cycle_B: 2.257 idt_B: 0.696 \n",
      "(epoch: 76, iters: 325, time: 0.310, data: 0.002) D_A: 0.106 G_A: 0.621 cycle_A: 1.020 idt_A: 0.848 D_B: 0.106 G_B: 0.330 cycle_B: 1.513 idt_B: 0.470 \n",
      "End of epoch 76 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 77, iters: 92, time: 0.304, data: 0.001) D_A: 0.290 G_A: 0.776 cycle_A: 1.613 idt_A: 1.162 D_B: 0.110 G_B: 0.444 cycle_B: 2.857 idt_B: 0.808 \n",
      "(epoch: 77, iters: 192, time: 0.306, data: 0.001) D_A: 0.169 G_A: 0.401 cycle_A: 1.267 idt_A: 1.378 D_B: 0.095 G_B: 0.503 cycle_B: 4.014 idt_B: 0.422 \n",
      "(epoch: 77, iters: 292, time: 1.198, data: 0.001) D_A: 0.262 G_A: 0.311 cycle_A: 1.786 idt_A: 0.974 D_B: 0.117 G_B: 0.368 cycle_B: 1.387 idt_B: 0.733 \n",
      "End of epoch 77 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 78, iters: 59, time: 0.310, data: 0.001) D_A: 0.110 G_A: 0.343 cycle_A: 0.643 idt_A: 1.053 D_B: 0.257 G_B: 0.361 cycle_B: 1.656 idt_B: 0.286 \n",
      "(epoch: 78, iters: 159, time: 0.307, data: 0.001) D_A: 0.119 G_A: 0.555 cycle_A: 2.885 idt_A: 0.732 D_B: 0.086 G_B: 0.525 cycle_B: 1.139 idt_B: 1.907 \n",
      "(epoch: 78, iters: 259, time: 0.308, data: 0.001) D_A: 0.069 G_A: 0.416 cycle_A: 0.632 idt_A: 2.748 D_B: 0.097 G_B: 0.651 cycle_B: 5.681 idt_B: 0.265 \n",
      "End of epoch 78 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 79, iters: 26, time: 1.286, data: 0.001) D_A: 0.220 G_A: 0.505 cycle_A: 0.632 idt_A: 0.565 D_B: 0.121 G_B: 0.493 cycle_B: 0.989 idt_B: 0.230 \n",
      "(epoch: 79, iters: 126, time: 0.309, data: 0.002) D_A: 0.041 G_A: 0.598 cycle_A: 1.237 idt_A: 1.676 D_B: 0.168 G_B: 0.342 cycle_B: 2.518 idt_B: 0.441 \n",
      "(epoch: 79, iters: 226, time: 0.300, data: 0.002) D_A: 0.145 G_A: 0.664 cycle_A: 1.250 idt_A: 1.116 D_B: 0.128 G_B: 0.613 cycle_B: 1.092 idt_B: 0.438 \n",
      "(epoch: 79, iters: 326, time: 0.309, data: 0.002) D_A: 0.157 G_A: 0.649 cycle_A: 3.126 idt_A: 1.040 D_B: 0.150 G_B: 0.323 cycle_B: 3.019 idt_B: 1.211 \n",
      "End of epoch 79 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000100 -> 0.0000010\n",
      "(epoch: 80, iters: 93, time: 1.242, data: 0.002) D_A: 0.152 G_A: 0.574 cycle_A: 0.603 idt_A: 0.325 D_B: 0.189 G_B: 0.244 cycle_B: 1.136 idt_B: 0.265 \n",
      "(epoch: 80, iters: 193, time: 0.295, data: 0.002) D_A: 0.069 G_A: 0.468 cycle_A: 0.686 idt_A: 1.855 D_B: 0.261 G_B: 0.239 cycle_B: 3.348 idt_B: 0.308 \n",
      "(epoch: 80, iters: 293, time: 0.306, data: 0.001) D_A: 0.156 G_A: 0.600 cycle_A: 0.863 idt_A: 0.319 D_B: 0.101 G_B: 0.658 cycle_B: 1.014 idt_B: 0.331 \n",
      "End of epoch 80 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 81, iters: 60, time: 0.310, data: 0.002) D_A: 0.188 G_A: 0.608 cycle_A: 0.864 idt_A: 0.577 D_B: 0.113 G_B: 0.586 cycle_B: 2.135 idt_B: 0.362 \n",
      "(epoch: 81, iters: 160, time: 1.266, data: 0.002) D_A: 0.160 G_A: 0.573 cycle_A: 2.001 idt_A: 0.649 D_B: 0.133 G_B: 0.691 cycle_B: 1.184 idt_B: 0.824 \n",
      "(epoch: 81, iters: 260, time: 0.305, data: 0.002) D_A: 0.175 G_A: 0.579 cycle_A: 1.804 idt_A: 1.472 D_B: 0.208 G_B: 0.556 cycle_B: 3.433 idt_B: 0.674 \n",
      "End of epoch 81 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 82, iters: 27, time: 0.298, data: 0.001) D_A: 0.210 G_A: 0.589 cycle_A: 1.146 idt_A: 0.468 D_B: 0.092 G_B: 0.531 cycle_B: 1.123 idt_B: 0.460 \n",
      "(epoch: 82, iters: 127, time: 0.306, data: 0.002) D_A: 0.082 G_A: 0.434 cycle_A: 1.528 idt_A: 1.862 D_B: 0.116 G_B: 0.576 cycle_B: 2.546 idt_B: 0.751 \n",
      "(epoch: 82, iters: 227, time: 1.284, data: 0.002) D_A: 0.124 G_A: 0.372 cycle_A: 0.843 idt_A: 0.270 D_B: 0.061 G_B: 0.575 cycle_B: 0.849 idt_B: 0.563 \n",
      "(epoch: 82, iters: 327, time: 0.291, data: 0.002) D_A: 0.295 G_A: 0.534 cycle_A: 0.852 idt_A: 0.795 D_B: 0.071 G_B: 0.438 cycle_B: 1.226 idt_B: 0.532 \n",
      "End of epoch 82 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 83, iters: 94, time: 0.310, data: 0.002) D_A: 0.186 G_A: 0.465 cycle_A: 1.847 idt_A: 1.398 D_B: 0.080 G_B: 0.511 cycle_B: 1.426 idt_B: 0.709 \n",
      "(epoch: 83, iters: 194, time: 0.306, data: 0.002) D_A: 0.164 G_A: 0.413 cycle_A: 1.097 idt_A: 0.388 D_B: 0.035 G_B: 0.522 cycle_B: 1.237 idt_B: 0.477 \n",
      "(epoch: 83, iters: 294, time: 1.364, data: 0.001) D_A: 0.202 G_A: 0.328 cycle_A: 1.117 idt_A: 0.497 D_B: 0.163 G_B: 0.937 cycle_B: 0.966 idt_B: 0.250 \n",
      "End of epoch 83 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 84, iters: 61, time: 0.304, data: 0.002) D_A: 0.197 G_A: 0.294 cycle_A: 1.611 idt_A: 0.981 D_B: 0.365 G_B: 0.564 cycle_B: 1.176 idt_B: 0.661 \n",
      "(epoch: 84, iters: 161, time: 0.297, data: 0.002) D_A: 0.224 G_A: 0.408 cycle_A: 1.356 idt_A: 1.484 D_B: 0.156 G_B: 0.389 cycle_B: 3.469 idt_B: 0.548 \n",
      "(epoch: 84, iters: 261, time: 0.298, data: 0.002) D_A: 0.077 G_A: 0.292 cycle_A: 1.397 idt_A: 1.273 D_B: 0.096 G_B: 0.679 cycle_B: 1.838 idt_B: 0.760 \n",
      "End of epoch 84 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 85, iters: 28, time: 1.355, data: 0.002) D_A: 0.049 G_A: 0.521 cycle_A: 0.509 idt_A: 1.426 D_B: 0.104 G_B: 0.545 cycle_B: 2.914 idt_B: 0.228 \n",
      "(epoch: 85, iters: 128, time: 0.306, data: 0.002) D_A: 0.132 G_A: 0.499 cycle_A: 1.086 idt_A: 1.491 D_B: 0.134 G_B: 0.833 cycle_B: 3.858 idt_B: 0.398 \n",
      "(epoch: 85, iters: 228, time: 0.312, data: 0.001) D_A: 0.171 G_A: 0.439 cycle_A: 1.021 idt_A: 0.781 D_B: 0.047 G_B: 0.754 cycle_B: 2.704 idt_B: 0.597 \n",
      "(epoch: 85, iters: 328, time: 0.302, data: 0.002) D_A: 0.332 G_A: 0.541 cycle_A: 0.600 idt_A: 0.684 D_B: 0.072 G_B: 0.544 cycle_B: 2.279 idt_B: 0.325 \n",
      "End of epoch 85 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 86, iters: 95, time: 1.280, data: 0.002) D_A: 0.167 G_A: 0.449 cycle_A: 1.509 idt_A: 0.527 D_B: 0.052 G_B: 0.669 cycle_B: 1.158 idt_B: 0.734 \n",
      "(epoch: 86, iters: 195, time: 0.309, data: 0.002) D_A: 0.104 G_A: 0.509 cycle_A: 1.122 idt_A: 0.301 D_B: 0.071 G_B: 0.485 cycle_B: 1.092 idt_B: 0.412 \n",
      "(epoch: 86, iters: 295, time: 0.310, data: 0.001) D_A: 0.170 G_A: 0.324 cycle_A: 1.333 idt_A: 0.502 D_B: 0.221 G_B: 0.593 cycle_B: 0.862 idt_B: 0.551 \n",
      "End of epoch 86 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 87, iters: 62, time: 0.311, data: 0.002) D_A: 0.231 G_A: 0.514 cycle_A: 0.929 idt_A: 1.026 D_B: 0.104 G_B: 0.602 cycle_B: 2.823 idt_B: 0.375 \n",
      "(epoch: 87, iters: 162, time: 1.379, data: 0.002) D_A: 0.223 G_A: 0.552 cycle_A: 0.710 idt_A: 1.244 D_B: 0.075 G_B: 0.559 cycle_B: 3.980 idt_B: 0.281 \n",
      "(epoch: 87, iters: 262, time: 0.307, data: 0.002) D_A: 0.157 G_A: 0.359 cycle_A: 2.193 idt_A: 0.632 D_B: 0.212 G_B: 0.582 cycle_B: 2.086 idt_B: 0.971 \n",
      "End of epoch 87 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 88, iters: 29, time: 0.309, data: 0.002) D_A: 0.118 G_A: 0.695 cycle_A: 1.751 idt_A: 0.381 D_B: 0.148 G_B: 0.547 cycle_B: 0.831 idt_B: 1.052 \n",
      "(epoch: 88, iters: 129, time: 0.309, data: 0.002) D_A: 0.151 G_A: 0.445 cycle_A: 1.923 idt_A: 1.610 D_B: 0.204 G_B: 0.621 cycle_B: 2.035 idt_B: 0.778 \n",
      "(epoch: 88, iters: 229, time: 1.346, data: 0.002) D_A: 0.137 G_A: 0.463 cycle_A: 0.852 idt_A: 0.955 D_B: 0.116 G_B: 0.566 cycle_B: 4.130 idt_B: 0.336 \n",
      "(epoch: 88, iters: 329, time: 0.311, data: 0.001) D_A: 0.153 G_A: 0.574 cycle_A: 2.237 idt_A: 0.804 D_B: 0.075 G_B: 0.653 cycle_B: 1.559 idt_B: 1.052 \n",
      "End of epoch 88 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 89, iters: 96, time: 0.294, data: 0.002) D_A: 0.196 G_A: 0.348 cycle_A: 1.429 idt_A: 1.267 D_B: 0.078 G_B: 0.876 cycle_B: 1.143 idt_B: 0.635 \n",
      "(epoch: 89, iters: 196, time: 0.311, data: 0.002) D_A: 0.189 G_A: 0.333 cycle_A: 0.659 idt_A: 0.501 D_B: 0.129 G_B: 0.609 cycle_B: 1.005 idt_B: 0.293 \n",
      "(epoch: 89, iters: 296, time: 1.274, data: 0.002) D_A: 0.176 G_A: 0.619 cycle_A: 1.181 idt_A: 0.401 D_B: 0.076 G_B: 0.558 cycle_B: 2.877 idt_B: 0.481 \n",
      "End of epoch 89 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 90, iters: 63, time: 0.311, data: 0.002) D_A: 0.186 G_A: 0.465 cycle_A: 0.793 idt_A: 0.771 D_B: 0.058 G_B: 0.546 cycle_B: 1.418 idt_B: 0.406 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 90, iters: 163, time: 0.309, data: 0.002) D_A: 0.096 G_A: 0.580 cycle_A: 1.449 idt_A: 0.556 D_B: 0.084 G_B: 0.553 cycle_B: 1.713 idt_B: 0.566 \n",
      "(epoch: 90, iters: 263, time: 0.304, data: 0.002) D_A: 0.226 G_A: 0.297 cycle_A: 0.783 idt_A: 0.444 D_B: 0.109 G_B: 0.595 cycle_B: 1.055 idt_B: 0.405 \n",
      "End of epoch 90 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 91, iters: 30, time: 1.325, data: 0.002) D_A: 0.048 G_A: 0.585 cycle_A: 2.373 idt_A: 2.232 D_B: 0.070 G_B: 0.383 cycle_B: 3.926 idt_B: 1.130 \n",
      "saving the latest model (epoch 91, total_iters 30000)\n",
      "(epoch: 91, iters: 130, time: 0.289, data: 0.002) D_A: 0.188 G_A: 0.398 cycle_A: 1.500 idt_A: 1.178 D_B: 0.069 G_B: 0.551 cycle_B: 1.209 idt_B: 0.517 \n",
      "(epoch: 91, iters: 230, time: 0.302, data: 0.002) D_A: 0.191 G_A: 0.359 cycle_A: 0.855 idt_A: 1.084 D_B: 0.075 G_B: 0.556 cycle_B: 1.278 idt_B: 0.362 \n",
      "(epoch: 91, iters: 330, time: 0.306, data: 0.001) D_A: 0.167 G_A: 0.510 cycle_A: 1.517 idt_A: 0.301 D_B: 0.062 G_B: 0.508 cycle_B: 0.661 idt_B: 0.918 \n",
      "End of epoch 91 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 92, iters: 97, time: 1.381, data: 0.002) D_A: 0.209 G_A: 0.454 cycle_A: 0.677 idt_A: 1.120 D_B: 0.231 G_B: 0.558 cycle_B: 2.645 idt_B: 0.205 \n",
      "(epoch: 92, iters: 197, time: 0.312, data: 0.002) D_A: 0.206 G_A: 0.603 cycle_A: 0.960 idt_A: 0.688 D_B: 0.067 G_B: 0.632 cycle_B: 2.740 idt_B: 0.415 \n",
      "(epoch: 92, iters: 297, time: 0.306, data: 0.002) D_A: 0.056 G_A: 0.436 cycle_A: 1.809 idt_A: 0.552 D_B: 0.086 G_B: 0.526 cycle_B: 1.375 idt_B: 0.707 \n",
      "End of epoch 92 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 93, iters: 64, time: 0.295, data: 0.002) D_A: 0.304 G_A: 0.282 cycle_A: 2.730 idt_A: 0.995 D_B: 0.044 G_B: 0.618 cycle_B: 3.110 idt_B: 1.089 \n",
      "(epoch: 93, iters: 164, time: 1.338, data: 0.002) D_A: 0.201 G_A: 0.408 cycle_A: 1.601 idt_A: 0.591 D_B: 0.081 G_B: 0.707 cycle_B: 1.959 idt_B: 0.495 \n",
      "(epoch: 93, iters: 264, time: 0.312, data: 0.002) D_A: 0.187 G_A: 0.494 cycle_A: 0.968 idt_A: 0.430 D_B: 0.192 G_B: 0.645 cycle_B: 0.727 idt_B: 0.353 \n",
      "End of epoch 93 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 94, iters: 31, time: 0.311, data: 0.002) D_A: 0.101 G_A: 0.425 cycle_A: 1.825 idt_A: 0.511 D_B: 0.060 G_B: 0.630 cycle_B: 0.987 idt_B: 0.967 \n",
      "(epoch: 94, iters: 131, time: 0.309, data: 0.002) D_A: 0.200 G_A: 0.332 cycle_A: 1.861 idt_A: 1.493 D_B: 0.041 G_B: 0.618 cycle_B: 1.629 idt_B: 0.627 \n",
      "(epoch: 94, iters: 231, time: 1.261, data: 0.002) D_A: 0.187 G_A: 0.603 cycle_A: 1.169 idt_A: 0.505 D_B: 0.137 G_B: 0.547 cycle_B: 1.147 idt_B: 0.504 \n",
      "(epoch: 94, iters: 331, time: 0.308, data: 0.002) D_A: 0.084 G_A: 0.440 cycle_A: 0.731 idt_A: 1.046 D_B: 0.068 G_B: 0.621 cycle_B: 1.224 idt_B: 0.268 \n",
      "End of epoch 94 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 95, iters: 98, time: 0.307, data: 0.001) D_A: 0.075 G_A: 0.536 cycle_A: 0.850 idt_A: 1.384 D_B: 0.108 G_B: 0.621 cycle_B: 2.603 idt_B: 0.757 \n",
      "(epoch: 95, iters: 198, time: 0.305, data: 0.001) D_A: 0.168 G_A: 0.459 cycle_A: 1.302 idt_A: 0.330 D_B: 0.303 G_B: 0.484 cycle_B: 1.302 idt_B: 0.540 \n",
      "(epoch: 95, iters: 298, time: 1.347, data: 0.002) D_A: 0.136 G_A: 0.610 cycle_A: 1.087 idt_A: 0.677 D_B: 0.191 G_B: 0.599 cycle_B: 1.276 idt_B: 0.364 \n",
      "End of epoch 95 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 96, iters: 65, time: 0.309, data: 0.002) D_A: 0.074 G_A: 0.519 cycle_A: 1.664 idt_A: 1.274 D_B: 0.068 G_B: 0.620 cycle_B: 2.941 idt_B: 0.957 \n",
      "(epoch: 96, iters: 165, time: 0.306, data: 0.001) D_A: 0.217 G_A: 0.528 cycle_A: 0.952 idt_A: 0.805 D_B: 0.150 G_B: 0.617 cycle_B: 0.906 idt_B: 0.439 \n",
      "(epoch: 96, iters: 265, time: 0.306, data: 0.002) D_A: 0.147 G_A: 0.362 cycle_A: 1.021 idt_A: 0.570 D_B: 0.066 G_B: 0.674 cycle_B: 1.004 idt_B: 0.432 \n",
      "End of epoch 96 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 97, iters: 32, time: 1.359, data: 0.002) D_A: 0.141 G_A: 0.381 cycle_A: 0.442 idt_A: 0.853 D_B: 0.132 G_B: 0.434 cycle_B: 1.481 idt_B: 0.187 \n",
      "(epoch: 97, iters: 132, time: 0.294, data: 0.001) D_A: 0.132 G_A: 0.445 cycle_A: 0.910 idt_A: 0.264 D_B: 0.160 G_B: 0.486 cycle_B: 0.912 idt_B: 0.327 \n",
      "(epoch: 97, iters: 232, time: 0.295, data: 0.002) D_A: 0.105 G_A: 0.392 cycle_A: 1.026 idt_A: 1.085 D_B: 0.068 G_B: 0.511 cycle_B: 1.796 idt_B: 0.637 \n",
      "(epoch: 97, iters: 332, time: 0.311, data: 0.002) D_A: 0.107 G_A: 0.578 cycle_A: 0.790 idt_A: 1.204 D_B: 0.136 G_B: 0.730 cycle_B: 2.485 idt_B: 0.315 \n",
      "End of epoch 97 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 98, iters: 99, time: 1.344, data: 0.002) D_A: 0.123 G_A: 0.408 cycle_A: 0.597 idt_A: 1.370 D_B: 0.101 G_B: 0.504 cycle_B: 1.194 idt_B: 0.252 \n",
      "(epoch: 98, iters: 199, time: 0.311, data: 0.002) D_A: 0.115 G_A: 0.608 cycle_A: 1.342 idt_A: 0.852 D_B: 0.131 G_B: 0.539 cycle_B: 1.822 idt_B: 0.517 \n",
      "(epoch: 98, iters: 299, time: 0.310, data: 0.002) D_A: 0.245 G_A: 0.386 cycle_A: 1.284 idt_A: 0.521 D_B: 0.121 G_B: 0.569 cycle_B: 1.156 idt_B: 0.578 \n",
      "End of epoch 98 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 99, iters: 66, time: 0.310, data: 0.002) D_A: 0.078 G_A: 0.445 cycle_A: 0.590 idt_A: 1.199 D_B: 0.136 G_B: 0.508 cycle_B: 4.156 idt_B: 0.243 \n",
      "(epoch: 99, iters: 166, time: 1.411, data: 0.002) D_A: 0.161 G_A: 0.485 cycle_A: 1.781 idt_A: 0.645 D_B: 0.072 G_B: 0.581 cycle_B: 1.018 idt_B: 0.744 \n",
      "(epoch: 99, iters: 266, time: 0.311, data: 0.001) D_A: 0.190 G_A: 0.395 cycle_A: 1.272 idt_A: 1.417 D_B: 0.154 G_B: 0.536 cycle_B: 4.044 idt_B: 0.530 \n",
      "End of epoch 99 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 100, iters: 33, time: 0.294, data: 0.002) D_A: 0.190 G_A: 0.336 cycle_A: 0.897 idt_A: 1.472 D_B: 0.089 G_B: 0.503 cycle_B: 1.044 idt_B: 0.439 \n",
      "(epoch: 100, iters: 133, time: 0.306, data: 0.002) D_A: 0.143 G_A: 0.741 cycle_A: 3.066 idt_A: 0.510 D_B: 0.298 G_B: 0.575 cycle_B: 1.028 idt_B: 1.306 \n",
      "(epoch: 100, iters: 233, time: 1.408, data: 0.001) D_A: 0.099 G_A: 0.826 cycle_A: 1.249 idt_A: 0.360 D_B: 0.086 G_B: 0.735 cycle_B: 0.763 idt_B: 0.387 \n",
      "(epoch: 100, iters: 333, time: 0.305, data: 0.002) D_A: 0.138 G_A: 0.616 cycle_A: 4.400 idt_A: 0.580 D_B: 0.108 G_B: 0.433 cycle_B: 1.477 idt_B: 1.963 \n",
      "End of epoch 100 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 101, iters: 100, time: 0.299, data: 0.278) D_A: 0.077 G_A: 0.466 cycle_A: 1.679 idt_A: 0.853 D_B: 0.189 G_B: 0.766 cycle_B: 1.981 idt_B: 0.675 \n",
      "(epoch: 101, iters: 200, time: 0.303, data: 0.001) D_A: 0.208 G_A: 0.319 cycle_A: 0.645 idt_A: 0.397 D_B: 0.088 G_B: 0.528 cycle_B: 1.120 idt_B: 0.205 \n",
      "(epoch: 101, iters: 300, time: 1.375, data: 0.002) D_A: 0.068 G_A: 0.496 cycle_A: 2.261 idt_A: 1.825 D_B: 0.102 G_B: 0.560 cycle_B: 2.523 idt_B: 0.898 \n",
      "End of epoch 101 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 102, iters: 67, time: 0.311, data: 0.001) D_A: 0.121 G_A: 0.440 cycle_A: 0.704 idt_A: 0.301 D_B: 0.150 G_B: 0.614 cycle_B: 1.225 idt_B: 0.273 \n",
      "(epoch: 102, iters: 167, time: 0.311, data: 0.002) D_A: 0.158 G_A: 0.506 cycle_A: 1.305 idt_A: 0.557 D_B: 0.075 G_B: 0.499 cycle_B: 1.117 idt_B: 0.534 \n",
      "(epoch: 102, iters: 267, time: 0.308, data: 0.001) D_A: 0.098 G_A: 0.341 cycle_A: 0.819 idt_A: 0.524 D_B: 0.120 G_B: 0.619 cycle_B: 1.483 idt_B: 0.371 \n",
      "End of epoch 102 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 103, iters: 34, time: 1.309, data: 0.001) D_A: 0.204 G_A: 0.497 cycle_A: 2.804 idt_A: 0.614 D_B: 0.091 G_B: 0.426 cycle_B: 2.748 idt_B: 0.790 \n",
      "(epoch: 103, iters: 134, time: 0.307, data: 0.002) D_A: 0.078 G_A: 0.463 cycle_A: 1.484 idt_A: 0.810 D_B: 0.065 G_B: 0.599 cycle_B: 1.592 idt_B: 0.644 \n",
      "(epoch: 103, iters: 234, time: 0.311, data: 0.002) D_A: 0.170 G_A: 0.375 cycle_A: 0.631 idt_A: 0.850 D_B: 0.238 G_B: 0.807 cycle_B: 2.815 idt_B: 0.243 \n",
      "End of epoch 103 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 104, iters: 1, time: 0.290, data: 0.002) D_A: 0.233 G_A: 0.557 cycle_A: 0.744 idt_A: 0.502 D_B: 0.140 G_B: 0.604 cycle_B: 0.991 idt_B: 0.282 \n",
      "(epoch: 104, iters: 101, time: 1.447, data: 0.000) D_A: 0.129 G_A: 0.743 cycle_A: 0.979 idt_A: 0.495 D_B: 0.129 G_B: 0.771 cycle_B: 1.218 idt_B: 0.443 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 104, iters: 201, time: 0.308, data: 0.001) D_A: 0.216 G_A: 0.228 cycle_A: 0.638 idt_A: 1.153 D_B: 0.148 G_B: 0.469 cycle_B: 1.126 idt_B: 0.332 \n",
      "(epoch: 104, iters: 301, time: 0.311, data: 0.001) D_A: 0.183 G_A: 0.664 cycle_A: 4.380 idt_A: 1.414 D_B: 0.098 G_B: 0.662 cycle_B: 2.249 idt_B: 1.946 \n",
      "End of epoch 104 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 105, iters: 68, time: 0.293, data: 0.002) D_A: 0.119 G_A: 0.744 cycle_A: 1.182 idt_A: 0.697 D_B: 0.170 G_B: 0.550 cycle_B: 1.402 idt_B: 0.560 \n",
      "(epoch: 105, iters: 168, time: 1.384, data: 0.002) D_A: 0.198 G_A: 0.382 cycle_A: 0.916 idt_A: 0.595 D_B: 0.113 G_B: 0.749 cycle_B: 1.314 idt_B: 0.348 \n",
      "(epoch: 105, iters: 268, time: 0.293, data: 0.002) D_A: 0.221 G_A: 0.410 cycle_A: 0.601 idt_A: 0.460 D_B: 0.090 G_B: 0.362 cycle_B: 2.614 idt_B: 0.223 \n",
      "End of epoch 105 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 106, iters: 35, time: 0.298, data: 0.002) D_A: 0.066 G_A: 0.427 cycle_A: 0.664 idt_A: 1.523 D_B: 0.149 G_B: 0.495 cycle_B: 2.109 idt_B: 0.377 \n",
      "saving the latest model (epoch 106, total_iters 35000)\n",
      "(epoch: 106, iters: 135, time: 0.287, data: 0.002) D_A: 0.096 G_A: 0.570 cycle_A: 0.902 idt_A: 1.108 D_B: 0.173 G_B: 0.673 cycle_B: 2.238 idt_B: 0.533 \n",
      "(epoch: 106, iters: 235, time: 1.403, data: 0.002) D_A: 0.300 G_A: 0.544 cycle_A: 1.138 idt_A: 0.479 D_B: 0.104 G_B: 0.587 cycle_B: 0.898 idt_B: 0.489 \n",
      "End of epoch 106 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 107, iters: 2, time: 0.309, data: 0.002) D_A: 0.196 G_A: 0.565 cycle_A: 2.401 idt_A: 1.454 D_B: 0.072 G_B: 0.615 cycle_B: 2.060 idt_B: 1.234 \n",
      "(epoch: 107, iters: 102, time: 0.309, data: 0.002) D_A: 0.198 G_A: 0.429 cycle_A: 0.714 idt_A: 1.207 D_B: 0.191 G_B: 0.583 cycle_B: 1.204 idt_B: 0.246 \n",
      "(epoch: 107, iters: 202, time: 0.311, data: 0.002) D_A: 0.095 G_A: 0.463 cycle_A: 1.404 idt_A: 2.281 D_B: 0.112 G_B: 0.638 cycle_B: 2.123 idt_B: 0.557 \n",
      "(epoch: 107, iters: 302, time: 1.457, data: 0.002) D_A: 0.226 G_A: 0.473 cycle_A: 2.794 idt_A: 0.429 D_B: 0.041 G_B: 0.687 cycle_B: 1.475 idt_B: 1.300 \n",
      "End of epoch 107 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 108, iters: 69, time: 0.308, data: 0.002) D_A: 0.144 G_A: 0.458 cycle_A: 1.651 idt_A: 0.935 D_B: 0.030 G_B: 0.722 cycle_B: 1.642 idt_B: 0.579 \n",
      "(epoch: 108, iters: 169, time: 0.305, data: 0.002) D_A: 0.133 G_A: 0.384 cycle_A: 1.952 idt_A: 0.463 D_B: 0.043 G_B: 0.375 cycle_B: 1.503 idt_B: 0.851 \n",
      "(epoch: 108, iters: 269, time: 0.311, data: 0.002) D_A: 0.129 G_A: 0.469 cycle_A: 1.385 idt_A: 0.448 D_B: 0.163 G_B: 0.742 cycle_B: 1.023 idt_B: 0.621 \n",
      "End of epoch 108 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 109, iters: 36, time: 1.364, data: 0.002) D_A: 0.201 G_A: 0.688 cycle_A: 1.769 idt_A: 1.919 D_B: 0.046 G_B: 0.611 cycle_B: 1.141 idt_B: 0.740 \n",
      "(epoch: 109, iters: 136, time: 0.307, data: 0.002) D_A: 0.135 G_A: 0.554 cycle_A: 0.590 idt_A: 0.621 D_B: 0.154 G_B: 0.611 cycle_B: 1.148 idt_B: 0.321 \n",
      "(epoch: 109, iters: 236, time: 0.306, data: 0.001) D_A: 0.078 G_A: 0.477 cycle_A: 1.311 idt_A: 2.012 D_B: 0.029 G_B: 0.667 cycle_B: 2.804 idt_B: 0.628 \n",
      "End of epoch 109 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 110, iters: 3, time: 0.291, data: 0.001) D_A: 0.188 G_A: 0.521 cycle_A: 0.940 idt_A: 0.447 D_B: 0.104 G_B: 0.546 cycle_B: 1.896 idt_B: 0.485 \n",
      "(epoch: 110, iters: 103, time: 1.453, data: 0.002) D_A: 0.124 G_A: 0.491 cycle_A: 1.413 idt_A: 0.316 D_B: 0.053 G_B: 0.578 cycle_B: 1.009 idt_B: 0.742 \n",
      "(epoch: 110, iters: 203, time: 0.305, data: 0.002) D_A: 0.154 G_A: 0.486 cycle_A: 0.893 idt_A: 0.436 D_B: 0.160 G_B: 0.633 cycle_B: 1.155 idt_B: 0.601 \n",
      "(epoch: 110, iters: 303, time: 0.302, data: 0.002) D_A: 0.187 G_A: 0.327 cycle_A: 0.678 idt_A: 0.661 D_B: 0.114 G_B: 0.415 cycle_B: 1.198 idt_B: 0.273 \n",
      "End of epoch 110 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 111, iters: 70, time: 0.310, data: 0.002) D_A: 0.062 G_A: 0.531 cycle_A: 0.652 idt_A: 2.947 D_B: 0.111 G_B: 0.576 cycle_B: 6.156 idt_B: 0.441 \n",
      "(epoch: 111, iters: 170, time: 1.511, data: 0.002) D_A: 0.242 G_A: 0.414 cycle_A: 0.781 idt_A: 0.329 D_B: 0.033 G_B: 0.680 cycle_B: 1.515 idt_B: 0.291 \n",
      "(epoch: 111, iters: 270, time: 0.310, data: 0.002) D_A: 0.118 G_A: 0.431 cycle_A: 1.642 idt_A: 0.327 D_B: 0.133 G_B: 0.481 cycle_B: 1.189 idt_B: 0.650 \n",
      "End of epoch 111 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 112, iters: 37, time: 0.310, data: 0.002) D_A: 0.192 G_A: 0.579 cycle_A: 1.120 idt_A: 0.877 D_B: 0.077 G_B: 0.604 cycle_B: 1.197 idt_B: 0.622 \n",
      "(epoch: 112, iters: 137, time: 0.304, data: 0.002) D_A: 0.146 G_A: 0.576 cycle_A: 1.000 idt_A: 0.551 D_B: 0.124 G_B: 0.796 cycle_B: 0.968 idt_B: 0.361 \n",
      "(epoch: 112, iters: 237, time: 1.428, data: 0.002) D_A: 0.202 G_A: 0.464 cycle_A: 1.241 idt_A: 1.129 D_B: 0.130 G_B: 0.529 cycle_B: 3.123 idt_B: 0.476 \n",
      "End of epoch 112 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 113, iters: 4, time: 0.308, data: 0.002) D_A: 0.188 G_A: 0.447 cycle_A: 1.673 idt_A: 0.438 D_B: 0.039 G_B: 0.733 cycle_B: 1.215 idt_B: 0.892 \n",
      "(epoch: 113, iters: 104, time: 0.311, data: 0.005) D_A: 0.055 G_A: 0.415 cycle_A: 0.773 idt_A: 0.686 D_B: 0.048 G_B: 0.656 cycle_B: 1.011 idt_B: 0.305 \n",
      "(epoch: 113, iters: 204, time: 0.308, data: 0.002) D_A: 0.197 G_A: 0.436 cycle_A: 0.801 idt_A: 0.470 D_B: 0.095 G_B: 0.546 cycle_B: 0.994 idt_B: 0.307 \n",
      "(epoch: 113, iters: 304, time: 1.446, data: 0.002) D_A: 0.101 G_A: 0.515 cycle_A: 0.924 idt_A: 0.510 D_B: 0.128 G_B: 0.702 cycle_B: 1.041 idt_B: 0.362 \n",
      "End of epoch 113 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 114, iters: 71, time: 0.304, data: 0.002) D_A: 0.178 G_A: 0.442 cycle_A: 1.750 idt_A: 0.893 D_B: 0.119 G_B: 0.675 cycle_B: 1.707 idt_B: 0.879 \n",
      "(epoch: 114, iters: 171, time: 0.310, data: 0.001) D_A: 0.107 G_A: 0.513 cycle_A: 2.453 idt_A: 0.687 D_B: 0.140 G_B: 0.555 cycle_B: 1.097 idt_B: 0.339 \n",
      "(epoch: 114, iters: 271, time: 0.310, data: 0.002) D_A: 0.091 G_A: 0.575 cycle_A: 0.559 idt_A: 0.524 D_B: 0.061 G_B: 0.547 cycle_B: 1.084 idt_B: 0.322 \n",
      "End of epoch 114 / 300 \t Time Taken: 94 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 115, iters: 38, time: 1.416, data: 0.002) D_A: 0.073 G_A: 0.631 cycle_A: 1.005 idt_A: 1.887 D_B: 0.066 G_B: 0.717 cycle_B: 2.697 idt_B: 0.464 \n",
      "(epoch: 115, iters: 138, time: 0.311, data: 0.002) D_A: 0.081 G_A: 0.449 cycle_A: 0.567 idt_A: 0.432 D_B: 0.097 G_B: 0.577 cycle_B: 2.117 idt_B: 0.259 \n",
      "(epoch: 115, iters: 238, time: 0.308, data: 0.001) D_A: 0.198 G_A: 0.458 cycle_A: 0.979 idt_A: 1.299 D_B: 0.075 G_B: 0.536 cycle_B: 1.204 idt_B: 0.501 \n",
      "End of epoch 115 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 116, iters: 5, time: 0.308, data: 0.002) D_A: 0.112 G_A: 0.493 cycle_A: 1.283 idt_A: 0.332 D_B: 0.104 G_B: 0.637 cycle_B: 1.480 idt_B: 0.581 \n",
      "(epoch: 116, iters: 105, time: 1.418, data: 0.000) D_A: 0.223 G_A: 0.413 cycle_A: 1.870 idt_A: 1.389 D_B: 0.043 G_B: 0.692 cycle_B: 3.269 idt_B: 0.776 \n",
      "(epoch: 116, iters: 205, time: 0.308, data: 0.002) D_A: 0.135 G_A: 0.689 cycle_A: 1.181 idt_A: 1.310 D_B: 0.043 G_B: 0.650 cycle_B: 3.455 idt_B: 0.409 \n",
      "(epoch: 116, iters: 305, time: 0.310, data: 0.001) D_A: 0.137 G_A: 0.582 cycle_A: 3.466 idt_A: 0.539 D_B: 0.144 G_B: 0.444 cycle_B: 1.210 idt_B: 1.238 \n",
      "End of epoch 116 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 117, iters: 72, time: 0.310, data: 0.001) D_A: 0.168 G_A: 0.352 cycle_A: 2.637 idt_A: 0.316 D_B: 0.076 G_B: 0.542 cycle_B: 1.016 idt_B: 1.130 \n",
      "(epoch: 117, iters: 172, time: 1.504, data: 0.002) D_A: 0.114 G_A: 0.398 cycle_A: 1.416 idt_A: 0.510 D_B: 0.194 G_B: 0.449 cycle_B: 1.994 idt_B: 0.530 \n",
      "(epoch: 117, iters: 272, time: 0.311, data: 0.002) D_A: 0.119 G_A: 0.570 cycle_A: 1.239 idt_A: 1.291 D_B: 0.092 G_B: 0.711 cycle_B: 2.984 idt_B: 0.530 \n",
      "End of epoch 117 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 118, iters: 39, time: 0.310, data: 0.002) D_A: 0.162 G_A: 0.643 cycle_A: 1.752 idt_A: 1.672 D_B: 0.037 G_B: 0.586 cycle_B: 2.070 idt_B: 0.595 \n",
      "(epoch: 118, iters: 139, time: 0.295, data: 0.002) D_A: 0.101 G_A: 0.531 cycle_A: 1.485 idt_A: 0.527 D_B: 0.066 G_B: 0.685 cycle_B: 1.383 idt_B: 0.728 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 118, iters: 239, time: 1.500, data: 0.002) D_A: 0.044 G_A: 0.388 cycle_A: 1.618 idt_A: 1.223 D_B: 0.073 G_B: 0.491 cycle_B: 2.816 idt_B: 0.720 \n",
      "End of epoch 118 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 119, iters: 6, time: 0.301, data: 0.001) D_A: 0.079 G_A: 0.554 cycle_A: 1.093 idt_A: 1.590 D_B: 0.200 G_B: 0.671 cycle_B: 2.360 idt_B: 0.401 \n",
      "(epoch: 119, iters: 106, time: 0.310, data: 0.000) D_A: 0.082 G_A: 0.432 cycle_A: 3.409 idt_A: 1.430 D_B: 0.100 G_B: 0.589 cycle_B: 2.695 idt_B: 1.349 \n",
      "(epoch: 119, iters: 206, time: 0.309, data: 0.002) D_A: 0.023 G_A: 0.513 cycle_A: 0.551 idt_A: 2.156 D_B: 0.193 G_B: 0.668 cycle_B: 4.112 idt_B: 0.289 \n",
      "(epoch: 119, iters: 306, time: 1.493, data: 0.002) D_A: 0.062 G_A: 0.518 cycle_A: 0.891 idt_A: 1.085 D_B: 0.177 G_B: 0.630 cycle_B: 2.634 idt_B: 0.300 \n",
      "End of epoch 119 / 300 \t Time Taken: 96 sec\n",
      "learning rate 0.0000010 -> 0.0000001\n",
      "(epoch: 120, iters: 73, time: 0.309, data: 0.002) D_A: 0.129 G_A: 0.381 cycle_A: 1.536 idt_A: 1.500 D_B: 0.040 G_B: 0.692 cycle_B: 3.176 idt_B: 0.761 \n",
      "(epoch: 120, iters: 173, time: 0.300, data: 0.001) D_A: 0.122 G_A: 0.409 cycle_A: 1.906 idt_A: 1.402 D_B: 0.080 G_B: 0.713 cycle_B: 2.834 idt_B: 0.695 \n",
      "(epoch: 120, iters: 273, time: 0.307, data: 0.001) D_A: 0.042 G_A: 0.723 cycle_A: 1.074 idt_A: 1.386 D_B: 0.084 G_B: 0.490 cycle_B: 4.261 idt_B: 0.406 \n",
      "saving the model at the end of epoch 120, iters 39960\n",
      "End of epoch 120 / 300 \t Time Taken: 95 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 121, iters: 40, time: 1.542, data: 0.001) D_A: 0.106 G_A: 0.704 cycle_A: 1.350 idt_A: 0.498 D_B: 0.201 G_B: 0.897 cycle_B: 1.348 idt_B: 0.448 \n",
      "saving the latest model (epoch 121, total_iters 40000)\n",
      "(epoch: 121, iters: 140, time: 0.297, data: 0.002) D_A: 0.218 G_A: 0.490 cycle_A: 1.353 idt_A: 0.663 D_B: 0.165 G_B: 0.608 cycle_B: 1.237 idt_B: 0.406 \n",
      "(epoch: 121, iters: 240, time: 0.297, data: 0.002) D_A: 0.170 G_A: 0.393 cycle_A: 0.483 idt_A: 0.381 D_B: 0.149 G_B: 0.414 cycle_B: 1.303 idt_B: 0.163 \n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/3focal_plane/Aligned/train/ \\\n",
    "--name cycle_b18z40_63x_3f_resize_aligned \\\n",
    "--model cycle_gan --lr=0.0001 --lr_decay_iters=40 --lr_policy step --batch_size=1 \\\n",
    "--preprocess resize --load_size=256 \\\n",
    "--lambda_identity=0.45 --save_epoch_freq=60 --n_epochs=200  --display_port=8087 --gpu_ids=2 \\\n",
    "--gauss --contrast --sharpness --brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "                augment_a: False                         \n",
      "               batch_size: 1                             \n",
      "               brightness: False                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                 contrast: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/Aligned/test/testA/\t[default: None]\n",
      "             dataset_mode: single                        \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                    gauss: False                         \n",
      "                  gpu_ids: 3                             \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: test                          \n",
      "             model_suffix:                               \n",
      "               n_layers_D: 3                             \n",
      "                     name: cycle_b18z40-63x_resize_aligned\t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \t[default: False]\n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 10000                         \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize                        \t[default: resize_and_crop]\n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                sharpness: False                         \n",
      "                   suffix:                               \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "dataset [SingleDataset] was created\n",
      "^C\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gauss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cc9c3f6258bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python test.py --dataroot  /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/Aligned/test/testA/  --name cycle_b18z40-63x_resize_aligned --model test  --no_dropout --preprocess resize --load_size 256 --gpu_ids=3 --num_test=10000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgauss\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcontrast\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msharpness\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbrightness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gauss' is not defined"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot  /storage01/grexai/datasets/imgreg/Image_registration/b18z40-63x/Aligned/test/testA/ \\\n",
    "--name cycle_b18z40-63x_resize_aligned --model test \\\n",
    "--no_dropout --preprocess resize --load_size 2oo56 --gpu_ids=3 --num_test=10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unaligned dataset training 60-63x\n",
    "#unaligend testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                augment_a: False                         \n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "               brightness: False                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                 contrast: False                         \n",
      "                crop_size: 360                           \t[default: 256]\n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/Unaligned/\t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8088                          \t[default: 8097]\n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                    gauss: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 1024                          \t[default: 286]\n",
      "                       lr: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 40                            \t[default: 50]\n",
      "                lr_policy: step                          \t[default: linear]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 200                           \t[default: 100]\n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: cycle_hela63_resize_unaligned \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: scale_width_and_crop          \t[default: resize_and_crop]\n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 50                            \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                sharpness: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 288\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "Setting up a new session...\n",
      "Exception in user code:\n",
      "------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 170, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1285, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1331, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1280, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1046, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 984, in send\n",
      "    self.connect()\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 182, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f59f6436080>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8088): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f59f6436080>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/visdom/__init__.py\", line 711, in _send\n",
      "    data=json.dumps(msg),\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/visdom/__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8088): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f59f6436080>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "[Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Could not connect to Visdom server. \n",
      " Trying to start a server....\n",
      "Command: /storage01/grexai/dev/envs/pix2pix/bin/python -m visdom.server -p 8088 &>/dev/null &\n",
      "create web directory ./checkpoints/cycle_hela63_resize_unaligned/web...\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 1, iters: 100, time: 0.557, data: 0.484) D_A: 0.230 G_A: 0.489 cycle_A: 3.196 idt_A: 0.329 D_B: 0.295 G_B: 0.476 cycle_B: 0.674 idt_B: 1.673 \n",
      "(epoch: 1, iters: 200, time: 0.560, data: 0.002) D_A: 0.288 G_A: 0.539 cycle_A: 2.516 idt_A: 0.417 D_B: 0.246 G_B: 0.413 cycle_B: 0.852 idt_B: 1.158 \n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /storage01/grexai/datasets/imgreg/Image_registration/Unaligned/ \\\n",
    "--name cycle_hela63_scalecrop_unaligned \\\n",
    "--model cycle_gan --lr=0.0001 --lr_policy step --lr_decay_iters=40 \\\n",
    "--preprocess scale_width_and_crop --crop 360 --load_size 1024 \\\n",
    "--lambda_identity=0.5 --save_epoch_freq=50 --n_epochs=200  --display_port=8088 --gpu_ids=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "                augment_a: False                         \n",
      "               batch_size: 1                             \n",
      "               brightness: False                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                 contrast: False                         \n",
      "                crop_size: 360                           \t[default: 256]\n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/\t[default: None]\n",
      "             dataset_mode: single                        \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                    gauss: False                         \n",
      "                  gpu_ids: 3                             \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 1024                          \t[default: 256]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: test                          \n",
      "             model_suffix:                               \n",
      "               n_layers_D: 3                             \n",
      "                     name: cycle_hela63_unaligned_scale_crop\t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \t[default: False]\n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 10000                         \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: scale_width                   \t[default: resize_and_crop]\n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                sharpness: False                         \n",
      "                   suffix:                               \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [SingleDataset] was created\n",
      "initialize network with normal\n",
      "model [TestModel] was created\n",
      "loading the model from ./checkpoints/cycle_hela63_unaligned_scale_crop/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "creating web directory ./results/cycle_hela63_unaligned_scale_crop/test_latest\n",
      "<data.CustomDatasetDataLoader object at 0x7f58b031ac88>\n",
      "processing (0000)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m112_c1_z0_l1_o0_1.png']\n",
      "processing (0001)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m112_c1_z0_l1_o0_2.png']\n",
      "processing (0002)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m112_c1_z0_l1_o0_3.png']\n",
      "processing (0003)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m141_c1_z0_l1_o0_1.png']\n",
      "processing (0004)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m141_c1_z0_l1_o0_2.png']\n",
      "processing (0005)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m141_c1_z0_l1_o0_3.png']\n",
      "processing (0006)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m151_c1_z0_l1_o0_1.png']\n",
      "processing (0007)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m151_c1_z0_l1_o0_2.png']\n",
      "processing (0008)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m151_c1_z0_l1_o0_3.png']\n",
      "processing (0009)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m192_c1_z0_l1_o0_1.png']\n",
      "processing (0010)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m192_c1_z0_l1_o0_2.png']\n",
      "processing (0011)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m192_c1_z0_l1_o0_3.png']\n",
      "processing (0012)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m212_c1_z0_l1_o0_1.png']\n",
      "processing (0013)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m212_c1_z0_l1_o0_2.png']\n",
      "processing (0014)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m212_c1_z0_l1_o0_3.png']\n",
      "processing (0015)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m222_c1_z0_l1_o0_1.png']\n",
      "processing (0016)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m222_c1_z0_l1_o0_2.png']\n",
      "processing (0017)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m222_c1_z0_l1_o0_3.png']\n",
      "processing (0018)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m22_c1_z0_l1_o0_1.png']\n",
      "processing (0019)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m22_c1_z0_l1_o0_2.png']\n",
      "processing (0020)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m22_c1_z0_l1_o0_3.png']\n",
      "processing (0021)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m260_c1_z0_l1_o0_1.png']\n",
      "processing (0022)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m260_c1_z0_l1_o0_2.png']\n",
      "processing (0023)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m260_c1_z0_l1_o0_3.png']\n",
      "processing (0024)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m27_c1_z0_l1_o0_1.png']\n",
      "processing (0025)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m27_c1_z0_l1_o0_2.png']\n",
      "processing (0026)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m27_c1_z0_l1_o0_3.png']\n",
      "processing (0027)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m281_c1_z0_l1_o0_1.png']\n",
      "processing (0028)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m281_c1_z0_l1_o0_2.png']\n",
      "processing (0029)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m281_c1_z0_l1_o0_3.png']\n",
      "processing (0030)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m290_c1_z0_l1_o0_1.png']\n",
      "processing (0031)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m290_c1_z0_l1_o0_2.png']\n",
      "processing (0032)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m290_c1_z0_l1_o0_3.png']\n",
      "processing (0033)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m322_c1_z0_l1_o0_1.png']\n",
      "processing (0034)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m322_c1_z0_l1_o0_2.png']\n",
      "processing (0035)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m322_c1_z0_l1_o0_3.png']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing (0036)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m323_c1_z0_l1_o0_1.png']\n",
      "processing (0037)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m323_c1_z0_l1_o0_2.png']\n",
      "processing (0038)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m323_c1_z0_l1_o0_3.png']\n",
      "processing (0039)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m332_c1_z0_l1_o0_1.png']\n",
      "processing (0040)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m332_c1_z0_l1_o0_2.png']\n",
      "processing (0041)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m332_c1_z0_l1_o0_3.png']\n",
      "processing (0042)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m333_c1_z0_l1_o0_1.png']\n",
      "processing (0043)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m333_c1_z0_l1_o0_2.png']\n",
      "processing (0044)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m333_c1_z0_l1_o0_3.png']\n",
      "processing (0045)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m37_c1_z0_l1_o0_1.png']\n",
      "processing (0046)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m37_c1_z0_l1_o0_2.png']\n",
      "processing (0047)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m37_c1_z0_l1_o0_3.png']\n",
      "processing (0048)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m60_c1_z0_l1_o0_1.png']\n",
      "processing (0049)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m60_c1_z0_l1_o0_2.png']\n",
      "processing (0050)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m60_c1_z0_l1_o0_3.png']\n",
      "processing (0051)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m63_c1_z0_l1_o0_1.png']\n",
      "processing (0052)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m63_c1_z0_l1_o0_2.png']\n",
      "processing (0053)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m63_c1_z0_l1_o0_3.png']\n",
      "processing (0054)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m72_c1_z0_l1_o0_1.png']\n",
      "processing (0055)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m72_c1_z0_l1_o0_2.png']\n",
      "processing (0056)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m72_c1_z0_l1_o0_3.png']\n",
      "processing (0057)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m84_c1_z0_l1_o0_1.png']\n",
      "processing (0058)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m84_c1_z0_l1_o0_2.png']\n",
      "processing (0059)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m84_c1_z0_l1_o0_3.png']\n",
      "processing (0060)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m87_c1_z0_l1_o0_1.png']\n",
      "processing (0061)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m87_c1_z0_l1_o0_2.png']\n",
      "processing (0062)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m87_c1_z0_l1_o0_3.png']\n",
      "processing (0063)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m94_c1_z0_l1_o0_1.png']\n",
      "processing (0064)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m94_c1_z0_l1_o0_2.png']\n",
      "processing (0065)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m94_c1_z0_l1_o0_3.png']\n",
      "processing (0066)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m9_c1_z0_l1_o0_1.png']\n",
      "processing (0067)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m9_c1_z0_l1_o0_2.png']\n",
      "processing (0068)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/testA/p1_wA1_t1_m9_c1_z0_l1_o0_3.png']\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot  /storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/ \\\n",
    "--name cycle_hela63_resize_unaligned --model test \\\n",
    "--no_dropout --preprocess scale_width --load_size 1024 --crop 360 --gpu_ids=3 --num_test=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                augment_a: False                         \n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "               brightness: False                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                 contrast: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/Unaligned/\t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8088                          \t[default: 8097]\n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                    gauss: False                         \n",
      "                  gpu_ids: 0,1                           \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \t[default: 286]\n",
      "                       lr: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 40                            \t[default: 50]\n",
      "                lr_policy: step                          \t[default: linear]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 200                           \t[default: 100]\n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: cycle_hela63_resize_unaligned \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: resize                        \t[default: resize_and_crop]\n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 50                            \t[default: 5]\n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                sharpness: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 288\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "Setting up a new session...\n",
      "Exception in user code:\n",
      "------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 170, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n",
      "    raise err\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 234, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1285, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1331, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1280, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1046, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 984, in send\n",
      "    self.connect()\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 200, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connection.py\", line 182, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fb243d87080>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8088): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb243d87080>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/visdom/__init__.py\", line 711, in _send\n",
      "    data=json.dumps(msg),\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/visdom/__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8088): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb243d87080>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "[Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Could not connect to Visdom server. \n",
      " Trying to start a server....\n",
      "Command: /storage01/grexai/dev/envs/pix2pix/bin/python -m visdom.server -p 8088 &>/dev/null &\n",
      "create web directory ./checkpoints/cycle_hela63_resize_unaligned/web...\n",
      "/storage01/grexai/dev/envs/pix2pix/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 1, iters: 100, time: 0.278, data: 0.471) D_A: 0.304 G_A: 0.432 cycle_A: 2.841 idt_A: 0.405 D_B: 0.420 G_B: 0.453 cycle_B: 0.912 idt_B: 1.450 \n",
      "(epoch: 1, iters: 200, time: 0.281, data: 0.003) D_A: 0.329 G_A: 0.510 cycle_A: 2.872 idt_A: 0.283 D_B: 0.257 G_B: 0.297 cycle_B: 0.662 idt_B: 1.384 \n",
      "End of epoch 1 / 300 \t Time Taken: 77 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 2, iters: 12, time: 0.289, data: 0.001) D_A: 0.274 G_A: 0.384 cycle_A: 2.377 idt_A: 0.257 D_B: 0.477 G_B: 0.373 cycle_B: 0.555 idt_B: 1.190 \n",
      "(epoch: 2, iters: 112, time: 0.967, data: 0.001) D_A: 0.176 G_A: 0.342 cycle_A: 4.861 idt_A: 0.224 D_B: 0.217 G_B: 0.427 cycle_B: 0.509 idt_B: 2.346 \n",
      "(epoch: 2, iters: 212, time: 0.313, data: 0.001) D_A: 0.204 G_A: 0.269 cycle_A: 1.416 idt_A: 0.267 D_B: 0.309 G_B: 0.309 cycle_B: 0.640 idt_B: 0.739 \n",
      "End of epoch 2 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 3, iters: 24, time: 0.315, data: 0.001) D_A: 0.239 G_A: 0.376 cycle_A: 3.809 idt_A: 0.257 D_B: 0.264 G_B: 0.343 cycle_B: 0.697 idt_B: 2.048 \n",
      "(epoch: 3, iters: 124, time: 0.278, data: 0.001) D_A: 0.156 G_A: 0.230 cycle_A: 2.156 idt_A: 0.281 D_B: 0.268 G_B: 0.534 cycle_B: 0.649 idt_B: 0.980 \n",
      "(epoch: 3, iters: 224, time: 0.796, data: 0.001) D_A: 0.185 G_A: 0.272 cycle_A: 2.654 idt_A: 0.183 D_B: 0.235 G_B: 0.472 cycle_B: 0.393 idt_B: 1.275 \n",
      "End of epoch 3 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 4, iters: 36, time: 0.303, data: 0.002) D_A: 0.230 G_A: 0.442 cycle_A: 3.056 idt_A: 0.310 D_B: 0.283 G_B: 0.271 cycle_B: 0.624 idt_B: 1.415 \n",
      "(epoch: 4, iters: 136, time: 0.331, data: 0.001) D_A: 0.180 G_A: 0.411 cycle_A: 1.379 idt_A: 0.384 D_B: 0.323 G_B: 0.402 cycle_B: 0.800 idt_B: 0.742 \n",
      "(epoch: 4, iters: 236, time: 0.284, data: 0.002) D_A: 0.073 G_A: 0.353 cycle_A: 1.704 idt_A: 0.185 D_B: 0.237 G_B: 0.369 cycle_B: 0.442 idt_B: 0.950 \n",
      "End of epoch 4 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 5, iters: 48, time: 0.790, data: 0.002) D_A: 0.280 G_A: 0.425 cycle_A: 2.074 idt_A: 0.187 D_B: 0.254 G_B: 0.711 cycle_B: 0.477 idt_B: 1.042 \n",
      "(epoch: 5, iters: 148, time: 0.319, data: 0.002) D_A: 0.395 G_A: 0.220 cycle_A: 2.016 idt_A: 0.210 D_B: 0.153 G_B: 0.412 cycle_B: 0.515 idt_B: 0.848 \n",
      "(epoch: 5, iters: 248, time: 0.318, data: 0.001) D_A: 0.112 G_A: 0.332 cycle_A: 1.935 idt_A: 0.195 D_B: 0.316 G_B: 0.745 cycle_B: 0.505 idt_B: 0.936 \n",
      "End of epoch 5 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 6, iters: 60, time: 0.303, data: 0.002) D_A: 0.118 G_A: 0.324 cycle_A: 1.833 idt_A: 0.242 D_B: 0.042 G_B: 0.921 cycle_B: 0.795 idt_B: 0.902 \n",
      "(epoch: 6, iters: 160, time: 0.831, data: 0.001) D_A: 0.238 G_A: 0.215 cycle_A: 2.383 idt_A: 0.232 D_B: 0.058 G_B: 0.372 cycle_B: 0.545 idt_B: 1.110 \n",
      "(epoch: 6, iters: 260, time: 0.313, data: 0.001) D_A: 0.113 G_A: 0.258 cycle_A: 2.175 idt_A: 0.185 D_B: 0.134 G_B: 0.586 cycle_B: 0.547 idt_B: 1.068 \n",
      "End of epoch 6 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 7, iters: 72, time: 0.311, data: 0.002) D_A: 0.118 G_A: 0.628 cycle_A: 1.954 idt_A: 0.279 D_B: 0.231 G_B: 1.363 cycle_B: 0.682 idt_B: 0.868 \n",
      "(epoch: 7, iters: 172, time: 0.310, data: 0.001) D_A: 0.158 G_A: 0.315 cycle_A: 1.487 idt_A: 0.197 D_B: 0.142 G_B: 0.499 cycle_B: 0.521 idt_B: 0.733 \n",
      "(epoch: 7, iters: 272, time: 0.835, data: 0.001) D_A: 0.089 G_A: 1.022 cycle_A: 1.867 idt_A: 0.629 D_B: 0.252 G_B: 0.598 cycle_B: 1.180 idt_B: 0.869 \n",
      "End of epoch 7 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 8, iters: 84, time: 0.298, data: 0.002) D_A: 0.246 G_A: 0.278 cycle_A: 1.695 idt_A: 0.191 D_B: 0.098 G_B: 0.834 cycle_B: 0.533 idt_B: 0.878 \n",
      "(epoch: 8, iters: 184, time: 0.321, data: 0.001) D_A: 0.239 G_A: 0.185 cycle_A: 2.844 idt_A: 0.194 D_B: 0.068 G_B: 0.172 cycle_B: 0.567 idt_B: 1.370 \n",
      "(epoch: 8, iters: 284, time: 0.302, data: 0.001) D_A: 0.155 G_A: 0.852 cycle_A: 1.348 idt_A: 0.209 D_B: 0.071 G_B: 0.420 cycle_B: 0.621 idt_B: 0.658 \n",
      "End of epoch 8 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 9, iters: 96, time: 0.857, data: 0.002) D_A: 0.379 G_A: 0.500 cycle_A: 2.307 idt_A: 0.218 D_B: 0.214 G_B: 0.584 cycle_B: 0.538 idt_B: 1.147 \n",
      "(epoch: 9, iters: 196, time: 0.311, data: 0.001) D_A: 0.219 G_A: 0.399 cycle_A: 2.590 idt_A: 0.223 D_B: 0.062 G_B: 0.344 cycle_B: 0.402 idt_B: 1.263 \n",
      "End of epoch 9 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 10, iters: 8, time: 0.322, data: 0.002) D_A: 0.220 G_A: 0.329 cycle_A: 1.860 idt_A: 0.234 D_B: 0.056 G_B: 0.512 cycle_B: 0.584 idt_B: 0.736 \n",
      "(epoch: 10, iters: 108, time: 0.296, data: 0.002) D_A: 0.223 G_A: 0.609 cycle_A: 1.628 idt_A: 0.178 D_B: 0.096 G_B: 0.106 cycle_B: 0.483 idt_B: 0.797 \n",
      "(epoch: 10, iters: 208, time: 0.833, data: 0.002) D_A: 0.191 G_A: 0.372 cycle_A: 1.808 idt_A: 0.214 D_B: 0.062 G_B: 0.139 cycle_B: 0.522 idt_B: 0.571 \n",
      "End of epoch 10 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 11, iters: 20, time: 0.307, data: 0.001) D_A: 0.204 G_A: 0.266 cycle_A: 0.767 idt_A: 0.155 D_B: 0.423 G_B: 0.690 cycle_B: 0.457 idt_B: 0.307 \n",
      "(epoch: 11, iters: 120, time: 0.280, data: 0.001) D_A: 0.238 G_A: 0.156 cycle_A: 2.719 idt_A: 0.208 D_B: 0.058 G_B: 0.665 cycle_B: 0.517 idt_B: 1.481 \n",
      "(epoch: 11, iters: 220, time: 0.298, data: 0.002) D_A: 0.075 G_A: 0.521 cycle_A: 1.001 idt_A: 0.192 D_B: 0.042 G_B: 0.693 cycle_B: 0.425 idt_B: 0.434 \n",
      "End of epoch 11 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 12, iters: 32, time: 0.825, data: 0.002) D_A: 0.218 G_A: 0.185 cycle_A: 1.714 idt_A: 0.163 D_B: 0.170 G_B: 0.366 cycle_B: 0.611 idt_B: 0.949 \n",
      "(epoch: 12, iters: 132, time: 0.300, data: 0.002) D_A: 0.109 G_A: 0.353 cycle_A: 1.165 idt_A: 0.206 D_B: 0.224 G_B: 0.304 cycle_B: 0.642 idt_B: 0.448 \n",
      "(epoch: 12, iters: 232, time: 0.309, data: 0.001) D_A: 0.185 G_A: 0.413 cycle_A: 1.121 idt_A: 0.212 D_B: 0.209 G_B: 0.442 cycle_B: 0.491 idt_B: 0.449 \n",
      "End of epoch 12 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 13, iters: 44, time: 0.298, data: 0.001) D_A: 0.145 G_A: 0.397 cycle_A: 2.102 idt_A: 0.187 D_B: 0.172 G_B: 0.308 cycle_B: 0.478 idt_B: 0.780 \n",
      "(epoch: 13, iters: 144, time: 0.895, data: 0.002) D_A: 0.199 G_A: 0.401 cycle_A: 0.706 idt_A: 0.231 D_B: 0.203 G_B: 0.532 cycle_B: 0.519 idt_B: 0.286 \n",
      "(epoch: 13, iters: 244, time: 0.282, data: 0.001) D_A: 0.265 G_A: 0.585 cycle_A: 0.887 idt_A: 0.215 D_B: 0.077 G_B: 0.413 cycle_B: 0.517 idt_B: 0.324 \n",
      "End of epoch 13 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 14, iters: 56, time: 0.284, data: 0.002) D_A: 0.110 G_A: 0.162 cycle_A: 2.524 idt_A: 0.196 D_B: 0.061 G_B: 0.433 cycle_B: 0.673 idt_B: 1.373 \n",
      "(epoch: 14, iters: 156, time: 0.320, data: 0.001) D_A: 0.145 G_A: 0.616 cycle_A: 1.330 idt_A: 0.141 D_B: 0.293 G_B: 0.314 cycle_B: 0.495 idt_B: 0.448 \n",
      "(epoch: 14, iters: 256, time: 0.836, data: 0.001) D_A: 0.083 G_A: 0.453 cycle_A: 1.533 idt_A: 0.110 D_B: 0.134 G_B: 0.197 cycle_B: 0.399 idt_B: 0.629 \n",
      "End of epoch 14 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 15, iters: 68, time: 0.316, data: 0.001) D_A: 0.075 G_A: 0.191 cycle_A: 1.587 idt_A: 0.269 D_B: 0.206 G_B: 0.370 cycle_B: 0.643 idt_B: 0.334 \n",
      "(epoch: 15, iters: 168, time: 0.292, data: 0.001) D_A: 0.111 G_A: 0.485 cycle_A: 0.815 idt_A: 0.209 D_B: 0.267 G_B: 1.034 cycle_B: 0.530 idt_B: 0.307 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 15, iters: 268, time: 0.288, data: 0.001) D_A: 0.204 G_A: 0.513 cycle_A: 0.943 idt_A: 0.145 D_B: 0.211 G_B: 0.686 cycle_B: 0.453 idt_B: 0.365 \n",
      "End of epoch 15 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 16, iters: 80, time: 0.890, data: 0.002) D_A: 0.162 G_A: 0.951 cycle_A: 2.710 idt_A: 0.171 D_B: 0.363 G_B: 0.623 cycle_B: 0.421 idt_B: 0.938 \n",
      "(epoch: 16, iters: 180, time: 0.317, data: 0.002) D_A: 0.113 G_A: 0.472 cycle_A: 0.938 idt_A: 0.147 D_B: 0.153 G_B: 0.290 cycle_B: 0.472 idt_B: 0.317 \n",
      "(epoch: 16, iters: 280, time: 0.286, data: 0.001) D_A: 0.137 G_A: 0.585 cycle_A: 1.700 idt_A: 0.126 D_B: 0.247 G_B: 0.467 cycle_B: 0.365 idt_B: 0.469 \n",
      "End of epoch 16 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 17, iters: 92, time: 0.314, data: 0.001) D_A: 0.236 G_A: 0.733 cycle_A: 1.407 idt_A: 0.194 D_B: 0.405 G_B: 0.269 cycle_B: 0.467 idt_B: 0.530 \n",
      "(epoch: 17, iters: 192, time: 0.926, data: 0.001) D_A: 0.203 G_A: 0.293 cycle_A: 1.472 idt_A: 0.261 D_B: 0.089 G_B: 0.339 cycle_B: 0.641 idt_B: 0.387 \n",
      "End of epoch 17 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 18, iters: 4, time: 0.322, data: 0.002) D_A: 0.149 G_A: 0.381 cycle_A: 0.778 idt_A: 0.189 D_B: 0.099 G_B: 0.471 cycle_B: 0.501 idt_B: 0.245 \n",
      "(epoch: 18, iters: 104, time: 0.281, data: 0.002) D_A: 0.254 G_A: 0.187 cycle_A: 1.246 idt_A: 0.125 D_B: 0.202 G_B: 0.259 cycle_B: 0.402 idt_B: 0.421 \n",
      "saving the latest model (epoch 18, total_iters 5000)\n",
      "(epoch: 18, iters: 204, time: 0.289, data: 0.002) D_A: 0.231 G_A: 0.411 cycle_A: 1.347 idt_A: 0.239 D_B: 0.088 G_B: 0.545 cycle_B: 0.492 idt_B: 0.502 \n",
      "End of epoch 18 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 19, iters: 16, time: 0.867, data: 0.001) D_A: 0.123 G_A: 0.493 cycle_A: 2.292 idt_A: 0.121 D_B: 0.182 G_B: 0.460 cycle_B: 0.380 idt_B: 0.493 \n",
      "(epoch: 19, iters: 116, time: 0.309, data: 0.002) D_A: 0.141 G_A: 0.377 cycle_A: 4.195 idt_A: 0.173 D_B: 0.049 G_B: 0.233 cycle_B: 0.529 idt_B: 2.164 \n",
      "(epoch: 19, iters: 216, time: 0.304, data: 0.002) D_A: 0.180 G_A: 0.606 cycle_A: 1.340 idt_A: 0.156 D_B: 0.285 G_B: 0.159 cycle_B: 0.458 idt_B: 0.478 \n",
      "End of epoch 19 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 20, iters: 28, time: 0.314, data: 0.001) D_A: 0.227 G_A: 0.179 cycle_A: 0.819 idt_A: 0.159 D_B: 0.308 G_B: 0.793 cycle_B: 0.468 idt_B: 0.255 \n",
      "(epoch: 20, iters: 128, time: 0.953, data: 0.001) D_A: 0.297 G_A: 0.160 cycle_A: 0.956 idt_A: 0.285 D_B: 0.161 G_B: 0.342 cycle_B: 0.626 idt_B: 0.297 \n",
      "(epoch: 20, iters: 228, time: 0.323, data: 0.001) D_A: 0.199 G_A: 0.428 cycle_A: 0.790 idt_A: 0.224 D_B: 0.145 G_B: 0.435 cycle_B: 0.586 idt_B: 0.251 \n",
      "End of epoch 20 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 21, iters: 40, time: 0.308, data: 0.001) D_A: 0.159 G_A: 0.295 cycle_A: 0.849 idt_A: 0.187 D_B: 0.205 G_B: 0.352 cycle_B: 0.544 idt_B: 0.299 \n",
      "(epoch: 21, iters: 140, time: 0.319, data: 0.002) D_A: 0.085 G_A: 0.213 cycle_A: 0.664 idt_A: 0.159 D_B: 0.109 G_B: 0.134 cycle_B: 0.359 idt_B: 0.296 \n",
      "(epoch: 21, iters: 240, time: 0.953, data: 0.002) D_A: 0.275 G_A: 0.128 cycle_A: 1.333 idt_A: 0.201 D_B: 0.219 G_B: 0.483 cycle_B: 0.484 idt_B: 0.523 \n",
      "End of epoch 21 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 22, iters: 52, time: 0.302, data: 0.002) D_A: 0.150 G_A: 0.514 cycle_A: 2.932 idt_A: 0.153 D_B: 0.327 G_B: 0.860 cycle_B: 0.407 idt_B: 0.835 \n",
      "(epoch: 22, iters: 152, time: 0.286, data: 0.001) D_A: 0.134 G_A: 0.388 cycle_A: 3.185 idt_A: 0.187 D_B: 0.303 G_B: 0.108 cycle_B: 0.520 idt_B: 2.156 \n",
      "(epoch: 22, iters: 252, time: 0.303, data: 0.001) D_A: 0.113 G_A: 0.480 cycle_A: 1.153 idt_A: 0.179 D_B: 0.344 G_B: 0.291 cycle_B: 0.464 idt_B: 0.426 \n",
      "End of epoch 22 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 23, iters: 64, time: 0.957, data: 0.002) D_A: 0.113 G_A: 0.413 cycle_A: 1.761 idt_A: 0.154 D_B: 0.121 G_B: 0.606 cycle_B: 0.500 idt_B: 0.375 \n",
      "(epoch: 23, iters: 164, time: 0.312, data: 0.002) D_A: 0.090 G_A: 0.553 cycle_A: 1.034 idt_A: 0.141 D_B: 0.158 G_B: 0.406 cycle_B: 0.390 idt_B: 0.365 \n",
      "(epoch: 23, iters: 264, time: 0.285, data: 0.002) D_A: 0.127 G_A: 0.363 cycle_A: 1.832 idt_A: 0.186 D_B: 0.317 G_B: 0.519 cycle_B: 0.661 idt_B: 0.985 \n",
      "End of epoch 23 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 24, iters: 76, time: 0.298, data: 0.002) D_A: 0.154 G_A: 0.744 cycle_A: 3.053 idt_A: 0.233 D_B: 0.261 G_B: 0.319 cycle_B: 0.639 idt_B: 0.444 \n",
      "(epoch: 24, iters: 176, time: 0.891, data: 0.002) D_A: 0.060 G_A: 0.603 cycle_A: 1.051 idt_A: 0.139 D_B: 0.369 G_B: 0.571 cycle_B: 0.318 idt_B: 0.337 \n",
      "(epoch: 24, iters: 276, time: 0.281, data: 0.002) D_A: 0.198 G_A: 0.519 cycle_A: 0.571 idt_A: 0.174 D_B: 0.192 G_B: 0.252 cycle_B: 0.457 idt_B: 0.213 \n",
      "End of epoch 24 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 25, iters: 88, time: 0.281, data: 0.001) D_A: 0.209 G_A: 0.921 cycle_A: 1.089 idt_A: 0.203 D_B: 0.139 G_B: 0.201 cycle_B: 0.521 idt_B: 0.368 \n",
      "(epoch: 25, iters: 188, time: 0.316, data: 0.001) D_A: 0.240 G_A: 0.195 cycle_A: 0.736 idt_A: 0.233 D_B: 0.384 G_B: 0.057 cycle_B: 0.637 idt_B: 0.287 \n",
      "(epoch: 25, iters: 288, time: 0.969, data: 0.001) D_A: 0.168 G_A: 0.286 cycle_A: 2.332 idt_A: 0.174 D_B: 0.084 G_B: 0.594 cycle_B: 0.506 idt_B: 0.365 \n",
      "End of epoch 25 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 26, iters: 100, time: 0.314, data: 0.289) D_A: 0.069 G_A: 0.672 cycle_A: 1.207 idt_A: 0.199 D_B: 0.211 G_B: 0.250 cycle_B: 0.470 idt_B: 0.420 \n",
      "(epoch: 26, iters: 200, time: 0.314, data: 0.001) D_A: 0.207 G_A: 0.215 cycle_A: 3.688 idt_A: 0.143 D_B: 0.118 G_B: 0.145 cycle_B: 0.489 idt_B: 1.778 \n",
      "End of epoch 26 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 27, iters: 12, time: 0.310, data: 0.001) D_A: 0.174 G_A: 0.354 cycle_A: 1.417 idt_A: 0.179 D_B: 0.094 G_B: 0.118 cycle_B: 0.546 idt_B: 0.473 \n",
      "(epoch: 27, iters: 112, time: 0.945, data: 0.002) D_A: 0.207 G_A: 0.222 cycle_A: 2.499 idt_A: 0.127 D_B: 0.147 G_B: 0.344 cycle_B: 0.388 idt_B: 1.059 \n",
      "(epoch: 27, iters: 212, time: 0.286, data: 0.001) D_A: 0.256 G_A: 0.316 cycle_A: 1.400 idt_A: 0.137 D_B: 0.125 G_B: 0.490 cycle_B: 0.421 idt_B: 0.459 \n",
      "End of epoch 27 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 28, iters: 24, time: 0.311, data: 0.001) D_A: 0.139 G_A: 0.400 cycle_A: 0.914 idt_A: 0.156 D_B: 0.075 G_B: 0.109 cycle_B: 0.414 idt_B: 0.311 \n",
      "(epoch: 28, iters: 124, time: 0.281, data: 0.001) D_A: 0.220 G_A: 0.207 cycle_A: 1.258 idt_A: 0.114 D_B: 0.281 G_B: 0.216 cycle_B: 0.318 idt_B: 1.039 \n",
      "(epoch: 28, iters: 224, time: 0.957, data: 0.001) D_A: 0.213 G_A: 0.394 cycle_A: 1.598 idt_A: 0.124 D_B: 0.209 G_B: 0.570 cycle_B: 0.420 idt_B: 0.541 \n",
      "End of epoch 28 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 29, iters: 36, time: 0.283, data: 0.001) D_A: 0.110 G_A: 0.400 cycle_A: 2.260 idt_A: 0.191 D_B: 0.056 G_B: 0.344 cycle_B: 0.550 idt_B: 1.741 \n",
      "(epoch: 29, iters: 136, time: 0.308, data: 0.002) D_A: 0.138 G_A: 0.706 cycle_A: 1.453 idt_A: 0.161 D_B: 0.209 G_B: 0.238 cycle_B: 0.472 idt_B: 0.483 \n",
      "(epoch: 29, iters: 236, time: 0.286, data: 0.001) D_A: 0.071 G_A: 0.192 cycle_A: 0.885 idt_A: 0.170 D_B: 0.060 G_B: 0.348 cycle_B: 0.465 idt_B: 0.292 \n",
      "End of epoch 29 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 30, iters: 48, time: 1.002, data: 0.001) D_A: 0.177 G_A: 0.229 cycle_A: 3.930 idt_A: 0.144 D_B: 0.052 G_B: 0.242 cycle_B: 0.415 idt_B: 1.694 \n",
      "(epoch: 30, iters: 148, time: 0.299, data: 0.002) D_A: 0.144 G_A: 0.596 cycle_A: 3.244 idt_A: 0.204 D_B: 0.077 G_B: 0.788 cycle_B: 0.630 idt_B: 0.757 \n",
      "(epoch: 30, iters: 248, time: 0.293, data: 0.001) D_A: 0.145 G_A: 0.323 cycle_A: 0.742 idt_A: 0.130 D_B: 0.224 G_B: 0.850 cycle_B: 0.376 idt_B: 0.267 \n",
      "End of epoch 30 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 31, iters: 60, time: 0.316, data: 0.001) D_A: 0.148 G_A: 0.221 cycle_A: 1.295 idt_A: 0.135 D_B: 0.166 G_B: 0.298 cycle_B: 0.368 idt_B: 0.376 \n",
      "(epoch: 31, iters: 160, time: 0.943, data: 0.001) D_A: 0.226 G_A: 0.371 cycle_A: 1.269 idt_A: 0.153 D_B: 0.193 G_B: 0.240 cycle_B: 0.326 idt_B: 0.340 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 31, iters: 260, time: 0.290, data: 0.001) D_A: 0.086 G_A: 0.540 cycle_A: 2.143 idt_A: 0.183 D_B: 0.124 G_B: 0.233 cycle_B: 0.509 idt_B: 1.419 \n",
      "End of epoch 31 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 32, iters: 72, time: 0.284, data: 0.001) D_A: 0.041 G_A: 0.225 cycle_A: 1.083 idt_A: 0.148 D_B: 0.142 G_B: 0.736 cycle_B: 0.452 idt_B: 0.468 \n",
      "(epoch: 32, iters: 172, time: 0.307, data: 0.001) D_A: 0.112 G_A: 0.612 cycle_A: 0.736 idt_A: 0.186 D_B: 0.235 G_B: 0.672 cycle_B: 0.531 idt_B: 0.252 \n",
      "(epoch: 32, iters: 272, time: 1.022, data: 0.001) D_A: 0.066 G_A: 0.665 cycle_A: 3.337 idt_A: 0.135 D_B: 0.065 G_B: 0.465 cycle_B: 0.443 idt_B: 2.189 \n",
      "End of epoch 32 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 33, iters: 84, time: 0.283, data: 0.001) D_A: 0.123 G_A: 0.195 cycle_A: 1.389 idt_A: 0.175 D_B: 0.310 G_B: 0.103 cycle_B: 0.537 idt_B: 0.449 \n",
      "(epoch: 33, iters: 184, time: 0.316, data: 0.001) D_A: 0.154 G_A: 0.946 cycle_A: 1.015 idt_A: 0.260 D_B: 0.193 G_B: 0.321 cycle_B: 0.534 idt_B: 0.384 \n",
      "(epoch: 33, iters: 284, time: 0.290, data: 0.001) D_A: 0.197 G_A: 0.460 cycle_A: 2.838 idt_A: 0.104 D_B: 0.110 G_B: 0.645 cycle_B: 0.304 idt_B: 0.279 \n",
      "End of epoch 33 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 34, iters: 96, time: 0.991, data: 0.001) D_A: 0.158 G_A: 0.396 cycle_A: 3.516 idt_A: 0.182 D_B: 0.146 G_B: 0.327 cycle_B: 0.479 idt_B: 1.147 \n",
      "(epoch: 34, iters: 196, time: 0.315, data: 0.002) D_A: 0.080 G_A: 0.484 cycle_A: 0.991 idt_A: 0.178 D_B: 0.222 G_B: 0.138 cycle_B: 0.349 idt_B: 0.276 \n",
      "End of epoch 34 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 35, iters: 8, time: 0.302, data: 0.002) D_A: 0.050 G_A: 0.389 cycle_A: 1.624 idt_A: 0.147 D_B: 0.123 G_B: 0.245 cycle_B: 0.409 idt_B: 0.435 \n",
      "(epoch: 35, iters: 108, time: 0.315, data: 0.002) D_A: 0.241 G_A: 0.105 cycle_A: 1.415 idt_A: 0.173 D_B: 0.214 G_B: 0.269 cycle_B: 0.481 idt_B: 0.439 \n",
      "(epoch: 35, iters: 208, time: 0.985, data: 0.001) D_A: 0.134 G_A: 0.377 cycle_A: 1.178 idt_A: 0.134 D_B: 0.168 G_B: 0.587 cycle_B: 0.431 idt_B: 0.445 \n",
      "saving the latest model (epoch 35, total_iters 10000)\n",
      "End of epoch 35 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 36, iters: 20, time: 0.311, data: 0.002) D_A: 0.129 G_A: 0.409 cycle_A: 0.799 idt_A: 0.121 D_B: 0.097 G_B: 0.232 cycle_B: 0.372 idt_B: 0.296 \n",
      "(epoch: 36, iters: 120, time: 0.303, data: 0.001) D_A: 0.086 G_A: 0.588 cycle_A: 1.813 idt_A: 0.142 D_B: 0.037 G_B: 0.448 cycle_B: 0.361 idt_B: 0.730 \n",
      "(epoch: 36, iters: 220, time: 0.298, data: 0.001) D_A: 0.044 G_A: 0.594 cycle_A: 0.936 idt_A: 0.115 D_B: 0.245 G_B: 0.213 cycle_B: 0.413 idt_B: 0.246 \n",
      "End of epoch 36 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 37, iters: 32, time: 1.021, data: 0.001) D_A: 0.172 G_A: 0.340 cycle_A: 1.162 idt_A: 0.160 D_B: 0.197 G_B: 0.294 cycle_B: 0.436 idt_B: 0.371 \n",
      "(epoch: 37, iters: 132, time: 0.279, data: 0.001) D_A: 0.087 G_A: 0.828 cycle_A: 1.395 idt_A: 0.174 D_B: 0.079 G_B: 0.611 cycle_B: 0.613 idt_B: 0.437 \n",
      "(epoch: 37, iters: 232, time: 0.290, data: 0.001) D_A: 0.094 G_A: 0.506 cycle_A: 0.863 idt_A: 0.184 D_B: 0.199 G_B: 0.181 cycle_B: 0.469 idt_B: 0.286 \n",
      "End of epoch 37 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 38, iters: 44, time: 0.295, data: 0.001) D_A: 0.085 G_A: 0.558 cycle_A: 0.848 idt_A: 0.275 D_B: 0.168 G_B: 0.371 cycle_B: 0.559 idt_B: 0.293 \n",
      "(epoch: 38, iters: 144, time: 1.027, data: 0.001) D_A: 0.077 G_A: 0.161 cycle_A: 1.063 idt_A: 0.236 D_B: 0.095 G_B: 0.409 cycle_B: 0.609 idt_B: 0.351 \n",
      "(epoch: 38, iters: 244, time: 0.316, data: 0.001) D_A: 0.136 G_A: 0.876 cycle_A: 1.505 idt_A: 0.159 D_B: 0.144 G_B: 0.322 cycle_B: 0.509 idt_B: 0.487 \n",
      "End of epoch 38 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0001000 -> 0.0001000\n",
      "(epoch: 39, iters: 56, time: 0.292, data: 0.002) D_A: 0.093 G_A: 0.913 cycle_A: 1.200 idt_A: 0.155 D_B: 0.162 G_B: 0.474 cycle_B: 0.397 idt_B: 0.426 \n",
      "(epoch: 39, iters: 156, time: 0.279, data: 0.002) D_A: 0.073 G_A: 0.404 cycle_A: 0.812 idt_A: 0.165 D_B: 0.161 G_B: 0.282 cycle_B: 0.529 idt_B: 0.247 \n",
      "(epoch: 39, iters: 256, time: 1.054, data: 0.001) D_A: 0.084 G_A: 0.309 cycle_A: 1.151 idt_A: 0.145 D_B: 0.124 G_B: 0.239 cycle_B: 0.467 idt_B: 0.400 \n",
      "End of epoch 39 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0001000 -> 0.0000100\n",
      "(epoch: 40, iters: 68, time: 0.314, data: 0.001) D_A: 0.031 G_A: 0.509 cycle_A: 0.777 idt_A: 0.091 D_B: 0.211 G_B: 0.616 cycle_B: 0.250 idt_B: 0.255 \n",
      "(epoch: 40, iters: 168, time: 0.294, data: 0.001) D_A: 0.156 G_A: 0.267 cycle_A: 0.784 idt_A: 0.101 D_B: 0.221 G_B: 0.458 cycle_B: 0.321 idt_B: 0.253 \n",
      "(epoch: 40, iters: 268, time: 0.314, data: 0.001) D_A: 0.056 G_A: 0.787 cycle_A: 1.259 idt_A: 0.123 D_B: 0.073 G_B: 0.647 cycle_B: 0.365 idt_B: 0.391 \n",
      "End of epoch 40 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 41, iters: 80, time: 1.067, data: 0.001) D_A: 0.080 G_A: 0.531 cycle_A: 1.224 idt_A: 0.108 D_B: 0.145 G_B: 0.385 cycle_B: 0.318 idt_B: 0.395 \n",
      "(epoch: 41, iters: 180, time: 0.316, data: 0.001) D_A: 0.073 G_A: 0.524 cycle_A: 2.937 idt_A: 0.098 D_B: 0.074 G_B: 0.455 cycle_B: 0.297 idt_B: 0.562 \n",
      "(epoch: 41, iters: 280, time: 0.305, data: 0.001) D_A: 0.106 G_A: 0.740 cycle_A: 1.199 idt_A: 0.114 D_B: 0.200 G_B: 0.388 cycle_B: 0.333 idt_B: 0.356 \n",
      "End of epoch 41 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 42, iters: 92, time: 0.300, data: 0.001) D_A: 0.124 G_A: 0.426 cycle_A: 0.836 idt_A: 0.121 D_B: 0.216 G_B: 0.291 cycle_B: 0.367 idt_B: 0.259 \n",
      "(epoch: 42, iters: 192, time: 1.075, data: 0.001) D_A: 0.074 G_A: 0.386 cycle_A: 1.129 idt_A: 0.141 D_B: 0.183 G_B: 0.288 cycle_B: 0.372 idt_B: 0.366 \n",
      "End of epoch 42 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 43, iters: 4, time: 0.305, data: 0.001) D_A: 0.070 G_A: 0.596 cycle_A: 0.894 idt_A: 0.107 D_B: 0.190 G_B: 0.345 cycle_B: 0.322 idt_B: 0.248 \n",
      "(epoch: 43, iters: 104, time: 0.286, data: 0.003) D_A: 0.104 G_A: 0.655 cycle_A: 1.183 idt_A: 0.125 D_B: 0.110 G_B: 0.381 cycle_B: 0.281 idt_B: 0.278 \n",
      "(epoch: 43, iters: 204, time: 0.316, data: 0.001) D_A: 0.090 G_A: 0.465 cycle_A: 0.761 idt_A: 0.104 D_B: 0.232 G_B: 0.446 cycle_B: 0.329 idt_B: 0.484 \n",
      "End of epoch 43 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 44, iters: 16, time: 1.098, data: 0.001) D_A: 0.083 G_A: 0.441 cycle_A: 1.390 idt_A: 0.114 D_B: 0.028 G_B: 0.444 cycle_B: 0.275 idt_B: 0.484 \n",
      "(epoch: 44, iters: 116, time: 0.289, data: 0.001) D_A: 0.092 G_A: 0.609 cycle_A: 0.878 idt_A: 0.099 D_B: 0.215 G_B: 0.384 cycle_B: 0.289 idt_B: 0.223 \n",
      "(epoch: 44, iters: 216, time: 0.318, data: 0.001) D_A: 0.104 G_A: 0.452 cycle_A: 1.130 idt_A: 0.109 D_B: 0.140 G_B: 0.145 cycle_B: 0.366 idt_B: 0.378 \n",
      "End of epoch 44 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 45, iters: 28, time: 0.288, data: 0.001) D_A: 0.135 G_A: 0.412 cycle_A: 1.132 idt_A: 0.105 D_B: 0.109 G_B: 0.406 cycle_B: 0.286 idt_B: 0.343 \n",
      "(epoch: 45, iters: 128, time: 1.086, data: 0.001) D_A: 0.083 G_A: 0.330 cycle_A: 1.484 idt_A: 0.107 D_B: 0.065 G_B: 0.475 cycle_B: 0.253 idt_B: 0.456 \n",
      "(epoch: 45, iters: 228, time: 0.283, data: 0.002) D_A: 0.149 G_A: 0.426 cycle_A: 0.701 idt_A: 0.124 D_B: 0.183 G_B: 0.330 cycle_B: 0.348 idt_B: 0.206 \n",
      "End of epoch 45 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 46, iters: 40, time: 0.308, data: 0.001) D_A: 0.116 G_A: 0.580 cycle_A: 0.876 idt_A: 0.099 D_B: 0.273 G_B: 0.329 cycle_B: 0.318 idt_B: 0.227 \n",
      "(epoch: 46, iters: 140, time: 0.315, data: 0.001) D_A: 0.104 G_A: 0.541 cycle_A: 2.725 idt_A: 0.150 D_B: 0.085 G_B: 0.223 cycle_B: 0.364 idt_B: 0.571 \n",
      "(epoch: 46, iters: 240, time: 1.092, data: 0.001) D_A: 0.085 G_A: 0.423 cycle_A: 1.116 idt_A: 0.124 D_B: 0.166 G_B: 0.437 cycle_B: 0.368 idt_B: 0.334 \n",
      "End of epoch 46 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 47, iters: 52, time: 0.306, data: 0.001) D_A: 0.097 G_A: 0.552 cycle_A: 0.889 idt_A: 0.090 D_B: 0.171 G_B: 0.496 cycle_B: 0.266 idt_B: 0.251 \n",
      "(epoch: 47, iters: 152, time: 0.309, data: 0.002) D_A: 0.144 G_A: 0.508 cycle_A: 1.116 idt_A: 0.099 D_B: 0.086 G_B: 0.458 cycle_B: 0.297 idt_B: 0.404 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 47, iters: 252, time: 0.280, data: 0.001) D_A: 0.133 G_A: 0.367 cycle_A: 0.762 idt_A: 0.123 D_B: 0.221 G_B: 0.340 cycle_B: 0.284 idt_B: 0.216 \n",
      "End of epoch 47 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 48, iters: 64, time: 1.108, data: 0.001) D_A: 0.079 G_A: 0.330 cycle_A: 0.777 idt_A: 0.104 D_B: 0.238 G_B: 0.331 cycle_B: 0.307 idt_B: 0.239 \n",
      "(epoch: 48, iters: 164, time: 0.292, data: 0.001) D_A: 0.091 G_A: 0.493 cycle_A: 1.241 idt_A: 0.111 D_B: 0.165 G_B: 0.270 cycle_B: 0.374 idt_B: 0.374 \n",
      "(epoch: 48, iters: 264, time: 0.307, data: 0.001) D_A: 0.099 G_A: 0.752 cycle_A: 0.975 idt_A: 0.140 D_B: 0.217 G_B: 0.371 cycle_B: 0.413 idt_B: 0.281 \n",
      "End of epoch 48 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 49, iters: 76, time: 0.317, data: 0.002) D_A: 0.083 G_A: 0.417 cycle_A: 1.405 idt_A: 0.119 D_B: 0.204 G_B: 0.408 cycle_B: 0.328 idt_B: 0.416 \n",
      "(epoch: 49, iters: 176, time: 1.111, data: 0.002) D_A: 0.193 G_A: 0.264 cycle_A: 1.038 idt_A: 0.142 D_B: 0.068 G_B: 0.204 cycle_B: 0.411 idt_B: 0.369 \n",
      "(epoch: 49, iters: 276, time: 0.280, data: 0.001) D_A: 0.117 G_A: 0.460 cycle_A: 1.403 idt_A: 0.105 D_B: 0.194 G_B: 0.349 cycle_B: 0.313 idt_B: 0.422 \n",
      "End of epoch 49 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 50, iters: 88, time: 0.297, data: 0.002) D_A: 0.142 G_A: 0.437 cycle_A: 0.823 idt_A: 0.092 D_B: 0.180 G_B: 0.488 cycle_B: 0.273 idt_B: 0.266 \n",
      "(epoch: 50, iters: 188, time: 0.299, data: 0.001) D_A: 0.091 G_A: 0.572 cycle_A: 0.986 idt_A: 0.083 D_B: 0.092 G_B: 0.505 cycle_B: 0.271 idt_B: 0.258 \n",
      "(epoch: 50, iters: 288, time: 1.117, data: 0.001) D_A: 0.072 G_A: 0.611 cycle_A: 1.233 idt_A: 0.114 D_B: 0.153 G_B: 0.318 cycle_B: 0.349 idt_B: 0.344 \n",
      "saving the model at the end of epoch 50, iters 14400\n",
      "End of epoch 50 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 51, iters: 100, time: 0.276, data: 0.282) D_A: 0.052 G_A: 0.466 cycle_A: 0.856 idt_A: 0.083 D_B: 0.206 G_B: 0.569 cycle_B: 0.284 idt_B: 0.305 \n",
      "(epoch: 51, iters: 200, time: 0.299, data: 0.002) D_A: 0.158 G_A: 0.585 cycle_A: 1.067 idt_A: 0.108 D_B: 0.116 G_B: 0.360 cycle_B: 0.315 idt_B: 0.243 \n",
      "End of epoch 51 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 52, iters: 12, time: 0.309, data: 0.001) D_A: 0.134 G_A: 0.428 cycle_A: 1.026 idt_A: 0.101 D_B: 0.099 G_B: 0.436 cycle_B: 0.310 idt_B: 0.311 \n",
      "(epoch: 52, iters: 112, time: 1.139, data: 0.002) D_A: 0.125 G_A: 0.535 cycle_A: 0.812 idt_A: 0.102 D_B: 0.208 G_B: 0.434 cycle_B: 0.362 idt_B: 0.230 \n",
      "(epoch: 52, iters: 212, time: 0.285, data: 0.001) D_A: 0.100 G_A: 0.419 cycle_A: 1.062 idt_A: 0.097 D_B: 0.048 G_B: 0.347 cycle_B: 0.326 idt_B: 0.492 \n",
      "End of epoch 52 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 53, iters: 24, time: 0.294, data: 0.001) D_A: 0.123 G_A: 0.567 cycle_A: 1.130 idt_A: 0.120 D_B: 0.109 G_B: 0.400 cycle_B: 0.365 idt_B: 0.273 \n",
      "saving the latest model (epoch 53, total_iters 15000)\n",
      "(epoch: 53, iters: 124, time: 0.300, data: 0.002) D_A: 0.169 G_A: 0.415 cycle_A: 0.821 idt_A: 0.101 D_B: 0.223 G_B: 0.471 cycle_B: 0.321 idt_B: 0.242 \n",
      "(epoch: 53, iters: 224, time: 1.131, data: 0.001) D_A: 0.147 G_A: 0.455 cycle_A: 0.898 idt_A: 0.084 D_B: 0.108 G_B: 0.543 cycle_B: 0.252 idt_B: 0.262 \n",
      "End of epoch 53 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 54, iters: 36, time: 0.315, data: 0.002) D_A: 0.144 G_A: 0.489 cycle_A: 1.050 idt_A: 0.104 D_B: 0.178 G_B: 0.360 cycle_B: 0.256 idt_B: 0.309 \n",
      "(epoch: 54, iters: 136, time: 0.306, data: 0.001) D_A: 0.150 G_A: 0.539 cycle_A: 1.209 idt_A: 0.110 D_B: 0.216 G_B: 0.390 cycle_B: 0.270 idt_B: 0.355 \n",
      "(epoch: 54, iters: 236, time: 0.278, data: 0.001) D_A: 0.149 G_A: 0.360 cycle_A: 0.924 idt_A: 0.159 D_B: 0.135 G_B: 0.452 cycle_B: 0.407 idt_B: 0.270 \n",
      "End of epoch 54 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 55, iters: 48, time: 1.129, data: 0.001) D_A: 0.249 G_A: 0.548 cycle_A: 1.198 idt_A: 0.106 D_B: 0.204 G_B: 0.444 cycle_B: 0.346 idt_B: 0.350 \n",
      "(epoch: 55, iters: 148, time: 0.307, data: 0.001) D_A: 0.080 G_A: 0.607 cycle_A: 1.148 idt_A: 0.092 D_B: 0.149 G_B: 0.437 cycle_B: 0.241 idt_B: 0.333 \n",
      "(epoch: 55, iters: 248, time: 0.288, data: 0.002) D_A: 0.158 G_A: 0.430 cycle_A: 1.433 idt_A: 0.116 D_B: 0.127 G_B: 0.269 cycle_B: 0.356 idt_B: 0.362 \n",
      "End of epoch 55 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 56, iters: 60, time: 0.291, data: 0.001) D_A: 0.120 G_A: 0.631 cycle_A: 1.110 idt_A: 0.120 D_B: 0.159 G_B: 0.434 cycle_B: 0.310 idt_B: 0.319 \n",
      "(epoch: 56, iters: 160, time: 1.128, data: 0.001) D_A: 0.133 G_A: 0.454 cycle_A: 1.019 idt_A: 0.092 D_B: 0.230 G_B: 0.391 cycle_B: 0.293 idt_B: 0.290 \n",
      "(epoch: 56, iters: 260, time: 0.315, data: 0.001) D_A: 0.117 G_A: 0.505 cycle_A: 1.467 idt_A: 0.089 D_B: 0.062 G_B: 0.247 cycle_B: 0.303 idt_B: 0.396 \n",
      "End of epoch 56 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 57, iters: 72, time: 0.285, data: 0.001) D_A: 0.127 G_A: 0.411 cycle_A: 0.819 idt_A: 0.088 D_B: 0.205 G_B: 0.423 cycle_B: 0.259 idt_B: 0.241 \n",
      "(epoch: 57, iters: 172, time: 0.284, data: 0.001) D_A: 0.096 G_A: 0.272 cycle_A: 0.841 idt_A: 0.085 D_B: 0.262 G_B: 0.389 cycle_B: 0.229 idt_B: 0.213 \n",
      "(epoch: 57, iters: 272, time: 1.123, data: 0.001) D_A: 0.109 G_A: 0.601 cycle_A: 1.183 idt_A: 0.068 D_B: 0.190 G_B: 0.380 cycle_B: 0.211 idt_B: 0.346 \n",
      "End of epoch 57 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 58, iters: 84, time: 0.311, data: 0.001) D_A: 0.165 G_A: 0.459 cycle_A: 1.493 idt_A: 0.137 D_B: 0.155 G_B: 0.280 cycle_B: 0.272 idt_B: 0.423 \n",
      "(epoch: 58, iters: 184, time: 0.308, data: 0.001) D_A: 0.156 G_A: 0.423 cycle_A: 1.022 idt_A: 0.096 D_B: 0.226 G_B: 0.425 cycle_B: 0.312 idt_B: 0.321 \n",
      "(epoch: 58, iters: 284, time: 0.287, data: 0.001) D_A: 0.181 G_A: 0.306 cycle_A: 0.676 idt_A: 0.110 D_B: 0.275 G_B: 0.304 cycle_B: 0.311 idt_B: 0.189 \n",
      "End of epoch 58 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 59, iters: 96, time: 1.136, data: 0.001) D_A: 0.158 G_A: 0.365 cycle_A: 0.824 idt_A: 0.124 D_B: 0.214 G_B: 0.368 cycle_B: 0.311 idt_B: 0.241 \n",
      "(epoch: 59, iters: 196, time: 0.289, data: 0.001) D_A: 0.122 G_A: 0.405 cycle_A: 0.861 idt_A: 0.154 D_B: 0.210 G_B: 0.435 cycle_B: 0.446 idt_B: 0.246 \n",
      "End of epoch 59 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 60, iters: 8, time: 0.317, data: 0.002) D_A: 0.115 G_A: 0.497 cycle_A: 1.351 idt_A: 0.089 D_B: 0.156 G_B: 0.374 cycle_B: 0.282 idt_B: 0.354 \n",
      "(epoch: 60, iters: 108, time: 0.281, data: 0.002) D_A: 0.188 G_A: 0.362 cycle_A: 0.797 idt_A: 0.117 D_B: 0.237 G_B: 0.363 cycle_B: 0.278 idt_B: 0.239 \n",
      "(epoch: 60, iters: 208, time: 1.122, data: 0.002) D_A: 0.157 G_A: 0.379 cycle_A: 0.907 idt_A: 0.089 D_B: 0.199 G_B: 0.276 cycle_B: 0.250 idt_B: 0.245 \n",
      "End of epoch 60 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 61, iters: 20, time: 0.312, data: 0.001) D_A: 0.121 G_A: 0.552 cycle_A: 0.906 idt_A: 0.110 D_B: 0.158 G_B: 0.533 cycle_B: 0.329 idt_B: 0.283 \n",
      "(epoch: 61, iters: 120, time: 0.310, data: 0.001) D_A: 0.139 G_A: 0.458 cycle_A: 0.943 idt_A: 0.088 D_B: 0.230 G_B: 0.342 cycle_B: 0.261 idt_B: 0.239 \n",
      "(epoch: 61, iters: 220, time: 0.291, data: 0.001) D_A: 0.137 G_A: 0.552 cycle_A: 1.235 idt_A: 0.092 D_B: 0.162 G_B: 0.505 cycle_B: 0.306 idt_B: 0.358 \n",
      "End of epoch 61 / 300 \t Time Taken: 80 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 62, iters: 32, time: 1.209, data: 0.001) D_A: 0.138 G_A: 0.584 cycle_A: 1.304 idt_A: 0.098 D_B: 0.151 G_B: 0.418 cycle_B: 0.343 idt_B: 0.386 \n",
      "(epoch: 62, iters: 132, time: 0.280, data: 0.001) D_A: 0.133 G_A: 0.468 cycle_A: 0.990 idt_A: 0.116 D_B: 0.186 G_B: 0.360 cycle_B: 0.371 idt_B: 0.295 \n",
      "(epoch: 62, iters: 232, time: 0.278, data: 0.001) D_A: 0.150 G_A: 0.665 cycle_A: 1.403 idt_A: 0.087 D_B: 0.168 G_B: 0.296 cycle_B: 0.289 idt_B: 0.400 \n",
      "End of epoch 62 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 63, iters: 44, time: 0.288, data: 0.001) D_A: 0.164 G_A: 0.433 cycle_A: 1.225 idt_A: 0.099 D_B: 0.172 G_B: 0.334 cycle_B: 0.303 idt_B: 0.351 \n",
      "(epoch: 63, iters: 144, time: 1.190, data: 0.001) D_A: 0.171 G_A: 0.452 cycle_A: 1.367 idt_A: 0.097 D_B: 0.272 G_B: 0.314 cycle_B: 0.248 idt_B: 0.394 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 63, iters: 244, time: 0.316, data: 0.001) D_A: 0.192 G_A: 0.450 cycle_A: 0.753 idt_A: 0.117 D_B: 0.136 G_B: 0.284 cycle_B: 0.351 idt_B: 0.197 \n",
      "End of epoch 63 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 64, iters: 56, time: 0.312, data: 0.001) D_A: 0.125 G_A: 0.407 cycle_A: 0.931 idt_A: 0.094 D_B: 0.146 G_B: 0.253 cycle_B: 0.302 idt_B: 0.266 \n",
      "(epoch: 64, iters: 156, time: 0.283, data: 0.002) D_A: 0.147 G_A: 0.443 cycle_A: 1.088 idt_A: 0.092 D_B: 0.152 G_B: 0.290 cycle_B: 0.335 idt_B: 0.305 \n",
      "(epoch: 64, iters: 256, time: 1.243, data: 0.001) D_A: 0.135 G_A: 0.555 cycle_A: 1.137 idt_A: 0.156 D_B: 0.207 G_B: 0.339 cycle_B: 0.352 idt_B: 0.335 \n",
      "End of epoch 64 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 65, iters: 68, time: 0.301, data: 0.001) D_A: 0.158 G_A: 0.519 cycle_A: 1.184 idt_A: 0.121 D_B: 0.200 G_B: 0.391 cycle_B: 0.358 idt_B: 0.278 \n",
      "(epoch: 65, iters: 168, time: 0.314, data: 0.001) D_A: 0.105 G_A: 0.507 cycle_A: 1.228 idt_A: 0.083 D_B: 0.141 G_B: 0.407 cycle_B: 0.237 idt_B: 0.326 \n",
      "(epoch: 65, iters: 268, time: 0.283, data: 0.001) D_A: 0.139 G_A: 0.402 cycle_A: 1.150 idt_A: 0.119 D_B: 0.229 G_B: 0.398 cycle_B: 0.286 idt_B: 0.328 \n",
      "End of epoch 65 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 66, iters: 80, time: 1.252, data: 0.001) D_A: 0.120 G_A: 0.452 cycle_A: 0.676 idt_A: 0.084 D_B: 0.239 G_B: 0.465 cycle_B: 0.240 idt_B: 0.223 \n",
      "(epoch: 66, iters: 180, time: 0.313, data: 0.001) D_A: 0.160 G_A: 0.370 cycle_A: 0.803 idt_A: 0.116 D_B: 0.209 G_B: 0.360 cycle_B: 0.380 idt_B: 0.239 \n",
      "(epoch: 66, iters: 280, time: 0.313, data: 0.002) D_A: 0.121 G_A: 0.452 cycle_A: 0.625 idt_A: 0.095 D_B: 0.104 G_B: 0.351 cycle_B: 0.286 idt_B: 0.156 \n",
      "End of epoch 66 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 67, iters: 92, time: 0.294, data: 0.002) D_A: 0.176 G_A: 0.512 cycle_A: 0.955 idt_A: 0.102 D_B: 0.119 G_B: 0.324 cycle_B: 0.292 idt_B: 0.250 \n",
      "(epoch: 67, iters: 192, time: 1.191, data: 0.001) D_A: 0.154 G_A: 0.427 cycle_A: 1.409 idt_A: 0.087 D_B: 0.191 G_B: 0.255 cycle_B: 0.246 idt_B: 0.395 \n",
      "End of epoch 67 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 68, iters: 4, time: 0.294, data: 0.001) D_A: 0.156 G_A: 0.456 cycle_A: 0.750 idt_A: 0.101 D_B: 0.337 G_B: 0.257 cycle_B: 0.266 idt_B: 0.209 \n",
      "(epoch: 68, iters: 104, time: 0.290, data: 0.004) D_A: 0.109 G_A: 0.511 cycle_A: 1.252 idt_A: 0.096 D_B: 0.140 G_B: 0.243 cycle_B: 0.291 idt_B: 0.324 \n",
      "(epoch: 68, iters: 204, time: 0.299, data: 0.001) D_A: 0.117 G_A: 0.366 cycle_A: 1.192 idt_A: 0.122 D_B: 0.144 G_B: 0.247 cycle_B: 0.344 idt_B: 0.344 \n",
      "End of epoch 68 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 69, iters: 16, time: 1.219, data: 0.002) D_A: 0.140 G_A: 0.438 cycle_A: 1.000 idt_A: 0.078 D_B: 0.186 G_B: 0.354 cycle_B: 0.200 idt_B: 0.296 \n",
      "(epoch: 69, iters: 116, time: 0.315, data: 0.001) D_A: 0.131 G_A: 0.340 cycle_A: 1.199 idt_A: 0.078 D_B: 0.106 G_B: 0.417 cycle_B: 0.204 idt_B: 0.348 \n",
      "(epoch: 69, iters: 216, time: 0.295, data: 0.001) D_A: 0.166 G_A: 0.458 cycle_A: 1.113 idt_A: 0.095 D_B: 0.214 G_B: 0.326 cycle_B: 0.250 idt_B: 0.315 \n",
      "End of epoch 69 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 70, iters: 28, time: 0.291, data: 0.001) D_A: 0.114 G_A: 0.640 cycle_A: 1.024 idt_A: 0.066 D_B: 0.126 G_B: 0.455 cycle_B: 0.198 idt_B: 0.291 \n",
      "(epoch: 70, iters: 128, time: 1.202, data: 0.001) D_A: 0.108 G_A: 0.516 cycle_A: 1.423 idt_A: 0.090 D_B: 0.136 G_B: 0.473 cycle_B: 0.237 idt_B: 0.402 \n",
      "saving the latest model (epoch 70, total_iters 20000)\n",
      "(epoch: 70, iters: 228, time: 0.313, data: 0.002) D_A: 0.177 G_A: 0.349 cycle_A: 1.198 idt_A: 0.107 D_B: 0.210 G_B: 0.351 cycle_B: 0.309 idt_B: 0.343 \n",
      "End of epoch 70 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 71, iters: 40, time: 0.296, data: 0.002) D_A: 0.143 G_A: 0.412 cycle_A: 1.224 idt_A: 0.074 D_B: 0.175 G_B: 0.415 cycle_B: 0.202 idt_B: 0.344 \n",
      "(epoch: 71, iters: 140, time: 0.316, data: 0.002) D_A: 0.124 G_A: 0.459 cycle_A: 0.930 idt_A: 0.090 D_B: 0.134 G_B: 0.291 cycle_B: 0.274 idt_B: 0.267 \n",
      "(epoch: 71, iters: 240, time: 1.249, data: 0.001) D_A: 0.185 G_A: 0.368 cycle_A: 0.810 idt_A: 0.092 D_B: 0.215 G_B: 0.468 cycle_B: 0.265 idt_B: 0.228 \n",
      "End of epoch 71 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 72, iters: 52, time: 0.295, data: 0.002) D_A: 0.184 G_A: 0.457 cycle_A: 1.111 idt_A: 0.071 D_B: 0.254 G_B: 0.459 cycle_B: 0.221 idt_B: 0.313 \n",
      "(epoch: 72, iters: 152, time: 0.290, data: 0.002) D_A: 0.117 G_A: 0.393 cycle_A: 1.038 idt_A: 0.084 D_B: 0.213 G_B: 0.412 cycle_B: 0.252 idt_B: 0.295 \n",
      "(epoch: 72, iters: 252, time: 0.293, data: 0.001) D_A: 0.148 G_A: 0.521 cycle_A: 0.815 idt_A: 0.097 D_B: 0.378 G_B: 0.442 cycle_B: 0.304 idt_B: 0.229 \n",
      "End of epoch 72 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 73, iters: 64, time: 1.302, data: 0.001) D_A: 0.217 G_A: 0.313 cycle_A: 0.807 idt_A: 0.112 D_B: 0.176 G_B: 0.459 cycle_B: 0.331 idt_B: 0.238 \n",
      "(epoch: 73, iters: 164, time: 0.315, data: 0.002) D_A: 0.143 G_A: 0.569 cycle_A: 1.430 idt_A: 0.084 D_B: 0.089 G_B: 0.363 cycle_B: 0.282 idt_B: 0.393 \n",
      "(epoch: 73, iters: 264, time: 0.286, data: 0.001) D_A: 0.133 G_A: 0.514 cycle_A: 1.271 idt_A: 0.102 D_B: 0.276 G_B: 0.329 cycle_B: 0.325 idt_B: 0.361 \n",
      "End of epoch 73 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 74, iters: 76, time: 0.313, data: 0.001) D_A: 0.163 G_A: 0.379 cycle_A: 0.583 idt_A: 0.095 D_B: 0.265 G_B: 0.625 cycle_B: 0.320 idt_B: 0.190 \n",
      "(epoch: 74, iters: 176, time: 1.270, data: 0.002) D_A: 0.126 G_A: 0.504 cycle_A: 0.948 idt_A: 0.123 D_B: 0.119 G_B: 0.347 cycle_B: 0.344 idt_B: 0.277 \n",
      "(epoch: 74, iters: 276, time: 0.282, data: 0.001) D_A: 0.188 G_A: 0.352 cycle_A: 0.798 idt_A: 0.072 D_B: 0.197 G_B: 0.453 cycle_B: 0.217 idt_B: 0.198 \n",
      "End of epoch 74 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 75, iters: 88, time: 0.288, data: 0.001) D_A: 0.118 G_A: 0.525 cycle_A: 0.976 idt_A: 0.107 D_B: 0.293 G_B: 0.248 cycle_B: 0.351 idt_B: 0.277 \n",
      "(epoch: 75, iters: 188, time: 0.298, data: 0.001) D_A: 0.143 G_A: 0.436 cycle_A: 1.168 idt_A: 0.065 D_B: 0.258 G_B: 0.335 cycle_B: 0.192 idt_B: 0.324 \n",
      "(epoch: 75, iters: 288, time: 1.257, data: 0.001) D_A: 0.109 G_A: 0.460 cycle_A: 0.948 idt_A: 0.084 D_B: 0.170 G_B: 0.274 cycle_B: 0.241 idt_B: 0.246 \n",
      "End of epoch 75 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 76, iters: 100, time: 0.318, data: 0.276) D_A: 0.133 G_A: 0.518 cycle_A: 0.788 idt_A: 0.092 D_B: 0.290 G_B: 0.470 cycle_B: 0.243 idt_B: 0.222 \n",
      "(epoch: 76, iters: 200, time: 0.310, data: 0.001) D_A: 0.131 G_A: 0.386 cycle_A: 1.045 idt_A: 0.086 D_B: 0.198 G_B: 0.508 cycle_B: 0.238 idt_B: 0.291 \n",
      "End of epoch 76 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 77, iters: 12, time: 0.287, data: 0.001) D_A: 0.128 G_A: 0.498 cycle_A: 1.081 idt_A: 0.080 D_B: 0.151 G_B: 0.396 cycle_B: 0.199 idt_B: 0.310 \n",
      "(epoch: 77, iters: 112, time: 1.234, data: 0.001) D_A: 0.164 G_A: 0.513 cycle_A: 1.003 idt_A: 0.090 D_B: 0.233 G_B: 0.368 cycle_B: 0.251 idt_B: 0.275 \n",
      "(epoch: 77, iters: 212, time: 0.286, data: 0.002) D_A: 0.143 G_A: 0.480 cycle_A: 0.610 idt_A: 0.112 D_B: 0.107 G_B: 0.360 cycle_B: 0.280 idt_B: 0.176 \n",
      "End of epoch 77 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 78, iters: 24, time: 0.304, data: 0.001) D_A: 0.141 G_A: 0.543 cycle_A: 1.086 idt_A: 0.108 D_B: 0.100 G_B: 0.385 cycle_B: 0.327 idt_B: 0.257 \n",
      "(epoch: 78, iters: 124, time: 0.310, data: 0.002) D_A: 0.119 G_A: 0.452 cycle_A: 0.876 idt_A: 0.068 D_B: 0.102 G_B: 0.389 cycle_B: 0.189 idt_B: 0.249 \n",
      "(epoch: 78, iters: 224, time: 1.287, data: 0.002) D_A: 0.154 G_A: 0.493 cycle_A: 1.144 idt_A: 0.124 D_B: 0.187 G_B: 0.310 cycle_B: 0.333 idt_B: 0.328 \n",
      "End of epoch 78 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000100 -> 0.0000100\n",
      "(epoch: 79, iters: 36, time: 0.293, data: 0.001) D_A: 0.261 G_A: 0.166 cycle_A: 0.851 idt_A: 0.092 D_B: 0.347 G_B: 0.516 cycle_B: 0.232 idt_B: 0.196 \n",
      "(epoch: 79, iters: 136, time: 0.313, data: 0.001) D_A: 0.136 G_A: 0.595 cycle_A: 1.127 idt_A: 0.086 D_B: 0.236 G_B: 0.465 cycle_B: 0.251 idt_B: 0.323 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 79, iters: 236, time: 0.316, data: 0.001) D_A: 0.089 G_A: 0.637 cycle_A: 1.503 idt_A: 0.096 D_B: 0.113 G_B: 0.381 cycle_B: 0.299 idt_B: 0.403 \n",
      "End of epoch 79 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000100 -> 0.0000010\n",
      "(epoch: 80, iters: 48, time: 1.303, data: 0.001) D_A: 0.120 G_A: 0.509 cycle_A: 1.032 idt_A: 0.122 D_B: 0.151 G_B: 0.383 cycle_B: 0.327 idt_B: 0.292 \n",
      "(epoch: 80, iters: 148, time: 0.289, data: 0.002) D_A: 0.148 G_A: 0.522 cycle_A: 0.671 idt_A: 0.109 D_B: 0.105 G_B: 0.340 cycle_B: 0.318 idt_B: 0.157 \n",
      "(epoch: 80, iters: 248, time: 0.276, data: 0.001) D_A: 0.113 G_A: 0.443 cycle_A: 0.744 idt_A: 0.139 D_B: 0.167 G_B: 0.360 cycle_B: 0.299 idt_B: 0.224 \n",
      "End of epoch 80 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 81, iters: 60, time: 0.286, data: 0.002) D_A: 0.112 G_A: 0.567 cycle_A: 1.062 idt_A: 0.082 D_B: 0.200 G_B: 0.445 cycle_B: 0.244 idt_B: 0.300 \n",
      "(epoch: 81, iters: 160, time: 1.365, data: 0.001) D_A: 0.135 G_A: 0.429 cycle_A: 0.666 idt_A: 0.128 D_B: 0.267 G_B: 0.371 cycle_B: 0.300 idt_B: 0.191 \n",
      "(epoch: 81, iters: 260, time: 0.293, data: 0.001) D_A: 0.113 G_A: 0.454 cycle_A: 1.103 idt_A: 0.083 D_B: 0.082 G_B: 0.427 cycle_B: 0.245 idt_B: 0.307 \n",
      "End of epoch 81 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 82, iters: 72, time: 0.306, data: 0.001) D_A: 0.140 G_A: 0.468 cycle_A: 1.066 idt_A: 0.092 D_B: 0.166 G_B: 0.304 cycle_B: 0.272 idt_B: 0.268 \n",
      "(epoch: 82, iters: 172, time: 0.291, data: 0.001) D_A: 0.121 G_A: 0.526 cycle_A: 1.342 idt_A: 0.091 D_B: 0.176 G_B: 0.296 cycle_B: 0.273 idt_B: 0.355 \n",
      "(epoch: 82, iters: 272, time: 1.338, data: 0.002) D_A: 0.146 G_A: 0.436 cycle_A: 0.649 idt_A: 0.082 D_B: 0.249 G_B: 0.375 cycle_B: 0.249 idt_B: 0.185 \n",
      "End of epoch 82 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 83, iters: 84, time: 0.313, data: 0.002) D_A: 0.183 G_A: 0.549 cycle_A: 1.418 idt_A: 0.099 D_B: 0.219 G_B: 0.288 cycle_B: 0.294 idt_B: 0.376 \n",
      "(epoch: 83, iters: 184, time: 0.310, data: 0.001) D_A: 0.156 G_A: 0.511 cycle_A: 1.257 idt_A: 0.072 D_B: 0.209 G_B: 0.436 cycle_B: 0.220 idt_B: 0.315 \n",
      "(epoch: 83, iters: 284, time: 0.295, data: 0.001) D_A: 0.121 G_A: 0.408 cycle_A: 0.753 idt_A: 0.062 D_B: 0.229 G_B: 0.437 cycle_B: 0.180 idt_B: 0.202 \n",
      "End of epoch 83 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 84, iters: 96, time: 1.290, data: 0.001) D_A: 0.153 G_A: 0.456 cycle_A: 1.182 idt_A: 0.082 D_B: 0.191 G_B: 0.401 cycle_B: 0.196 idt_B: 0.331 \n",
      "(epoch: 84, iters: 196, time: 0.280, data: 0.001) D_A: 0.134 G_A: 0.371 cycle_A: 0.649 idt_A: 0.087 D_B: 0.217 G_B: 0.322 cycle_B: 0.276 idt_B: 0.184 \n",
      "End of epoch 84 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 85, iters: 8, time: 0.317, data: 0.001) D_A: 0.148 G_A: 0.437 cycle_A: 0.772 idt_A: 0.074 D_B: 0.255 G_B: 0.331 cycle_B: 0.258 idt_B: 0.211 \n",
      "(epoch: 85, iters: 108, time: 0.316, data: 0.001) D_A: 0.134 G_A: 0.434 cycle_A: 1.071 idt_A: 0.117 D_B: 0.148 G_B: 0.418 cycle_B: 0.318 idt_B: 0.303 \n",
      "(epoch: 85, iters: 208, time: 1.303, data: 0.001) D_A: 0.152 G_A: 0.509 cycle_A: 0.821 idt_A: 0.071 D_B: 0.208 G_B: 0.441 cycle_B: 0.230 idt_B: 0.214 \n",
      "End of epoch 85 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 86, iters: 20, time: 0.295, data: 0.001) D_A: 0.158 G_A: 0.441 cycle_A: 1.302 idt_A: 0.116 D_B: 0.213 G_B: 0.403 cycle_B: 0.301 idt_B: 0.368 \n",
      "(epoch: 86, iters: 120, time: 0.296, data: 0.001) D_A: 0.132 G_A: 0.508 cycle_A: 0.768 idt_A: 0.081 D_B: 0.110 G_B: 0.329 cycle_B: 0.198 idt_B: 0.219 \n",
      "(epoch: 86, iters: 220, time: 0.280, data: 0.001) D_A: 0.124 G_A: 0.396 cycle_A: 0.723 idt_A: 0.137 D_B: 0.236 G_B: 0.249 cycle_B: 0.327 idt_B: 0.177 \n",
      "End of epoch 86 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 87, iters: 32, time: 1.257, data: 0.002) D_A: 0.137 G_A: 0.535 cycle_A: 1.313 idt_A: 0.103 D_B: 0.196 G_B: 0.496 cycle_B: 0.215 idt_B: 0.368 \n",
      "(epoch: 87, iters: 132, time: 0.316, data: 0.002) D_A: 0.145 G_A: 0.412 cycle_A: 0.798 idt_A: 0.085 D_B: 0.299 G_B: 0.524 cycle_B: 0.232 idt_B: 0.221 \n",
      "(epoch: 87, iters: 232, time: 0.285, data: 0.001) D_A: 0.145 G_A: 0.402 cycle_A: 0.803 idt_A: 0.103 D_B: 0.181 G_B: 0.436 cycle_B: 0.306 idt_B: 0.234 \n",
      "saving the latest model (epoch 87, total_iters 25000)\n",
      "End of epoch 87 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 88, iters: 44, time: 0.301, data: 0.002) D_A: 0.136 G_A: 0.412 cycle_A: 1.069 idt_A: 0.104 D_B: 0.177 G_B: 0.381 cycle_B: 0.268 idt_B: 0.303 \n",
      "(epoch: 88, iters: 144, time: 1.325, data: 0.002) D_A: 0.132 G_A: 0.570 cycle_A: 1.114 idt_A: 0.087 D_B: 0.236 G_B: 0.334 cycle_B: 0.261 idt_B: 0.302 \n",
      "(epoch: 88, iters: 244, time: 0.288, data: 0.002) D_A: 0.137 G_A: 0.522 cycle_A: 1.008 idt_A: 0.117 D_B: 0.263 G_B: 0.368 cycle_B: 0.241 idt_B: 0.262 \n",
      "End of epoch 88 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 89, iters: 56, time: 0.316, data: 0.001) D_A: 0.141 G_A: 0.511 cycle_A: 1.119 idt_A: 0.105 D_B: 0.198 G_B: 0.394 cycle_B: 0.315 idt_B: 0.302 \n",
      "(epoch: 89, iters: 156, time: 0.280, data: 0.001) D_A: 0.162 G_A: 0.410 cycle_A: 0.680 idt_A: 0.083 D_B: 0.110 G_B: 0.327 cycle_B: 0.237 idt_B: 0.156 \n",
      "(epoch: 89, iters: 256, time: 1.338, data: 0.001) D_A: 0.179 G_A: 0.391 cycle_A: 0.902 idt_A: 0.089 D_B: 0.171 G_B: 0.431 cycle_B: 0.258 idt_B: 0.272 \n",
      "End of epoch 89 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 90, iters: 68, time: 0.317, data: 0.001) D_A: 0.184 G_A: 0.383 cycle_A: 1.081 idt_A: 0.122 D_B: 0.202 G_B: 0.298 cycle_B: 0.395 idt_B: 0.234 \n",
      "(epoch: 90, iters: 168, time: 0.290, data: 0.001) D_A: 0.136 G_A: 0.410 cycle_A: 0.658 idt_A: 0.092 D_B: 0.275 G_B: 0.312 cycle_B: 0.256 idt_B: 0.189 \n",
      "(epoch: 90, iters: 268, time: 0.310, data: 0.002) D_A: 0.168 G_A: 0.462 cycle_A: 0.837 idt_A: 0.081 D_B: 0.250 G_B: 0.430 cycle_B: 0.195 idt_B: 0.216 \n",
      "End of epoch 90 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 91, iters: 80, time: 1.341, data: 0.001) D_A: 0.122 G_A: 0.545 cycle_A: 1.361 idt_A: 0.087 D_B: 0.157 G_B: 0.379 cycle_B: 0.264 idt_B: 0.379 \n",
      "(epoch: 91, iters: 180, time: 0.293, data: 0.001) D_A: 0.147 G_A: 0.490 cycle_A: 1.367 idt_A: 0.133 D_B: 0.134 G_B: 0.419 cycle_B: 0.296 idt_B: 0.369 \n",
      "(epoch: 91, iters: 280, time: 0.308, data: 0.001) D_A: 0.172 G_A: 0.404 cycle_A: 1.047 idt_A: 0.077 D_B: 0.211 G_B: 0.322 cycle_B: 0.248 idt_B: 0.235 \n",
      "End of epoch 91 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 92, iters: 92, time: 0.302, data: 0.002) D_A: 0.141 G_A: 0.644 cycle_A: 1.354 idt_A: 0.070 D_B: 0.131 G_B: 0.428 cycle_B: 0.161 idt_B: 0.354 \n",
      "(epoch: 92, iters: 192, time: 1.363, data: 0.002) D_A: 0.150 G_A: 0.389 cycle_A: 1.239 idt_A: 0.079 D_B: 0.176 G_B: 0.501 cycle_B: 0.219 idt_B: 0.351 \n",
      "End of epoch 92 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 93, iters: 4, time: 0.281, data: 0.002) D_A: 0.130 G_A: 0.396 cycle_A: 0.840 idt_A: 0.074 D_B: 0.162 G_B: 0.226 cycle_B: 0.241 idt_B: 0.240 \n",
      "(epoch: 93, iters: 104, time: 0.284, data: 0.002) D_A: 0.142 G_A: 0.433 cycle_A: 1.123 idt_A: 0.094 D_B: 0.193 G_B: 0.289 cycle_B: 0.265 idt_B: 0.308 \n",
      "(epoch: 93, iters: 204, time: 0.317, data: 0.001) D_A: 0.187 G_A: 0.354 cycle_A: 0.750 idt_A: 0.099 D_B: 0.235 G_B: 0.380 cycle_B: 0.288 idt_B: 0.208 \n",
      "End of epoch 93 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 94, iters: 16, time: 1.425, data: 0.001) D_A: 0.135 G_A: 0.430 cycle_A: 0.687 idt_A: 0.083 D_B: 0.068 G_B: 0.402 cycle_B: 0.257 idt_B: 0.194 \n",
      "(epoch: 94, iters: 116, time: 0.291, data: 0.002) D_A: 0.165 G_A: 0.416 cycle_A: 0.762 idt_A: 0.104 D_B: 0.069 G_B: 0.487 cycle_B: 0.291 idt_B: 0.212 \n",
      "(epoch: 94, iters: 216, time: 0.305, data: 0.001) D_A: 0.110 G_A: 0.481 cycle_A: 1.203 idt_A: 0.105 D_B: 0.254 G_B: 0.319 cycle_B: 0.320 idt_B: 0.335 \n",
      "End of epoch 94 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 95, iters: 28, time: 0.293, data: 0.002) D_A: 0.152 G_A: 0.466 cycle_A: 0.837 idt_A: 0.075 D_B: 0.199 G_B: 0.435 cycle_B: 0.202 idt_B: 0.215 \n",
      "(epoch: 95, iters: 128, time: 1.420, data: 0.002) D_A: 0.147 G_A: 0.421 cycle_A: 0.674 idt_A: 0.081 D_B: 0.079 G_B: 0.405 cycle_B: 0.221 idt_B: 0.190 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 95, iters: 228, time: 0.282, data: 0.001) D_A: 0.165 G_A: 0.444 cycle_A: 0.793 idt_A: 0.066 D_B: 0.205 G_B: 0.445 cycle_B: 0.183 idt_B: 0.215 \n",
      "End of epoch 95 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 96, iters: 40, time: 0.315, data: 0.001) D_A: 0.146 G_A: 0.483 cycle_A: 1.282 idt_A: 0.088 D_B: 0.142 G_B: 0.374 cycle_B: 0.232 idt_B: 0.353 \n",
      "(epoch: 96, iters: 140, time: 0.313, data: 0.002) D_A: 0.137 G_A: 0.497 cycle_A: 0.860 idt_A: 0.109 D_B: 0.095 G_B: 0.381 cycle_B: 0.262 idt_B: 0.237 \n",
      "(epoch: 96, iters: 240, time: 1.461, data: 0.001) D_A: 0.143 G_A: 0.523 cycle_A: 0.689 idt_A: 0.138 D_B: 0.164 G_B: 0.350 cycle_B: 0.300 idt_B: 0.174 \n",
      "End of epoch 96 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 97, iters: 52, time: 0.307, data: 0.002) D_A: 0.179 G_A: 0.342 cycle_A: 0.750 idt_A: 0.103 D_B: 0.261 G_B: 0.356 cycle_B: 0.276 idt_B: 0.214 \n",
      "(epoch: 97, iters: 152, time: 0.288, data: 0.001) D_A: 0.141 G_A: 0.586 cycle_A: 1.147 idt_A: 0.069 D_B: 0.215 G_B: 0.409 cycle_B: 0.195 idt_B: 0.315 \n",
      "(epoch: 97, iters: 252, time: 0.292, data: 0.001) D_A: 0.149 G_A: 0.579 cycle_A: 1.391 idt_A: 0.113 D_B: 0.146 G_B: 0.413 cycle_B: 0.310 idt_B: 0.349 \n",
      "End of epoch 97 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 98, iters: 64, time: 1.443, data: 0.001) D_A: 0.169 G_A: 0.443 cycle_A: 0.687 idt_A: 0.093 D_B: 0.302 G_B: 0.353 cycle_B: 0.263 idt_B: 0.181 \n",
      "(epoch: 98, iters: 164, time: 0.314, data: 0.002) D_A: 0.144 G_A: 0.596 cycle_A: 1.028 idt_A: 0.087 D_B: 0.207 G_B: 0.375 cycle_B: 0.244 idt_B: 0.236 \n",
      "(epoch: 98, iters: 264, time: 0.289, data: 0.001) D_A: 0.143 G_A: 0.494 cycle_A: 0.983 idt_A: 0.092 D_B: 0.178 G_B: 0.353 cycle_B: 0.266 idt_B: 0.230 \n",
      "End of epoch 98 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 99, iters: 76, time: 0.308, data: 0.001) D_A: 0.184 G_A: 0.392 cycle_A: 0.549 idt_A: 0.103 D_B: 0.281 G_B: 0.345 cycle_B: 0.283 idt_B: 0.192 \n",
      "(epoch: 99, iters: 176, time: 1.410, data: 0.002) D_A: 0.149 G_A: 0.441 cycle_A: 0.710 idt_A: 0.071 D_B: 0.247 G_B: 0.390 cycle_B: 0.209 idt_B: 0.207 \n",
      "(epoch: 99, iters: 276, time: 0.315, data: 0.001) D_A: 0.133 G_A: 0.475 cycle_A: 0.999 idt_A: 0.079 D_B: 0.208 G_B: 0.437 cycle_B: 0.214 idt_B: 0.279 \n",
      "End of epoch 99 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 100, iters: 88, time: 0.296, data: 0.002) D_A: 0.152 G_A: 0.425 cycle_A: 0.720 idt_A: 0.121 D_B: 0.281 G_B: 0.371 cycle_B: 0.237 idt_B: 0.194 \n",
      "(epoch: 100, iters: 188, time: 0.278, data: 0.001) D_A: 0.165 G_A: 0.418 cycle_A: 0.891 idt_A: 0.121 D_B: 0.102 G_B: 0.318 cycle_B: 0.330 idt_B: 0.254 \n",
      "(epoch: 100, iters: 288, time: 1.395, data: 0.002) D_A: 0.200 G_A: 0.532 cycle_A: 1.243 idt_A: 0.105 D_B: 0.224 G_B: 0.388 cycle_B: 0.256 idt_B: 0.353 \n",
      "saving the model at the end of epoch 100, iters 28800\n",
      "End of epoch 100 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 101, iters: 100, time: 0.299, data: 0.285) D_A: 0.193 G_A: 0.357 cycle_A: 0.736 idt_A: 0.098 D_B: 0.224 G_B: 0.289 cycle_B: 0.293 idt_B: 0.220 \n",
      "(epoch: 101, iters: 200, time: 0.319, data: 0.002) D_A: 0.144 G_A: 0.378 cycle_A: 0.914 idt_A: 0.068 D_B: 0.163 G_B: 0.455 cycle_B: 0.176 idt_B: 0.257 \n",
      "End of epoch 101 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 102, iters: 12, time: 0.315, data: 0.001) D_A: 0.145 G_A: 0.462 cycle_A: 1.029 idt_A: 0.062 D_B: 0.179 G_B: 0.442 cycle_B: 0.175 idt_B: 0.291 \n",
      "(epoch: 102, iters: 112, time: 1.394, data: 0.001) D_A: 0.149 G_A: 0.542 cycle_A: 1.415 idt_A: 0.108 D_B: 0.144 G_B: 0.383 cycle_B: 0.239 idt_B: 0.353 \n",
      "(epoch: 102, iters: 212, time: 0.312, data: 0.001) D_A: 0.132 G_A: 0.511 cycle_A: 1.158 idt_A: 0.083 D_B: 0.207 G_B: 0.215 cycle_B: 0.260 idt_B: 0.326 \n",
      "End of epoch 102 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 103, iters: 24, time: 0.296, data: 0.001) D_A: 0.156 G_A: 0.435 cycle_A: 1.304 idt_A: 0.093 D_B: 0.207 G_B: 0.310 cycle_B: 0.262 idt_B: 0.367 \n",
      "(epoch: 103, iters: 124, time: 0.300, data: 0.001) D_A: 0.147 G_A: 0.474 cycle_A: 1.216 idt_A: 0.073 D_B: 0.210 G_B: 0.428 cycle_B: 0.208 idt_B: 0.341 \n",
      "(epoch: 103, iters: 224, time: 1.450, data: 0.001) D_A: 0.164 G_A: 0.391 cycle_A: 1.190 idt_A: 0.088 D_B: 0.157 G_B: 0.354 cycle_B: 0.283 idt_B: 0.333 \n",
      "End of epoch 103 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 104, iters: 36, time: 0.283, data: 0.001) D_A: 0.177 G_A: 0.531 cycle_A: 0.992 idt_A: 0.104 D_B: 0.204 G_B: 0.386 cycle_B: 0.314 idt_B: 0.285 \n",
      "(epoch: 104, iters: 136, time: 0.289, data: 0.001) D_A: 0.143 G_A: 0.372 cycle_A: 0.925 idt_A: 0.084 D_B: 0.198 G_B: 0.435 cycle_B: 0.256 idt_B: 0.248 \n",
      "(epoch: 104, iters: 236, time: 0.291, data: 0.001) D_A: 0.163 G_A: 0.431 cycle_A: 0.692 idt_A: 0.072 D_B: 0.221 G_B: 0.492 cycle_B: 0.213 idt_B: 0.183 \n",
      "End of epoch 104 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 105, iters: 48, time: 1.467, data: 0.001) D_A: 0.144 G_A: 0.438 cycle_A: 1.091 idt_A: 0.089 D_B: 0.231 G_B: 0.308 cycle_B: 0.283 idt_B: 0.303 \n",
      "saving the latest model (epoch 105, total_iters 30000)\n",
      "(epoch: 105, iters: 148, time: 0.299, data: 0.002) D_A: 0.192 G_A: 0.432 cycle_A: 1.085 idt_A: 0.098 D_B: 0.174 G_B: 0.488 cycle_B: 0.215 idt_B: 0.298 \n",
      "(epoch: 105, iters: 248, time: 0.309, data: 0.002) D_A: 0.184 G_A: 0.304 cycle_A: 0.600 idt_A: 0.097 D_B: 0.147 G_B: 0.262 cycle_B: 0.292 idt_B: 0.174 \n",
      "End of epoch 105 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 106, iters: 60, time: 0.280, data: 0.001) D_A: 0.150 G_A: 0.462 cycle_A: 1.231 idt_A: 0.065 D_B: 0.192 G_B: 0.393 cycle_B: 0.177 idt_B: 0.349 \n",
      "(epoch: 106, iters: 160, time: 1.399, data: 0.001) D_A: 0.179 G_A: 0.443 cycle_A: 0.960 idt_A: 0.086 D_B: 0.188 G_B: 0.436 cycle_B: 0.257 idt_B: 0.253 \n",
      "(epoch: 106, iters: 260, time: 0.317, data: 0.002) D_A: 0.143 G_A: 0.538 cycle_A: 1.424 idt_A: 0.071 D_B: 0.158 G_B: 0.384 cycle_B: 0.209 idt_B: 0.374 \n",
      "End of epoch 106 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 107, iters: 72, time: 0.315, data: 0.001) D_A: 0.149 G_A: 0.480 cycle_A: 0.687 idt_A: 0.084 D_B: 0.111 G_B: 0.306 cycle_B: 0.242 idt_B: 0.174 \n",
      "(epoch: 107, iters: 172, time: 0.296, data: 0.001) D_A: 0.178 G_A: 0.360 cycle_A: 0.898 idt_A: 0.089 D_B: 0.290 G_B: 0.415 cycle_B: 0.258 idt_B: 0.271 \n",
      "(epoch: 107, iters: 272, time: 1.418, data: 0.001) D_A: 0.138 G_A: 0.586 cycle_A: 1.122 idt_A: 0.089 D_B: 0.271 G_B: 0.270 cycle_B: 0.270 idt_B: 0.300 \n",
      "End of epoch 107 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 108, iters: 84, time: 0.284, data: 0.002) D_A: 0.159 G_A: 0.379 cycle_A: 0.599 idt_A: 0.100 D_B: 0.087 G_B: 0.391 cycle_B: 0.274 idt_B: 0.174 \n",
      "(epoch: 108, iters: 184, time: 0.291, data: 0.001) D_A: 0.110 G_A: 0.558 cycle_A: 1.421 idt_A: 0.090 D_B: 0.177 G_B: 0.452 cycle_B: 0.265 idt_B: 0.390 \n",
      "(epoch: 108, iters: 284, time: 0.301, data: 0.001) D_A: 0.143 G_A: 0.599 cycle_A: 0.998 idt_A: 0.061 D_B: 0.202 G_B: 0.443 cycle_B: 0.172 idt_B: 0.250 \n",
      "End of epoch 108 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 109, iters: 96, time: 1.448, data: 0.001) D_A: 0.138 G_A: 0.449 cycle_A: 0.997 idt_A: 0.113 D_B: 0.225 G_B: 0.376 cycle_B: 0.308 idt_B: 0.278 \n",
      "(epoch: 109, iters: 196, time: 0.319, data: 0.001) D_A: 0.174 G_A: 0.449 cycle_A: 0.756 idt_A: 0.097 D_B: 0.104 G_B: 0.348 cycle_B: 0.271 idt_B: 0.217 \n",
      "End of epoch 109 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 110, iters: 8, time: 0.314, data: 0.001) D_A: 0.155 G_A: 0.522 cycle_A: 0.831 idt_A: 0.077 D_B: 0.252 G_B: 0.441 cycle_B: 0.215 idt_B: 0.218 \n",
      "(epoch: 110, iters: 108, time: 0.316, data: 0.001) D_A: 0.134 G_A: 0.456 cycle_A: 1.429 idt_A: 0.098 D_B: 0.268 G_B: 0.221 cycle_B: 0.266 idt_B: 0.388 \n",
      "(epoch: 110, iters: 208, time: 1.427, data: 0.001) D_A: 0.165 G_A: 0.445 cycle_A: 1.213 idt_A: 0.071 D_B: 0.182 G_B: 0.443 cycle_B: 0.211 idt_B: 0.335 \n",
      "End of epoch 110 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 111, iters: 20, time: 0.293, data: 0.001) D_A: 0.141 G_A: 0.444 cycle_A: 1.086 idt_A: 0.084 D_B: 0.314 G_B: 0.261 cycle_B: 0.250 idt_B: 0.297 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 111, iters: 120, time: 0.293, data: 0.001) D_A: 0.160 G_A: 0.503 cycle_A: 1.076 idt_A: 0.104 D_B: 0.211 G_B: 0.355 cycle_B: 0.304 idt_B: 0.253 \n",
      "(epoch: 111, iters: 220, time: 0.304, data: 0.001) D_A: 0.150 G_A: 0.498 cycle_A: 1.125 idt_A: 0.083 D_B: 0.196 G_B: 0.459 cycle_B: 0.215 idt_B: 0.314 \n",
      "End of epoch 111 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 112, iters: 32, time: 1.429, data: 0.001) D_A: 0.139 G_A: 0.540 cycle_A: 1.134 idt_A: 0.062 D_B: 0.149 G_B: 0.399 cycle_B: 0.188 idt_B: 0.295 \n",
      "(epoch: 112, iters: 132, time: 0.290, data: 0.001) D_A: 0.122 G_A: 0.347 cycle_A: 0.767 idt_A: 0.079 D_B: 0.106 G_B: 0.443 cycle_B: 0.228 idt_B: 0.223 \n",
      "(epoch: 112, iters: 232, time: 0.306, data: 0.001) D_A: 0.141 G_A: 0.445 cycle_A: 0.788 idt_A: 0.100 D_B: 0.303 G_B: 0.420 cycle_B: 0.275 idt_B: 0.198 \n",
      "End of epoch 112 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 113, iters: 44, time: 0.312, data: 0.001) D_A: 0.126 G_A: 0.337 cycle_A: 0.783 idt_A: 0.069 D_B: 0.350 G_B: 0.380 cycle_B: 0.211 idt_B: 0.205 \n",
      "(epoch: 113, iters: 144, time: 1.490, data: 0.001) D_A: 0.152 G_A: 0.538 cycle_A: 1.121 idt_A: 0.084 D_B: 0.203 G_B: 0.261 cycle_B: 0.271 idt_B: 0.296 \n",
      "(epoch: 113, iters: 244, time: 0.308, data: 0.001) D_A: 0.184 G_A: 0.480 cycle_A: 0.858 idt_A: 0.072 D_B: 0.145 G_B: 0.395 cycle_B: 0.213 idt_B: 0.219 \n",
      "End of epoch 113 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 114, iters: 56, time: 0.277, data: 0.001) D_A: 0.149 G_A: 0.515 cycle_A: 1.395 idt_A: 0.064 D_B: 0.169 G_B: 0.419 cycle_B: 0.178 idt_B: 0.384 \n",
      "(epoch: 114, iters: 156, time: 0.308, data: 0.001) D_A: 0.140 G_A: 0.436 cycle_A: 1.376 idt_A: 0.088 D_B: 0.214 G_B: 0.280 cycle_B: 0.281 idt_B: 0.382 \n",
      "(epoch: 114, iters: 256, time: 1.493, data: 0.002) D_A: 0.139 G_A: 0.486 cycle_A: 1.324 idt_A: 0.125 D_B: 0.183 G_B: 0.292 cycle_B: 0.335 idt_B: 0.325 \n",
      "End of epoch 114 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 115, iters: 68, time: 0.286, data: 0.002) D_A: 0.148 G_A: 0.504 cycle_A: 0.645 idt_A: 0.068 D_B: 0.267 G_B: 0.404 cycle_B: 0.176 idt_B: 0.183 \n",
      "(epoch: 115, iters: 168, time: 0.286, data: 0.002) D_A: 0.157 G_A: 0.286 cycle_A: 0.488 idt_A: 0.103 D_B: 0.255 G_B: 0.346 cycle_B: 0.300 idt_B: 0.166 \n",
      "(epoch: 115, iters: 268, time: 0.311, data: 0.001) D_A: 0.174 G_A: 0.518 cycle_A: 0.999 idt_A: 0.106 D_B: 0.214 G_B: 0.381 cycle_B: 0.252 idt_B: 0.261 \n",
      "End of epoch 115 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 116, iters: 80, time: 1.541, data: 0.001) D_A: 0.185 G_A: 0.390 cycle_A: 1.072 idt_A: 0.115 D_B: 0.277 G_B: 0.326 cycle_B: 0.301 idt_B: 0.299 \n",
      "(epoch: 116, iters: 180, time: 0.311, data: 0.001) D_A: 0.172 G_A: 0.446 cycle_A: 1.083 idt_A: 0.084 D_B: 0.238 G_B: 0.262 cycle_B: 0.276 idt_B: 0.297 \n",
      "(epoch: 116, iters: 280, time: 0.280, data: 0.001) D_A: 0.162 G_A: 0.431 cycle_A: 0.867 idt_A: 0.095 D_B: 0.208 G_B: 0.477 cycle_B: 0.212 idt_B: 0.244 \n",
      "End of epoch 116 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 117, iters: 92, time: 0.280, data: 0.001) D_A: 0.140 G_A: 0.371 cycle_A: 0.688 idt_A: 0.087 D_B: 0.277 G_B: 0.422 cycle_B: 0.261 idt_B: 0.192 \n",
      "(epoch: 117, iters: 192, time: 1.514, data: 0.001) D_A: 0.142 G_A: 0.411 cycle_A: 0.782 idt_A: 0.103 D_B: 0.248 G_B: 0.360 cycle_B: 0.258 idt_B: 0.228 \n",
      "End of epoch 117 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 118, iters: 4, time: 0.306, data: 0.001) D_A: 0.191 G_A: 0.477 cycle_A: 1.178 idt_A: 0.139 D_B: 0.221 G_B: 0.347 cycle_B: 0.295 idt_B: 0.331 \n",
      "(epoch: 118, iters: 104, time: 0.300, data: 0.004) D_A: 0.144 G_A: 0.432 cycle_A: 1.136 idt_A: 0.101 D_B: 0.117 G_B: 0.429 cycle_B: 0.276 idt_B: 0.279 \n",
      "(epoch: 118, iters: 204, time: 0.289, data: 0.001) D_A: 0.134 G_A: 0.519 cycle_A: 1.343 idt_A: 0.091 D_B: 0.248 G_B: 0.249 cycle_B: 0.268 idt_B: 0.362 \n",
      "End of epoch 118 / 300 \t Time Taken: 80 sec\n",
      "learning rate 0.0000010 -> 0.0000010\n",
      "(epoch: 119, iters: 16, time: 1.481, data: 0.001) D_A: 0.169 G_A: 0.458 cycle_A: 0.899 idt_A: 0.079 D_B: 0.177 G_B: 0.380 cycle_B: 0.214 idt_B: 0.229 \n",
      "(epoch: 119, iters: 116, time: 0.290, data: 0.002) D_A: 0.150 G_A: 0.414 cycle_A: 0.886 idt_A: 0.082 D_B: 0.095 G_B: 0.356 cycle_B: 0.245 idt_B: 0.228 \n",
      "(epoch: 119, iters: 216, time: 0.293, data: 0.001) D_A: 0.170 G_A: 0.479 cycle_A: 1.083 idt_A: 0.115 D_B: 0.203 G_B: 0.315 cycle_B: 0.301 idt_B: 0.261 \n",
      "End of epoch 119 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000010 -> 0.0000001\n",
      "(epoch: 120, iters: 28, time: 0.317, data: 0.001) D_A: 0.167 G_A: 0.386 cycle_A: 0.683 idt_A: 0.084 D_B: 0.267 G_B: 0.419 cycle_B: 0.226 idt_B: 0.180 \n",
      "(epoch: 120, iters: 128, time: 1.516, data: 0.002) D_A: 0.139 G_A: 0.482 cycle_A: 0.981 idt_A: 0.083 D_B: 0.178 G_B: 0.412 cycle_B: 0.250 idt_B: 0.227 \n",
      "(epoch: 120, iters: 228, time: 0.295, data: 0.001) D_A: 0.146 G_A: 0.430 cycle_A: 0.546 idt_A: 0.087 D_B: 0.346 G_B: 0.315 cycle_B: 0.260 idt_B: 0.167 \n",
      "End of epoch 120 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 121, iters: 40, time: 0.289, data: 0.002) D_A: 0.141 G_A: 0.509 cycle_A: 1.196 idt_A: 0.092 D_B: 0.168 G_B: 0.201 cycle_B: 0.258 idt_B: 0.313 \n",
      "(epoch: 121, iters: 140, time: 0.316, data: 0.001) D_A: 0.139 G_A: 0.402 cycle_A: 0.852 idt_A: 0.092 D_B: 0.240 G_B: 0.414 cycle_B: 0.232 idt_B: 0.239 \n",
      "(epoch: 121, iters: 240, time: 1.509, data: 0.001) D_A: 0.159 G_A: 0.401 cycle_A: 1.317 idt_A: 0.069 D_B: 0.152 G_B: 0.463 cycle_B: 0.177 idt_B: 0.363 \n",
      "End of epoch 121 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 122, iters: 52, time: 0.313, data: 0.001) D_A: 0.143 G_A: 0.472 cycle_A: 1.127 idt_A: 0.081 D_B: 0.213 G_B: 0.341 cycle_B: 0.181 idt_B: 0.258 \n",
      "(epoch: 122, iters: 152, time: 0.312, data: 0.001) D_A: 0.138 G_A: 0.447 cycle_A: 0.783 idt_A: 0.064 D_B: 0.161 G_B: 0.417 cycle_B: 0.178 idt_B: 0.240 \n",
      "saving the latest model (epoch 122, total_iters 35000)\n",
      "(epoch: 122, iters: 252, time: 0.305, data: 0.001) D_A: 0.186 G_A: 0.433 cycle_A: 1.055 idt_A: 0.103 D_B: 0.196 G_B: 0.319 cycle_B: 0.272 idt_B: 0.296 \n",
      "End of epoch 122 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 123, iters: 64, time: 1.563, data: 0.001) D_A: 0.181 G_A: 0.453 cycle_A: 0.654 idt_A: 0.102 D_B: 0.347 G_B: 0.361 cycle_B: 0.300 idt_B: 0.181 \n",
      "(epoch: 123, iters: 164, time: 0.301, data: 0.001) D_A: 0.125 G_A: 0.533 cycle_A: 1.067 idt_A: 0.078 D_B: 0.219 G_B: 0.386 cycle_B: 0.237 idt_B: 0.278 \n",
      "(epoch: 123, iters: 264, time: 0.291, data: 0.001) D_A: 0.187 G_A: 0.349 cycle_A: 0.764 idt_A: 0.083 D_B: 0.115 G_B: 0.379 cycle_B: 0.230 idt_B: 0.223 \n",
      "End of epoch 123 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 124, iters: 76, time: 0.316, data: 0.001) D_A: 0.145 G_A: 0.551 cycle_A: 0.936 idt_A: 0.079 D_B: 0.227 G_B: 0.382 cycle_B: 0.209 idt_B: 0.227 \n",
      "(epoch: 124, iters: 176, time: 1.563, data: 0.001) D_A: 0.146 G_A: 0.461 cycle_A: 0.916 idt_A: 0.086 D_B: 0.217 G_B: 0.315 cycle_B: 0.259 idt_B: 0.257 \n",
      "(epoch: 124, iters: 276, time: 0.299, data: 0.001) D_A: 0.166 G_A: 0.453 cycle_A: 0.897 idt_A: 0.071 D_B: 0.201 G_B: 0.456 cycle_B: 0.208 idt_B: 0.260 \n",
      "End of epoch 124 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 125, iters: 88, time: 0.286, data: 0.001) D_A: 0.169 G_A: 0.453 cycle_A: 1.091 idt_A: 0.093 D_B: 0.229 G_B: 0.301 cycle_B: 0.280 idt_B: 0.249 \n",
      "(epoch: 125, iters: 188, time: 0.315, data: 0.002) D_A: 0.144 G_A: 0.453 cycle_A: 0.551 idt_A: 0.065 D_B: 0.085 G_B: 0.416 cycle_B: 0.178 idt_B: 0.159 \n",
      "(epoch: 125, iters: 288, time: 1.571, data: 0.002) D_A: 0.172 G_A: 0.461 cycle_A: 0.914 idt_A: 0.093 D_B: 0.171 G_B: 0.340 cycle_B: 0.254 idt_B: 0.208 \n",
      "End of epoch 125 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 126, iters: 100, time: 0.309, data: 0.303) D_A: 0.233 G_A: 0.454 cycle_A: 0.994 idt_A: 0.102 D_B: 0.198 G_B: 0.398 cycle_B: 0.246 idt_B: 0.276 \n",
      "(epoch: 126, iters: 200, time: 0.317, data: 0.002) D_A: 0.171 G_A: 0.541 cycle_A: 1.420 idt_A: 0.083 D_B: 0.258 G_B: 0.376 cycle_B: 0.230 idt_B: 0.389 \n",
      "End of epoch 126 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 127, iters: 12, time: 0.318, data: 0.001) D_A: 0.152 G_A: 0.484 cycle_A: 0.884 idt_A: 0.079 D_B: 0.103 G_B: 0.240 cycle_B: 0.236 idt_B: 0.265 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 127, iters: 112, time: 1.506, data: 0.002) D_A: 0.152 G_A: 0.620 cycle_A: 1.404 idt_A: 0.088 D_B: 0.144 G_B: 0.330 cycle_B: 0.280 idt_B: 0.351 \n",
      "(epoch: 127, iters: 212, time: 0.305, data: 0.001) D_A: 0.139 G_A: 0.509 cycle_A: 1.406 idt_A: 0.102 D_B: 0.196 G_B: 0.310 cycle_B: 0.312 idt_B: 0.386 \n",
      "End of epoch 127 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 128, iters: 24, time: 0.314, data: 0.001) D_A: 0.222 G_A: 0.306 cycle_A: 0.611 idt_A: 0.080 D_B: 0.060 G_B: 0.474 cycle_B: 0.214 idt_B: 0.171 \n",
      "(epoch: 128, iters: 124, time: 0.282, data: 0.001) D_A: 0.158 G_A: 0.445 cycle_A: 0.775 idt_A: 0.077 D_B: 0.276 G_B: 0.325 cycle_B: 0.245 idt_B: 0.206 \n",
      "(epoch: 128, iters: 224, time: 1.594, data: 0.002) D_A: 0.170 G_A: 0.466 cycle_A: 0.858 idt_A: 0.103 D_B: 0.226 G_B: 0.338 cycle_B: 0.304 idt_B: 0.225 \n",
      "End of epoch 128 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 129, iters: 36, time: 0.292, data: 0.001) D_A: 0.130 G_A: 0.545 cycle_A: 0.978 idt_A: 0.079 D_B: 0.227 G_B: 0.242 cycle_B: 0.236 idt_B: 0.279 \n",
      "(epoch: 129, iters: 136, time: 0.314, data: 0.002) D_A: 0.143 G_A: 0.421 cycle_A: 1.199 idt_A: 0.091 D_B: 0.174 G_B: 0.374 cycle_B: 0.228 idt_B: 0.333 \n",
      "(epoch: 129, iters: 236, time: 0.311, data: 0.001) D_A: 0.167 G_A: 0.395 cycle_A: 0.800 idt_A: 0.091 D_B: 0.232 G_B: 0.322 cycle_B: 0.258 idt_B: 0.232 \n",
      "End of epoch 129 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 130, iters: 48, time: 1.601, data: 0.001) D_A: 0.138 G_A: 0.507 cycle_A: 1.074 idt_A: 0.118 D_B: 0.192 G_B: 0.429 cycle_B: 0.309 idt_B: 0.244 \n",
      "(epoch: 130, iters: 148, time: 0.290, data: 0.001) D_A: 0.159 G_A: 0.475 cycle_A: 0.700 idt_A: 0.112 D_B: 0.089 G_B: 0.408 cycle_B: 0.283 idt_B: 0.153 \n",
      "(epoch: 130, iters: 248, time: 0.311, data: 0.001) D_A: 0.145 G_A: 0.370 cycle_A: 0.545 idt_A: 0.091 D_B: 0.303 G_B: 0.344 cycle_B: 0.268 idt_B: 0.165 \n",
      "End of epoch 130 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 131, iters: 60, time: 0.295, data: 0.002) D_A: 0.158 G_A: 0.437 cycle_A: 0.955 idt_A: 0.064 D_B: 0.236 G_B: 0.451 cycle_B: 0.183 idt_B: 0.225 \n",
      "(epoch: 131, iters: 160, time: 1.585, data: 0.001) D_A: 0.131 G_A: 0.555 cycle_A: 1.371 idt_A: 0.104 D_B: 0.201 G_B: 0.317 cycle_B: 0.303 idt_B: 0.373 \n",
      "(epoch: 131, iters: 260, time: 0.292, data: 0.002) D_A: 0.225 G_A: 0.441 cycle_A: 0.996 idt_A: 0.074 D_B: 0.195 G_B: 0.263 cycle_B: 0.263 idt_B: 0.274 \n",
      "End of epoch 131 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 132, iters: 72, time: 0.301, data: 0.001) D_A: 0.152 G_A: 0.448 cycle_A: 1.303 idt_A: 0.072 D_B: 0.194 G_B: 0.418 cycle_B: 0.207 idt_B: 0.355 \n",
      "(epoch: 132, iters: 172, time: 0.280, data: 0.002) D_A: 0.156 G_A: 0.394 cycle_A: 0.675 idt_A: 0.110 D_B: 0.082 G_B: 0.291 cycle_B: 0.293 idt_B: 0.192 \n",
      "(epoch: 132, iters: 272, time: 1.589, data: 0.001) D_A: 0.154 G_A: 0.414 cycle_A: 0.680 idt_A: 0.079 D_B: 0.160 G_B: 0.396 cycle_B: 0.188 idt_B: 0.203 \n",
      "End of epoch 132 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 133, iters: 84, time: 0.286, data: 0.001) D_A: 0.173 G_A: 0.458 cycle_A: 0.879 idt_A: 0.093 D_B: 0.163 G_B: 0.234 cycle_B: 0.277 idt_B: 0.251 \n",
      "(epoch: 133, iters: 184, time: 0.291, data: 0.002) D_A: 0.160 G_A: 0.445 cycle_A: 1.204 idt_A: 0.088 D_B: 0.220 G_B: 0.197 cycle_B: 0.249 idt_B: 0.331 \n",
      "(epoch: 133, iters: 284, time: 0.312, data: 0.001) D_A: 0.147 G_A: 0.397 cycle_A: 0.859 idt_A: 0.089 D_B: 0.282 G_B: 0.268 cycle_B: 0.269 idt_B: 0.251 \n",
      "End of epoch 133 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 134, iters: 96, time: 1.618, data: 0.001) D_A: 0.185 G_A: 0.490 cycle_A: 1.011 idt_A: 0.113 D_B: 0.116 G_B: 0.331 cycle_B: 0.299 idt_B: 0.283 \n",
      "(epoch: 134, iters: 196, time: 0.298, data: 0.001) D_A: 0.134 G_A: 0.496 cycle_A: 1.201 idt_A: 0.081 D_B: 0.180 G_B: 0.331 cycle_B: 0.185 idt_B: 0.332 \n",
      "End of epoch 134 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 135, iters: 8, time: 0.310, data: 0.002) D_A: 0.159 G_A: 0.456 cycle_A: 0.920 idt_A: 0.097 D_B: 0.244 G_B: 0.274 cycle_B: 0.262 idt_B: 0.235 \n",
      "(epoch: 135, iters: 108, time: 0.313, data: 0.002) D_A: 0.157 G_A: 0.481 cycle_A: 1.073 idt_A: 0.109 D_B: 0.250 G_B: 0.214 cycle_B: 0.262 idt_B: 0.279 \n",
      "(epoch: 135, iters: 208, time: 1.566, data: 0.001) D_A: 0.152 G_A: 0.515 cycle_A: 1.405 idt_A: 0.069 D_B: 0.123 G_B: 0.479 cycle_B: 0.214 idt_B: 0.348 \n",
      "End of epoch 135 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 136, iters: 20, time: 0.285, data: 0.001) D_A: 0.180 G_A: 0.425 cycle_A: 1.061 idt_A: 0.099 D_B: 0.235 G_B: 0.367 cycle_B: 0.267 idt_B: 0.283 \n",
      "(epoch: 136, iters: 120, time: 0.308, data: 0.002) D_A: 0.193 G_A: 0.339 cycle_A: 0.681 idt_A: 0.118 D_B: 0.326 G_B: 0.369 cycle_B: 0.287 idt_B: 0.181 \n",
      "(epoch: 136, iters: 220, time: 0.285, data: 0.001) D_A: 0.200 G_A: 0.376 cycle_A: 0.608 idt_A: 0.096 D_B: 0.087 G_B: 0.364 cycle_B: 0.271 idt_B: 0.160 \n",
      "End of epoch 136 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 137, iters: 32, time: 1.643, data: 0.002) D_A: 0.162 G_A: 0.435 cycle_A: 0.756 idt_A: 0.066 D_B: 0.109 G_B: 0.383 cycle_B: 0.162 idt_B: 0.181 \n",
      "(epoch: 137, iters: 132, time: 0.285, data: 0.002) D_A: 0.147 G_A: 0.510 cycle_A: 0.967 idt_A: 0.108 D_B: 0.232 G_B: 0.347 cycle_B: 0.294 idt_B: 0.270 \n",
      "(epoch: 137, iters: 232, time: 0.316, data: 0.001) D_A: 0.170 G_A: 0.415 cycle_A: 1.097 idt_A: 0.071 D_B: 0.187 G_B: 0.480 cycle_B: 0.199 idt_B: 0.305 \n",
      "End of epoch 137 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 138, iters: 44, time: 0.314, data: 0.001) D_A: 0.169 G_A: 0.433 cycle_A: 1.045 idt_A: 0.086 D_B: 0.191 G_B: 0.389 cycle_B: 0.236 idt_B: 0.288 \n",
      "(epoch: 138, iters: 144, time: 1.672, data: 0.002) D_A: 0.177 G_A: 0.434 cycle_A: 0.779 idt_A: 0.096 D_B: 0.069 G_B: 0.452 cycle_B: 0.214 idt_B: 0.215 \n",
      "(epoch: 138, iters: 244, time: 0.286, data: 0.002) D_A: 0.166 G_A: 0.382 cycle_A: 0.850 idt_A: 0.087 D_B: 0.251 G_B: 0.305 cycle_B: 0.254 idt_B: 0.246 \n",
      "End of epoch 138 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 139, iters: 56, time: 0.279, data: 0.001) D_A: 0.136 G_A: 0.488 cycle_A: 1.207 idt_A: 0.093 D_B: 0.249 G_B: 0.332 cycle_B: 0.272 idt_B: 0.340 \n",
      "(epoch: 139, iters: 156, time: 0.303, data: 0.001) D_A: 0.178 G_A: 0.549 cycle_A: 0.978 idt_A: 0.061 D_B: 0.170 G_B: 0.452 cycle_B: 0.167 idt_B: 0.279 \n",
      "(epoch: 139, iters: 256, time: 1.634, data: 0.001) D_A: 0.163 G_A: 0.473 cycle_A: 0.858 idt_A: 0.136 D_B: 0.171 G_B: 0.349 cycle_B: 0.300 idt_B: 0.225 \n",
      "saving the latest model (epoch 139, total_iters 40000)\n",
      "End of epoch 139 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 140, iters: 68, time: 0.290, data: 0.002) D_A: 0.135 G_A: 0.553 cycle_A: 1.435 idt_A: 0.079 D_B: 0.147 G_B: 0.451 cycle_B: 0.213 idt_B: 0.383 \n",
      "(epoch: 140, iters: 168, time: 0.291, data: 0.001) D_A: 0.180 G_A: 0.568 cycle_A: 1.026 idt_A: 0.127 D_B: 0.194 G_B: 0.470 cycle_B: 0.252 idt_B: 0.233 \n",
      "(epoch: 140, iters: 268, time: 0.285, data: 0.001) D_A: 0.141 G_A: 0.476 cycle_A: 0.818 idt_A: 0.094 D_B: 0.299 G_B: 0.326 cycle_B: 0.257 idt_B: 0.226 \n",
      "End of epoch 140 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 141, iters: 80, time: 1.637, data: 0.001) D_A: 0.160 G_A: 0.435 cycle_A: 1.156 idt_A: 0.090 D_B: 0.295 G_B: 0.210 cycle_B: 0.257 idt_B: 0.321 \n",
      "(epoch: 141, iters: 180, time: 0.291, data: 0.001) D_A: 0.154 G_A: 0.528 cycle_A: 1.319 idt_A: 0.078 D_B: 0.177 G_B: 0.389 cycle_B: 0.234 idt_B: 0.322 \n",
      "(epoch: 141, iters: 280, time: 0.307, data: 0.002) D_A: 0.160 G_A: 0.516 cycle_A: 1.422 idt_A: 0.133 D_B: 0.198 G_B: 0.315 cycle_B: 0.323 idt_B: 0.372 \n",
      "End of epoch 141 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 142, iters: 92, time: 0.306, data: 0.001) D_A: 0.155 G_A: 0.421 cycle_A: 0.690 idt_A: 0.071 D_B: 0.106 G_B: 0.475 cycle_B: 0.199 idt_B: 0.179 \n",
      "(epoch: 142, iters: 192, time: 1.655, data: 0.002) D_A: 0.128 G_A: 0.591 cycle_A: 1.357 idt_A: 0.071 D_B: 0.185 G_B: 0.374 cycle_B: 0.207 idt_B: 0.377 \n",
      "End of epoch 142 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 143, iters: 4, time: 0.278, data: 0.001) D_A: 0.215 G_A: 0.451 cycle_A: 1.186 idt_A: 0.096 D_B: 0.210 G_B: 0.448 cycle_B: 0.214 idt_B: 0.330 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 143, iters: 104, time: 0.295, data: 0.003) D_A: 0.191 G_A: 0.459 cycle_A: 1.244 idt_A: 0.094 D_B: 0.251 G_B: 0.251 cycle_B: 0.260 idt_B: 0.337 \n",
      "(epoch: 143, iters: 204, time: 0.302, data: 0.001) D_A: 0.143 G_A: 0.505 cycle_A: 0.997 idt_A: 0.104 D_B: 0.252 G_B: 0.352 cycle_B: 0.299 idt_B: 0.244 \n",
      "End of epoch 143 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 144, iters: 16, time: 1.699, data: 0.001) D_A: 0.188 G_A: 0.373 cycle_A: 1.129 idt_A: 0.102 D_B: 0.220 G_B: 0.391 cycle_B: 0.266 idt_B: 0.314 \n",
      "(epoch: 144, iters: 116, time: 0.294, data: 0.001) D_A: 0.169 G_A: 0.521 cycle_A: 1.258 idt_A: 0.087 D_B: 0.163 G_B: 0.400 cycle_B: 0.236 idt_B: 0.311 \n",
      "(epoch: 144, iters: 216, time: 0.291, data: 0.001) D_A: 0.140 G_A: 0.499 cycle_A: 1.252 idt_A: 0.097 D_B: 0.310 G_B: 0.209 cycle_B: 0.264 idt_B: 0.349 \n",
      "End of epoch 144 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 145, iters: 28, time: 0.293, data: 0.002) D_A: 0.133 G_A: 0.481 cycle_A: 0.837 idt_A: 0.087 D_B: 0.147 G_B: 0.362 cycle_B: 0.231 idt_B: 0.204 \n",
      "(epoch: 145, iters: 128, time: 1.678, data: 0.001) D_A: 0.141 G_A: 0.394 cycle_A: 0.807 idt_A: 0.074 D_B: 0.345 G_B: 0.213 cycle_B: 0.250 idt_B: 0.222 \n",
      "(epoch: 145, iters: 228, time: 0.292, data: 0.002) D_A: 0.148 G_A: 0.486 cycle_A: 1.061 idt_A: 0.093 D_B: 0.239 G_B: 0.393 cycle_B: 0.232 idt_B: 0.287 \n",
      "End of epoch 145 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 146, iters: 40, time: 0.307, data: 0.002) D_A: 0.210 G_A: 0.389 cycle_A: 1.004 idt_A: 0.107 D_B: 0.253 G_B: 0.377 cycle_B: 0.266 idt_B: 0.278 \n",
      "(epoch: 146, iters: 140, time: 0.313, data: 0.001) D_A: 0.134 G_A: 0.466 cycle_A: 0.549 idt_A: 0.090 D_B: 0.077 G_B: 0.352 cycle_B: 0.259 idt_B: 0.159 \n",
      "(epoch: 146, iters: 240, time: 1.692, data: 0.001) D_A: 0.178 G_A: 0.446 cycle_A: 1.342 idt_A: 0.104 D_B: 0.201 G_B: 0.354 cycle_B: 0.299 idt_B: 0.351 \n",
      "End of epoch 146 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 147, iters: 52, time: 0.308, data: 0.001) D_A: 0.169 G_A: 0.408 cycle_A: 0.772 idt_A: 0.079 D_B: 0.220 G_B: 0.463 cycle_B: 0.213 idt_B: 0.209 \n",
      "(epoch: 147, iters: 152, time: 0.309, data: 0.001) D_A: 0.174 G_A: 0.444 cycle_A: 1.060 idt_A: 0.104 D_B: 0.188 G_B: 0.341 cycle_B: 0.304 idt_B: 0.262 \n",
      "(epoch: 147, iters: 252, time: 0.303, data: 0.002) D_A: 0.171 G_A: 0.432 cycle_A: 0.886 idt_A: 0.083 D_B: 0.264 G_B: 0.273 cycle_B: 0.239 idt_B: 0.256 \n",
      "End of epoch 147 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 148, iters: 64, time: 1.717, data: 0.001) D_A: 0.155 G_A: 0.468 cycle_A: 0.875 idt_A: 0.096 D_B: 0.261 G_B: 0.368 cycle_B: 0.242 idt_B: 0.250 \n",
      "(epoch: 148, iters: 164, time: 0.301, data: 0.002) D_A: 0.138 G_A: 0.509 cycle_A: 0.998 idt_A: 0.085 D_B: 0.178 G_B: 0.403 cycle_B: 0.224 idt_B: 0.244 \n",
      "(epoch: 148, iters: 264, time: 0.310, data: 0.002) D_A: 0.171 G_A: 0.429 cycle_A: 0.778 idt_A: 0.132 D_B: 0.246 G_B: 0.326 cycle_B: 0.286 idt_B: 0.215 \n",
      "End of epoch 148 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 149, iters: 76, time: 0.316, data: 0.002) D_A: 0.128 G_A: 0.401 cycle_A: 1.205 idt_A: 0.079 D_B: 0.177 G_B: 0.374 cycle_B: 0.209 idt_B: 0.331 \n",
      "(epoch: 149, iters: 176, time: 1.725, data: 0.001) D_A: 0.186 G_A: 0.516 cycle_A: 0.967 idt_A: 0.089 D_B: 0.221 G_B: 0.321 cycle_B: 0.273 idt_B: 0.270 \n",
      "(epoch: 149, iters: 276, time: 0.286, data: 0.002) D_A: 0.139 G_A: 0.377 cycle_A: 1.124 idt_A: 0.090 D_B: 0.175 G_B: 0.215 cycle_B: 0.256 idt_B: 0.286 \n",
      "End of epoch 149 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 150, iters: 88, time: 0.306, data: 0.001) D_A: 0.157 G_A: 0.334 cycle_A: 0.876 idt_A: 0.087 D_B: 0.222 G_B: 0.301 cycle_B: 0.254 idt_B: 0.256 \n",
      "(epoch: 150, iters: 188, time: 0.308, data: 0.001) D_A: 0.144 G_A: 0.346 cycle_A: 0.871 idt_A: 0.101 D_B: 0.136 G_B: 0.442 cycle_B: 0.273 idt_B: 0.202 \n",
      "(epoch: 150, iters: 288, time: 1.687, data: 0.001) D_A: 0.146 G_A: 0.450 cycle_A: 1.134 idt_A: 0.085 D_B: 0.210 G_B: 0.223 cycle_B: 0.262 idt_B: 0.293 \n",
      "saving the model at the end of epoch 150, iters 43200\n",
      "End of epoch 150 / 300 \t Time Taken: 85 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 151, iters: 100, time: 0.284, data: 0.285) D_A: 0.141 G_A: 0.341 cycle_A: 0.782 idt_A: 0.064 D_B: 0.260 G_B: 0.403 cycle_B: 0.177 idt_B: 0.204 \n",
      "(epoch: 151, iters: 200, time: 0.286, data: 0.001) D_A: 0.125 G_A: 0.384 cycle_A: 0.765 idt_A: 0.064 D_B: 0.295 G_B: 0.441 cycle_B: 0.182 idt_B: 0.190 \n",
      "End of epoch 151 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 152, iters: 12, time: 0.314, data: 0.001) D_A: 0.173 G_A: 0.428 cycle_A: 1.373 idt_A: 0.070 D_B: 0.197 G_B: 0.388 cycle_B: 0.153 idt_B: 0.364 \n",
      "(epoch: 152, iters: 112, time: 1.700, data: 0.002) D_A: 0.132 G_A: 0.527 cycle_A: 1.213 idt_A: 0.087 D_B: 0.163 G_B: 0.425 cycle_B: 0.259 idt_B: 0.291 \n",
      "(epoch: 152, iters: 212, time: 0.308, data: 0.002) D_A: 0.143 G_A: 0.439 cycle_A: 0.995 idt_A: 0.116 D_B: 0.282 G_B: 0.348 cycle_B: 0.259 idt_B: 0.266 \n",
      "End of epoch 152 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 153, iters: 24, time: 0.289, data: 0.002) D_A: 0.167 G_A: 0.443 cycle_A: 0.849 idt_A: 0.125 D_B: 0.072 G_B: 0.445 cycle_B: 0.253 idt_B: 0.244 \n",
      "(epoch: 153, iters: 124, time: 0.280, data: 0.001) D_A: 0.172 G_A: 0.397 cycle_A: 0.676 idt_A: 0.079 D_B: 0.143 G_B: 0.251 cycle_B: 0.235 idt_B: 0.190 \n",
      "(epoch: 153, iters: 224, time: 1.673, data: 0.001) D_A: 0.127 G_A: 0.531 cycle_A: 1.129 idt_A: 0.077 D_B: 0.205 G_B: 0.241 cycle_B: 0.246 idt_B: 0.320 \n",
      "End of epoch 153 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 154, iters: 36, time: 0.307, data: 0.001) D_A: 0.189 G_A: 0.434 cycle_A: 1.037 idt_A: 0.106 D_B: 0.179 G_B: 0.376 cycle_B: 0.267 idt_B: 0.286 \n",
      "(epoch: 154, iters: 136, time: 0.317, data: 0.001) D_A: 0.167 G_A: 0.479 cycle_A: 0.962 idt_A: 0.120 D_B: 0.270 G_B: 0.331 cycle_B: 0.322 idt_B: 0.262 \n",
      "(epoch: 154, iters: 236, time: 0.315, data: 0.002) D_A: 0.153 G_A: 0.453 cycle_A: 1.010 idt_A: 0.073 D_B: 0.271 G_B: 0.409 cycle_B: 0.206 idt_B: 0.273 \n",
      "End of epoch 154 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 155, iters: 48, time: 1.771, data: 0.002) D_A: 0.132 G_A: 0.363 cycle_A: 0.682 idt_A: 0.080 D_B: 0.068 G_B: 0.479 cycle_B: 0.213 idt_B: 0.191 \n",
      "(epoch: 155, iters: 148, time: 0.315, data: 0.001) D_A: 0.163 G_A: 0.388 cycle_A: 0.732 idt_A: 0.082 D_B: 0.322 G_B: 0.394 cycle_B: 0.253 idt_B: 0.186 \n",
      "(epoch: 155, iters: 248, time: 0.297, data: 0.001) D_A: 0.195 G_A: 0.378 cycle_A: 0.675 idt_A: 0.061 D_B: 0.208 G_B: 0.454 cycle_B: 0.167 idt_B: 0.186 \n",
      "End of epoch 155 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 156, iters: 60, time: 0.278, data: 0.001) D_A: 0.169 G_A: 0.415 cycle_A: 0.773 idt_A: 0.137 D_B: 0.224 G_B: 0.356 cycle_B: 0.293 idt_B: 0.202 \n",
      "(epoch: 156, iters: 160, time: 1.778, data: 0.002) D_A: 0.124 G_A: 0.465 cycle_A: 0.738 idt_A: 0.078 D_B: 0.093 G_B: 0.367 cycle_B: 0.237 idt_B: 0.205 \n",
      "(epoch: 156, iters: 260, time: 0.290, data: 0.002) D_A: 0.149 G_A: 0.419 cycle_A: 0.865 idt_A: 0.104 D_B: 0.170 G_B: 0.317 cycle_B: 0.301 idt_B: 0.238 \n",
      "End of epoch 156 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 157, iters: 72, time: 0.304, data: 0.002) D_A: 0.178 G_A: 0.364 cycle_A: 0.890 idt_A: 0.112 D_B: 0.193 G_B: 0.293 cycle_B: 0.305 idt_B: 0.271 \n",
      "saving the latest model (epoch 157, total_iters 45000)\n",
      "(epoch: 157, iters: 172, time: 0.315, data: 0.002) D_A: 0.129 G_A: 0.613 cycle_A: 1.115 idt_A: 0.079 D_B: 0.263 G_B: 0.247 cycle_B: 0.235 idt_B: 0.236 \n",
      "(epoch: 157, iters: 272, time: 1.776, data: 0.001) D_A: 0.153 G_A: 0.470 cycle_A: 0.645 idt_A: 0.109 D_B: 0.089 G_B: 0.382 cycle_B: 0.235 idt_B: 0.168 \n",
      "End of epoch 157 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 158, iters: 84, time: 0.308, data: 0.002) D_A: 0.191 G_A: 0.453 cycle_A: 0.995 idt_A: 0.103 D_B: 0.219 G_B: 0.351 cycle_B: 0.298 idt_B: 0.276 \n",
      "(epoch: 158, iters: 184, time: 0.315, data: 0.002) D_A: 0.163 G_A: 0.453 cycle_A: 0.771 idt_A: 0.082 D_B: 0.249 G_B: 0.365 cycle_B: 0.229 idt_B: 0.209 \n",
      "(epoch: 158, iters: 284, time: 0.316, data: 0.001) D_A: 0.176 G_A: 0.473 cycle_A: 1.238 idt_A: 0.061 D_B: 0.220 G_B: 0.436 cycle_B: 0.169 idt_B: 0.351 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 158 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000001 -> 0.0000001\n",
      "(epoch: 159, iters: 96, time: 1.823, data: 0.002) D_A: 0.118 G_A: 0.554 cycle_A: 0.998 idt_A: 0.131 D_B: 0.199 G_B: 0.311 cycle_B: 0.323 idt_B: 0.281 \n",
      "(epoch: 159, iters: 196, time: 0.315, data: 0.002) D_A: 0.180 G_A: 0.408 cycle_A: 1.014 idt_A: 0.106 D_B: 0.164 G_B: 0.291 cycle_B: 0.308 idt_B: 0.243 \n",
      "End of epoch 159 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000001 -> 0.0000000\n",
      "(epoch: 160, iters: 8, time: 0.316, data: 0.001) D_A: 0.168 G_A: 0.571 cycle_A: 1.224 idt_A: 0.083 D_B: 0.176 G_B: 0.346 cycle_B: 0.243 idt_B: 0.291 \n",
      "(epoch: 160, iters: 108, time: 0.287, data: 0.002) D_A: 0.142 G_A: 0.402 cycle_A: 0.852 idt_A: 0.076 D_B: 0.253 G_B: 0.434 cycle_B: 0.208 idt_B: 0.235 \n",
      "(epoch: 160, iters: 208, time: 1.787, data: 0.001) D_A: 0.144 G_A: 0.473 cycle_A: 0.971 idt_A: 0.096 D_B: 0.149 G_B: 0.264 cycle_B: 0.287 idt_B: 0.262 \n",
      "End of epoch 160 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 161, iters: 20, time: 0.292, data: 0.001) D_A: 0.152 G_A: 0.336 cycle_A: 0.779 idt_A: 0.112 D_B: 0.266 G_B: 0.327 cycle_B: 0.302 idt_B: 0.221 \n",
      "(epoch: 161, iters: 120, time: 0.309, data: 0.002) D_A: 0.141 G_A: 0.587 cycle_A: 1.356 idt_A: 0.079 D_B: 0.204 G_B: 0.466 cycle_B: 0.212 idt_B: 0.377 \n",
      "(epoch: 161, iters: 220, time: 0.296, data: 0.002) D_A: 0.121 G_A: 0.461 cycle_A: 0.742 idt_A: 0.083 D_B: 0.327 G_B: 0.272 cycle_B: 0.247 idt_B: 0.205 \n",
      "End of epoch 161 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 162, iters: 32, time: 1.800, data: 0.001) D_A: 0.139 G_A: 0.424 cycle_A: 0.886 idt_A: 0.074 D_B: 0.089 G_B: 0.419 cycle_B: 0.194 idt_B: 0.228 \n",
      "(epoch: 162, iters: 132, time: 0.307, data: 0.002) D_A: 0.168 G_A: 0.428 cycle_A: 0.778 idt_A: 0.102 D_B: 0.270 G_B: 0.339 cycle_B: 0.269 idt_B: 0.215 \n",
      "(epoch: 162, iters: 232, time: 0.291, data: 0.001) D_A: 0.142 G_A: 0.458 cycle_A: 0.869 idt_A: 0.097 D_B: 0.187 G_B: 0.273 cycle_B: 0.262 idt_B: 0.236 \n",
      "End of epoch 162 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 163, iters: 44, time: 0.287, data: 0.002) D_A: 0.184 G_A: 0.378 cycle_A: 0.810 idt_A: 0.074 D_B: 0.215 G_B: 0.398 cycle_B: 0.201 idt_B: 0.218 \n",
      "(epoch: 163, iters: 144, time: 1.767, data: 0.001) D_A: 0.148 G_A: 0.477 cycle_A: 0.890 idt_A: 0.099 D_B: 0.303 G_B: 0.290 cycle_B: 0.263 idt_B: 0.220 \n",
      "(epoch: 163, iters: 244, time: 0.316, data: 0.002) D_A: 0.142 G_A: 0.499 cycle_A: 1.123 idt_A: 0.117 D_B: 0.228 G_B: 0.360 cycle_B: 0.286 idt_B: 0.312 \n",
      "End of epoch 163 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 164, iters: 56, time: 0.298, data: 0.001) D_A: 0.153 G_A: 0.430 cycle_A: 0.789 idt_A: 0.083 D_B: 0.186 G_B: 0.271 cycle_B: 0.247 idt_B: 0.240 \n",
      "(epoch: 164, iters: 156, time: 0.283, data: 0.001) D_A: 0.145 G_A: 0.403 cycle_A: 0.756 idt_A: 0.079 D_B: 0.146 G_B: 0.252 cycle_B: 0.235 idt_B: 0.208 \n",
      "(epoch: 164, iters: 256, time: 1.797, data: 0.001) D_A: 0.131 G_A: 0.474 cycle_A: 1.154 idt_A: 0.098 D_B: 0.246 G_B: 0.273 cycle_B: 0.262 idt_B: 0.320 \n",
      "End of epoch 164 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 165, iters: 68, time: 0.291, data: 0.002) D_A: 0.143 G_A: 0.481 cycle_A: 0.686 idt_A: 0.095 D_B: 0.082 G_B: 0.453 cycle_B: 0.211 idt_B: 0.172 \n",
      "(epoch: 165, iters: 168, time: 0.295, data: 0.001) D_A: 0.141 G_A: 0.513 cycle_A: 1.032 idt_A: 0.079 D_B: 0.175 G_B: 0.402 cycle_B: 0.233 idt_B: 0.292 \n",
      "(epoch: 165, iters: 268, time: 0.293, data: 0.002) D_A: 0.165 G_A: 0.420 cycle_A: 1.199 idt_A: 0.091 D_B: 0.172 G_B: 0.343 cycle_B: 0.268 idt_B: 0.332 \n",
      "End of epoch 165 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 166, iters: 80, time: 1.823, data: 0.002) D_A: 0.188 G_A: 0.389 cycle_A: 0.798 idt_A: 0.102 D_B: 0.248 G_B: 0.380 cycle_B: 0.265 idt_B: 0.224 \n",
      "(epoch: 166, iters: 180, time: 0.293, data: 0.002) D_A: 0.149 G_A: 0.583 cycle_A: 1.353 idt_A: 0.081 D_B: 0.158 G_B: 0.375 cycle_B: 0.233 idt_B: 0.350 \n",
      "(epoch: 166, iters: 280, time: 0.296, data: 0.002) D_A: 0.176 G_A: 0.380 cycle_A: 0.729 idt_A: 0.098 D_B: 0.227 G_B: 0.296 cycle_B: 0.264 idt_B: 0.218 \n",
      "End of epoch 166 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 167, iters: 92, time: 0.308, data: 0.001) D_A: 0.126 G_A: 0.434 cycle_A: 0.794 idt_A: 0.080 D_B: 0.226 G_B: 0.375 cycle_B: 0.209 idt_B: 0.214 \n",
      "(epoch: 167, iters: 192, time: 1.787, data: 0.001) D_A: 0.174 G_A: 0.442 cycle_A: 1.117 idt_A: 0.098 D_B: 0.254 G_B: 0.273 cycle_B: 0.262 idt_B: 0.305 \n",
      "End of epoch 167 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 168, iters: 4, time: 0.315, data: 0.001) D_A: 0.150 G_A: 0.403 cycle_A: 0.756 idt_A: 0.088 D_B: 0.120 G_B: 0.312 cycle_B: 0.249 idt_B: 0.208 \n",
      "(epoch: 168, iters: 104, time: 0.316, data: 0.005) D_A: 0.181 G_A: 0.374 cycle_A: 0.675 idt_A: 0.082 D_B: 0.229 G_B: 0.362 cycle_B: 0.229 idt_B: 0.185 \n",
      "(epoch: 168, iters: 204, time: 0.299, data: 0.002) D_A: 0.131 G_A: 0.362 cycle_A: 0.671 idt_A: 0.081 D_B: 0.092 G_B: 0.383 cycle_B: 0.213 idt_B: 0.196 \n",
      "End of epoch 168 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 169, iters: 16, time: 1.808, data: 0.001) D_A: 0.149 G_A: 0.525 cycle_A: 1.257 idt_A: 0.109 D_B: 0.190 G_B: 0.343 cycle_B: 0.294 idt_B: 0.311 \n",
      "(epoch: 169, iters: 116, time: 0.307, data: 0.002) D_A: 0.140 G_A: 0.447 cycle_A: 1.133 idt_A: 0.116 D_B: 0.212 G_B: 0.343 cycle_B: 0.258 idt_B: 0.293 \n",
      "(epoch: 169, iters: 216, time: 0.295, data: 0.001) D_A: 0.144 G_A: 0.457 cycle_A: 0.957 idt_A: 0.131 D_B: 0.236 G_B: 0.306 cycle_B: 0.323 idt_B: 0.262 \n",
      "End of epoch 169 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 170, iters: 28, time: 0.288, data: 0.001) D_A: 0.158 G_A: 0.494 cycle_A: 1.083 idt_A: 0.105 D_B: 0.180 G_B: 0.288 cycle_B: 0.307 idt_B: 0.235 \n",
      "(epoch: 170, iters: 128, time: 1.852, data: 0.002) D_A: 0.140 G_A: 0.431 cycle_A: 0.870 idt_A: 0.132 D_B: 0.229 G_B: 0.337 cycle_B: 0.275 idt_B: 0.235 \n",
      "(epoch: 170, iters: 228, time: 0.281, data: 0.002) D_A: 0.136 G_A: 0.484 cycle_A: 1.131 idt_A: 0.071 D_B: 0.291 G_B: 0.445 cycle_B: 0.208 idt_B: 0.313 \n",
      "End of epoch 170 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 171, iters: 40, time: 0.280, data: 0.001) D_A: 0.123 G_A: 0.557 cycle_A: 1.354 idt_A: 0.086 D_B: 0.181 G_B: 0.356 cycle_B: 0.227 idt_B: 0.365 \n",
      "(epoch: 171, iters: 140, time: 0.308, data: 0.001) D_A: 0.143 G_A: 0.364 cycle_A: 0.740 idt_A: 0.092 D_B: 0.157 G_B: 0.217 cycle_B: 0.288 idt_B: 0.207 \n",
      "(epoch: 171, iters: 240, time: 1.869, data: 0.002) D_A: 0.185 G_A: 0.402 cycle_A: 0.852 idt_A: 0.093 D_B: 0.238 G_B: 0.343 cycle_B: 0.253 idt_B: 0.239 \n",
      "End of epoch 171 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 172, iters: 52, time: 0.312, data: 0.002) D_A: 0.165 G_A: 0.402 cycle_A: 1.205 idt_A: 0.069 D_B: 0.188 G_B: 0.434 cycle_B: 0.185 idt_B: 0.331 \n",
      "(epoch: 172, iters: 152, time: 0.316, data: 0.002) D_A: 0.170 G_A: 0.369 cycle_A: 0.750 idt_A: 0.085 D_B: 0.258 G_B: 0.223 cycle_B: 0.262 idt_B: 0.204 \n",
      "(epoch: 172, iters: 252, time: 0.283, data: 0.001) D_A: 0.160 G_A: 0.415 cycle_A: 0.839 idt_A: 0.091 D_B: 0.080 G_B: 0.370 cycle_B: 0.223 idt_B: 0.204 \n",
      "End of epoch 172 / 300 \t Time Taken: 81 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 173, iters: 64, time: 1.798, data: 0.002) D_A: 0.118 G_A: 0.527 cycle_A: 1.257 idt_A: 0.075 D_B: 0.159 G_B: 0.419 cycle_B: 0.194 idt_B: 0.311 \n",
      "(epoch: 173, iters: 164, time: 0.317, data: 0.002) D_A: 0.164 G_A: 0.455 cycle_A: 1.161 idt_A: 0.126 D_B: 0.180 G_B: 0.470 cycle_B: 0.250 idt_B: 0.289 \n",
      "(epoch: 173, iters: 264, time: 0.315, data: 0.002) D_A: 0.157 G_A: 0.400 cycle_A: 0.712 idt_A: 0.095 D_B: 0.134 G_B: 0.454 cycle_B: 0.211 idt_B: 0.199 \n",
      "End of epoch 173 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 174, iters: 76, time: 0.317, data: 0.001) D_A: 0.169 G_A: 0.422 cycle_A: 0.791 idt_A: 0.103 D_B: 0.250 G_B: 0.420 cycle_B: 0.245 idt_B: 0.235 \n",
      "(epoch: 174, iters: 176, time: 1.840, data: 0.002) D_A: 0.190 G_A: 0.407 cycle_A: 0.851 idt_A: 0.070 D_B: 0.252 G_B: 0.399 cycle_B: 0.210 idt_B: 0.246 \n",
      "saving the latest model (epoch 174, total_iters 50000)\n",
      "(epoch: 174, iters: 276, time: 0.282, data: 0.002) D_A: 0.155 G_A: 0.369 cycle_A: 0.762 idt_A: 0.124 D_B: 0.242 G_B: 0.448 cycle_B: 0.252 idt_B: 0.221 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 174 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 175, iters: 88, time: 0.293, data: 0.002) D_A: 0.159 G_A: 0.370 cycle_A: 0.835 idt_A: 0.103 D_B: 0.117 G_B: 0.419 cycle_B: 0.245 idt_B: 0.240 \n",
      "(epoch: 175, iters: 188, time: 0.303, data: 0.001) D_A: 0.162 G_A: 0.436 cycle_A: 0.867 idt_A: 0.103 D_B: 0.180 G_B: 0.314 cycle_B: 0.300 idt_B: 0.249 \n",
      "(epoch: 175, iters: 288, time: 1.852, data: 0.002) D_A: 0.143 G_A: 0.444 cycle_A: 1.129 idt_A: 0.145 D_B: 0.210 G_B: 0.314 cycle_B: 0.292 idt_B: 0.315 \n",
      "End of epoch 175 / 300 \t Time Taken: 84 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 176, iters: 100, time: 0.315, data: 0.279) D_A: 0.145 G_A: 0.498 cycle_A: 1.121 idt_A: 0.098 D_B: 0.305 G_B: 0.213 cycle_B: 0.263 idt_B: 0.278 \n",
      "(epoch: 176, iters: 200, time: 0.314, data: 0.001) D_A: 0.167 G_A: 0.500 cycle_A: 0.830 idt_A: 0.093 D_B: 0.290 G_B: 0.343 cycle_B: 0.253 idt_B: 0.217 \n",
      "End of epoch 176 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 177, iters: 12, time: 0.291, data: 0.001) D_A: 0.139 G_A: 0.553 cycle_A: 0.895 idt_A: 0.075 D_B: 0.215 G_B: 0.399 cycle_B: 0.201 idt_B: 0.248 \n",
      "(epoch: 177, iters: 112, time: 1.922, data: 0.002) D_A: 0.151 G_A: 0.477 cycle_A: 0.763 idt_A: 0.120 D_B: 0.110 G_B: 0.331 cycle_B: 0.322 idt_B: 0.216 \n",
      "(epoch: 177, iters: 212, time: 0.285, data: 0.002) D_A: 0.138 G_A: 0.509 cycle_A: 0.997 idt_A: 0.074 D_B: 0.258 G_B: 0.214 cycle_B: 0.249 idt_B: 0.244 \n",
      "End of epoch 177 / 300 \t Time Taken: 83 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 178, iters: 24, time: 0.289, data: 0.001) D_A: 0.178 G_A: 0.536 cycle_A: 0.855 idt_A: 0.137 D_B: 0.213 G_B: 0.335 cycle_B: 0.293 idt_B: 0.239 \n",
      "(epoch: 178, iters: 124, time: 0.285, data: 0.002) D_A: 0.166 G_A: 0.380 cycle_A: 0.729 idt_A: 0.079 D_B: 0.207 G_B: 0.451 cycle_B: 0.226 idt_B: 0.218 \n",
      "(epoch: 178, iters: 224, time: 1.882, data: 0.001) D_A: 0.173 G_A: 0.458 cycle_A: 0.879 idt_A: 0.112 D_B: 0.091 G_B: 0.389 cycle_B: 0.283 idt_B: 0.250 \n",
      "End of epoch 178 / 300 \t Time Taken: 82 sec\n",
      "learning rate 0.0000000 -> 0.0000000\n",
      "(epoch: 179, iters: 36, time: 0.318, data: 0.002) D_A: 0.163 G_A: 0.433 cycle_A: 1.206 idt_A: 0.103 D_B: 0.185 G_B: 0.314 cycle_B: 0.300 idt_B: 0.331 \n",
      "(epoch: 179, iters: 136, time: 0.309, data: 0.002) D_A: 0.143 G_A: 0.471 cycle_A: 0.884 idt_A: 0.126 D_B: 0.172 G_B: 0.470 cycle_B: 0.250 idt_B: 0.265 \n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /storage01/grexai/datasets/imgreg/Image_registration/Unaligned/ \\\n",
    "--name cycle_hela63_resize_ualigned \\\n",
    "--model cycle_gan --lr=0.0001 --lr_policy step --lr_decay_iters=40 \\\n",
    "--preprocess resize --load_size 256 \\\n",
    "--lambda_identity=0.5 --save_epoch_freq=50 --n_epochs=200  --display_port=8088 --gpu_ids=0,1 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "                augment_a: False                         \n",
      "               batch_size: 1                             \n",
      "               brightness: False                         \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                 contrast: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/\t[default: None]\n",
      "             dataset_mode: single                        \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                    gauss: False                         \n",
      "                  gpu_ids: 3                             \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: test                          \n",
      "             model_suffix:                               \n",
      "               n_layers_D: 3                             \n",
      "                     name: cycle_hela63_resize_unaligned \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \t[default: False]\n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 10000                         \t[default: 50]\n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: scale_width                   \t[default: resize_and_crop]\n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                sharpness: False                         \n",
      "                   suffix:                               \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [SingleDataset] was created\n",
      "initialize network with normal\n",
      "model [TestModel] was created\n",
      "loading the model from ./checkpoints/cycle_hela63_resize_unaligned/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "creating web directory ./results/cycle_hela63_resize_unaligned/test_latest\n",
      "<data.CustomDatasetDataLoader object at 0x7fe62411cc18>\n",
      "processing (0000)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m112_c1_z0_l1_o0_1.png']\n",
      "processing (0001)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m112_c1_z0_l1_o0_2.png']\n",
      "processing (0002)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m112_c1_z0_l1_o0_3.png']\n",
      "processing (0003)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m141_c1_z0_l1_o0_1.png']\n",
      "processing (0004)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m141_c1_z0_l1_o0_2.png']\n",
      "processing (0005)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m141_c1_z0_l1_o0_3.png']\n",
      "processing (0006)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m151_c1_z0_l1_o0_1.png']\n",
      "processing (0007)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m151_c1_z0_l1_o0_2.png']\n",
      "processing (0008)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m151_c1_z0_l1_o0_3.png']\n",
      "processing (0009)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m192_c1_z0_l1_o0_1.png']\n",
      "processing (0010)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m192_c1_z0_l1_o0_2.png']\n",
      "processing (0011)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m192_c1_z0_l1_o0_3.png']\n",
      "processing (0012)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m212_c1_z0_l1_o0_1.png']\n",
      "processing (0013)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m212_c1_z0_l1_o0_2.png']\n",
      "processing (0014)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m212_c1_z0_l1_o0_3.png']\n",
      "processing (0015)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m222_c1_z0_l1_o0_1.png']\n",
      "processing (0016)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m222_c1_z0_l1_o0_2.png']\n",
      "processing (0017)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m222_c1_z0_l1_o0_3.png']\n",
      "processing (0018)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m22_c1_z0_l1_o0_1.png']\n",
      "processing (0019)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m22_c1_z0_l1_o0_2.png']\n",
      "processing (0020)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m22_c1_z0_l1_o0_3.png']\n",
      "processing (0021)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m260_c1_z0_l1_o0_1.png']\n",
      "processing (0022)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m260_c1_z0_l1_o0_2.png']\n",
      "processing (0023)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m260_c1_z0_l1_o0_3.png']\n",
      "processing (0024)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m27_c1_z0_l1_o0_1.png']\n",
      "processing (0025)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m27_c1_z0_l1_o0_2.png']\n",
      "processing (0026)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m27_c1_z0_l1_o0_3.png']\n",
      "processing (0027)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m281_c1_z0_l1_o0_1.png']\n",
      "processing (0028)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m281_c1_z0_l1_o0_2.png']\n",
      "processing (0029)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m281_c1_z0_l1_o0_3.png']\n",
      "processing (0030)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m290_c1_z0_l1_o0_1.png']\n",
      "processing (0031)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m290_c1_z0_l1_o0_2.png']\n",
      "processing (0032)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m290_c1_z0_l1_o0_3.png']\n",
      "processing (0033)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m322_c1_z0_l1_o0_1.png']\n",
      "processing (0034)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m322_c1_z0_l1_o0_2.png']\n",
      "processing (0035)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m322_c1_z0_l1_o0_3.png']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing (0036)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m323_c1_z0_l1_o0_1.png']\n",
      "processing (0037)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m323_c1_z0_l1_o0_2.png']\n",
      "processing (0038)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m323_c1_z0_l1_o0_3.png']\n",
      "processing (0039)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m332_c1_z0_l1_o0_1.png']\n",
      "processing (0040)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m332_c1_z0_l1_o0_2.png']\n",
      "processing (0041)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m332_c1_z0_l1_o0_3.png']\n",
      "processing (0042)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m333_c1_z0_l1_o0_1.png']\n",
      "processing (0043)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m333_c1_z0_l1_o0_2.png']\n",
      "processing (0044)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m333_c1_z0_l1_o0_3.png']\n",
      "processing (0045)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m37_c1_z0_l1_o0_1.png']\n",
      "processing (0046)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m37_c1_z0_l1_o0_2.png']\n",
      "processing (0047)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m37_c1_z0_l1_o0_3.png']\n",
      "processing (0048)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m60_c1_z0_l1_o0_1.png']\n",
      "processing (0049)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m60_c1_z0_l1_o0_2.png']\n",
      "processing (0050)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m60_c1_z0_l1_o0_3.png']\n",
      "processing (0051)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m63_c1_z0_l1_o0_1.png']\n",
      "processing (0052)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m63_c1_z0_l1_o0_2.png']\n",
      "processing (0053)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m63_c1_z0_l1_o0_3.png']\n",
      "processing (0054)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m72_c1_z0_l1_o0_1.png']\n",
      "processing (0055)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m72_c1_z0_l1_o0_2.png']\n",
      "processing (0056)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m72_c1_z0_l1_o0_3.png']\n",
      "processing (0057)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m84_c1_z0_l1_o0_1.png']\n",
      "processing (0058)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m84_c1_z0_l1_o0_2.png']\n",
      "processing (0059)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m84_c1_z0_l1_o0_3.png']\n",
      "processing (0060)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m87_c1_z0_l1_o0_1.png']\n",
      "processing (0061)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m87_c1_z0_l1_o0_2.png']\n",
      "processing (0062)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m87_c1_z0_l1_o0_3.png']\n",
      "processing (0063)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m94_c1_z0_l1_o0_1.png']\n",
      "processing (0064)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m94_c1_z0_l1_o0_2.png']\n",
      "processing (0065)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m94_c1_z0_l1_o0_3.png']\n",
      "processing (0066)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m9_c1_z0_l1_o0_1.png']\n",
      "processing (0067)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m9_c1_z0_l1_o0_2.png']\n",
      "processing (0068)-th image... ['/storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/p1_wA1_t1_m9_c1_z0_l1_o0_3.png']\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot  /storage01/grexai/datasets/imgreg/Image_registration/Aligned/test/trainA/ \\\n",
    "--name cycle_hela63_resize_unaligned --model test \\\n",
    "--no_dropout --preprocess scale_width --load_size 256 --gpu_ids=3 --num_test=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzSKIPUByfiN"
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Mgg8raPyizq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd79de2ac18>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/biastoleica/test_latest/images/1347_5_real.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G3oVH9DyqLQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd794262470>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/biastoleica/test_latest/images/1347_5_fake.png')\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CycleGAN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
